{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import create_model\n",
    "import pickle\n",
    "import time \n",
    "import copy\n",
    "import random\n",
    "import data_formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = 'enron'\n",
    "\n",
    "#vocab_dict = pickle.load(open('../processed_data/word_dict_v02_enron_py35_seq_length_3_49_sample_4256_limited_vocab.pkl', 'rb'))\n",
    "#df_all = pd.read_pickle('../processed_data/processed_data_v02_enron_py35_seq_length_3_49_sample_4256_limited_vocab.pkl')\n",
    "\n",
    "dataset = 'twitter'\n",
    "\n",
    "#vocab_dict = pickle.load(open('../processed_data/word_dict_v02_twitter_py35_seq_length_3_25_sample_1764604_questions.pkl', 'rb'))\n",
    "#df_all = pd.read_pickle('../processed_data/processed_data_v02_twitter_py35_seq_length_3_25_sample_1764604_questions.pkl')\n",
    "\n",
    "#vocab_dict = pickle.load(open('../processed_data/word_dict_v02_twitter_py35_seq_length_4_15_sample_134241_full.pkl', 'rb'))\n",
    "#df_all = pd.read_pickle('../processed_data/processed_data_v02_twitter_py35_seq_length_4_15_sample_134241_full.pkl')\n",
    "\n",
    "vocab_dict = pickle.load(open('../processed_data/word_dict_v02_twitter_py35_seq_length_3_19_sample_21946_lem.pkl', 'rb'))\n",
    "df_all = pd.read_pickle('../processed_data/processed_data_v02_twitter_py35_seq_length_3_19_sample_21946_lem.pkl')\n",
    "\n",
    "#vocab_dict = pickle.load(open('../processed_data/word_dict_v02_twitter_py35_seq_length_3_25_sample_1901567_full.pkl', 'rb'))\n",
    "#df_all = pd.read_pickle('../processed_data/processed_data_v02_twitter_py35_seq_length_3_25_sample_1901567_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = data_formatting.createInvMap(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['alpha_Pair_0_encoding'] = df_all['alpha_Pair_0_tokens'].apply(lambda x: data_formatting.encodeSent(x, vocab_dict))\n",
    "df_all['alpha_Pair_1_encoding'] = df_all['alpha_Pair_1_tokens'].apply(lambda x: data_formatting.encodeSent(x, vocab_dict))\n",
    "df_all['Index'] = df_all.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_train = df_all.sample(frac=0.97, random_state=1)\n",
    "\n",
    "df_all_dev = df_all[df_all['Index'].isin(df_all_train['Index'].values) == False]\n",
    "\n",
    "df_all_test = df_all_dev.sample(frac=0.10, random_state=1)\n",
    "\n",
    "df_all_dev = df_all_dev[df_all_dev['Index'].isin(df_all_test['Index'].values) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17690 17159 478 53 17322\n"
     ]
    }
   ],
   "source": [
    "print (df_all.shape[0], df_all_train.shape[0],  df_all_dev.shape[0], df_all_test.shape[0], len(vocab_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_formatting.prepare_train_batch(df_all_train['alpha_Pair_0_encoding'].values, \n",
    "                                                    df_all_train['alpha_Pair_1_encoding'].values)\n",
    "\n",
    "dev_data = data_formatting.prepare_train_batch(df_all_dev['alpha_Pair_0_encoding'].values, \n",
    "                                                    df_all_dev['alpha_Pair_1_encoding'].values)\n",
    "\n",
    "test_data = data_formatting.prepare_train_batch(df_all_test['alpha_Pair_0_encoding'].values, \n",
    "                                                    df_all_test['alpha_Pair_1_encoding'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_params = {'n_cells':256, 'num_layers':2, 'embedding_size':512, \n",
    "          'vocab_size':len(vocab_dict) + 1, 'minibatch_size':64, 'n_threads':128,\n",
    "          'beam_width':10, 'limit_decode_steps': None,\n",
    "          'encoder_input_keep':1, 'decoder_input_keep':1,\n",
    "          'encoder_output_keep':1, 'decoder_output_keep':1,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_model_params = train_model_params\n",
    "dev_model_params['encoder_input_keep'] = 1\n",
    "dev_model_params['encoder_output_keep'] = 1\n",
    "dev_model_params['decoder_input_keep'] = 1\n",
    "dev_model_params['decoder_output_keep'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = { 'vocab_lower':3, 'vocab_upper':train_model_params['vocab_size']-1, \n",
    "                    'n_epochs':500000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enc input           > ['i', 'll', 'be', 'there', 'tomorrow', '<EOS>']\n",
    "#dec input           > ['last', 'game', 'of', 'the', 'regular', 'season', '<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import tensor_array_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "slice index 2 of dimension 0 out of bounds. for 'training_model/while/strided_slice_7' (op: 'StridedSlice') with input shapes: [2], [1], [1], [1] and with computed input tensors: input[1] = <2>, input[2] = <3>, input[3] = <1>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    653\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 654\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    655\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: slice index 2 of dimension 0 out of bounds. for 'training_model/while/strided_slice_7' (op: 'StridedSlice') with input shapes: [2], [1], [1], [1] and with computed input tensors: input[1] = <2>, input[2] = <3>, input[3] = <1>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4c4518c78dea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0minf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minf_model_2_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'debug'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\coding\\seq2seq\\create_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, mode, sequence_data)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'debug'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_inference_decoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'infer'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\coding\\seq2seq\\create_model.py\u001b[0m in \u001b[0;36mcreate_inference_decoder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    282\u001b[0m                                         \u001b[0mloop_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minitial_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_finished\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_logits\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m                                         \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m                                         swap_memory=False)\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_training_decoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\u001b[0m\n\u001b[0;32m   2773\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWhileContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2774\u001b[0m     \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2775\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2776\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[1;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2602\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2603\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[1;32m-> 2604\u001b[1;33m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[0;32m   2605\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2606\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[1;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2552\u001b[0m         \u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moriginal_loop_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2553\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m-> 2554\u001b[1;33m     \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2555\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2556\u001b[0m       \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\coding\\seq2seq\\create_model.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(time, state, inputs, finished, logits)\u001b[0m\n\u001b[0;32m    226\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcell_outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[0mstep_log_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                 \u001b[0mstep_log_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_helpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mask_probs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_log_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreviously_finished\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                 \u001b[1;31m#Here is where we will add the anti-language model at a later date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\coding\\seq2seq\\tf_helpers.py\u001b[0m in \u001b[0;36m_mask_probs\u001b[1;34m(probs, eos_token, finished)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprobability\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mEOS\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m   \"\"\"\n\u001b[1;32m---> 20\u001b[1;33m   \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m   finished_mask = tf.expand_dims(\n\u001b[0;32m     22\u001b[0m       math_ops.to_float(1. - tf.to_float(finished)), 2)\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_SliceHelper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[0mvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m    675\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m   3742\u001b[0m                                 \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3743\u001b[0m                                 \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3744\u001b[1;33m                                 shrink_axis_mask=shrink_axis_mask, name=name)\n\u001b[0m\u001b[0;32m   3745\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    768\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2631\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2632\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, require_shape_fn)\u001b[0m\n\u001b[0;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m                                   require_shape_fn)\n\u001b[0m\u001b[0;32m    596\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: slice index 2 of dimension 0 out of bounds. for 'training_model/while/strided_slice_7' (op: 'StridedSlice') with input shapes: [2], [1], [1], [1] and with computed input tensors: input[1] = <2>, input[2] = <3>, input[3] = <1>."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "inf_model_1_params = {'n_cells':256, 'num_layers':2, 'embedding_size':512, \n",
    "          'vocab_size':len(vocab_dict) + 1, 'minibatch_size':64, 'n_threads':128,\n",
    "          'beam_width':1, 'limit_decode_steps': None,\n",
    "          'encoder_input_keep':1, 'decoder_input_keep':1,\n",
    "          'encoder_output_keep':1, 'decoder_output_keep':1,\n",
    "         }\n",
    "\n",
    "#inf_model_2_params = {'n_cells':2, 'num_layers':1, 'embedding_size':5, \n",
    "#            'vocab_size':7,\n",
    "#          'minibatch_size':64, 'n_threads':128,\n",
    "#          'beam_width':3, 'limit_decode_steps': None,\n",
    "#          'encoder_input_keep':1, 'decoder_input_keep':1,\n",
    "#          'encoder_output_keep':1, 'decoder_output_keep':1,\n",
    "#         }\n",
    "\n",
    "\n",
    "inf_model_2_params = {'n_cells':256, 'num_layers':2, 'embedding_size':512, \n",
    "          'vocab_size':len(vocab_dict) + 1,\n",
    "          'minibatch_size':64, 'n_threads':128,\n",
    "          'beam_width':3, 'limit_decode_steps': None,\n",
    "          'encoder_input_keep':1, 'decoder_input_keep':1,\n",
    "          'encoder_output_keep':1, 'decoder_output_keep':1,\n",
    "         }\n",
    "\n",
    "#with tf.variable_scope('training_model'):\n",
    "\n",
    "#    inf_model_1 = create_model.Model(inf_model_1_params, 'infer', None)\n",
    "    \n",
    "#with tf.variable_scope('training_model'):\n",
    "\n",
    "#    inf_model_2 = create_model.Model(inf_model_2_params, 'infer', None)   \n",
    "\n",
    "with tf.variable_scope('training_model'):\n",
    "\n",
    "    inf_train = create_model.Model(inf_model_2_params, 'debug', None)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    #saver = tf.train.Saver()\n",
    "   \n",
    "    #saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run(inf_train.decode_loop, \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00019024, -0.10373823, -0.00839701, ...,  0.00793328,\n",
       "        -0.0569768 ,  0.05554437],\n",
       "       [-0.00019024, -0.10373823, -0.00839701, ...,  0.00793328,\n",
       "        -0.0569768 ,  0.05554437],\n",
       "       [-0.00019024, -0.10373822, -0.00839701, ...,  0.00793327,\n",
       "        -0.0569768 ,  0.05554438]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tensor we want to print the value of\n",
    "x = tf.placeholder(tf.float32, shape=[2, 2, 2])\n",
    "a = np.array([[[1.,1.], [1.,1.]], [[2.,2.], [2.,2.]]])\n",
    "\n",
    "m = tf.Print(x,[x])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    m_eval = m.eval(session=sess,feed_dict={x: a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = inf_model_2.inference_decoder._batch_size\n",
    "beam_width = inf_model_2.inference_decoder._beam_width\n",
    "end_token = inf_model_2.inference_decoder._end_token\n",
    "length_penalty_weight = inf_model_2.inference_decoder._length_penalty_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "name= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished, first_inputs, initial_state = inf_model_2.inference_decoder.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'training_model/embedding_lookup_1:0' shape=(?, 3, 512) dtype=float32>,\n",
       " BeamSearchDecoderState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'training_model/Reshape:0' shape=(?, 3, 256) dtype=float32>, h=<tf.Tensor 'training_model/Reshape_1:0' shape=(?, 3, 256) dtype=float32>), AttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'training_model/Reshape_2:0' shape=(?, 3, 256) dtype=float32>, h=<tf.Tensor 'training_model/Reshape_3:0' shape=(?, 3, 256) dtype=float32>), attention=<tf.Tensor 'training_model/Reshape_4:0' shape=(?, 3, 256) dtype=float32>, time=<tf.Tensor 'training_model/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'training_model/Reshape_5:0' shape=(?, 3, ?) dtype=float32>, alignment_history=())), log_probs=<tf.Tensor 'zeros:0' shape=(?, 3) dtype=float32>, finished=<tf.Tensor 'training_model/zeros:0' shape=(?, 3) dtype=bool>, lengths=<tf.Tensor 'zeros_1:0' shape=(?, 3) dtype=int32>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_inputs, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_inputs\n",
    "#These are the word embeddings of the initial token (which for initialize should be from the encoder?)\n",
    "#Shape is [batch_size, embedding_size]\n",
    "#initial_state\n",
    "#This is the initial state of the decoder, a tuple consisting of \n",
    "#1. the LSTM cell and hiddens states\n",
    "#2. The attention states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When we pass time !=0 to inference decoder, even with initial step results we get non-zero parent ids\n",
    "\n",
    "#beam_search_output, beam_search_state, next_inputs, finished = \\\n",
    "#                                                inf_model_2.inference_decoder.step(time, first_inputs, initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run([first_inputs, initial_state, batch_size], \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user, input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user), len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 512)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with ops.name_scope(name, \"BeamSearchDecoderStep\", (time, first_inputs, initial_state)):\n",
    "    \n",
    "cell_state = initial_state.cell_state\n",
    "\n",
    "first_inputs = nest.map_structure(lambda inp: inf_model_2.inference_decoder._merge_batch_beams(inp, s=inp.shape[2:]), \n",
    "                                  first_inputs)\n",
    "\n",
    "cell_state = nest.map_structure(inf_model_2.inference_decoder._maybe_merge_batch_beams, cell_state, \n",
    "                                inf_model_2.inference_decoder._cell.state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run([first_inputs, initial_state], \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user, input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user), len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 512)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This goes through _cell first\n",
    "#The output of cell_outputs is also present in the next_cell_state, the cell_outputs is the attention\n",
    "\n",
    "#The cell_state includes, the cell and hidden states of the LSTM, the attention states, the alignment history, and timestep, \n",
    "#what does alignment history and attention states really mean? need to internalize...\n",
    "\n",
    "cell_outputs, next_cell_state = inf_model_2.inference_decoder._cell(first_inputs, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions convert the shape of the cell outputs and states from [batch_size, beam_width, embedding_size]\n",
    "#to [batch_size*beam_width, embedding_size], essentially, reducing a dimensionality by flattening the nested structure\n",
    "#the map structure fn is used because cell state is a complex tuple structure, \n",
    "#outputs is a simpe structure (should be able to just call flatten?)\n",
    "cell_outputs = nest.map_structure(lambda out: inf_model_2.inference_decoder._split_batch_beams(out, out.shape[1:]), \n",
    "                                  cell_outputs)\n",
    "\n",
    "next_cell_state = nest.map_structure(inf_model_2.inference_decoder._maybe_split_batch_beams, next_cell_state, \n",
    "                                     inf_model_2.inference_decoder._cell.state_size)\n",
    "\n",
    "if inf_model_2.inference_decoder._output_layer is not None:\n",
    "    cell_outputs = inf_model_2.inference_decoder._output_layer(cell_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run([cell_outputs, next_cell_state], \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-15.48709488,  -8.94940948,  -9.60930347, ...,   1.44873977,\n",
       "         -7.69063234, -10.08348083],\n",
       "       [-15.48709869,  -8.94941139,  -9.60930443, ...,   1.44873857,\n",
       "         -7.69063282, -10.08348083],\n",
       "       [-15.48709869,  -8.94941139,  -9.60930347, ...,   1.44873834,\n",
       "         -7.69063234, -10.08348179]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell_outputs_rnn_call, next_cell_state_rnn_call = inf_model_2.inference_decoder._cell.call(first_inputs, cell_state)\n",
    "\n",
    "#cell_outputs_rnn_call_call, next_cell_state_rnn_call_call = inf_model_2.inference_decoder._cell.call(cell_outputs_rnn_call, \n",
    "#                                                                                                     next_cell_state_rnn_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_batch_size = tensor_util.constant_value(batch_size)\n",
    "\n",
    "prediction_lengths = initial_state.lengths\n",
    "previously_finished = initial_state.finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token = data_formatting.EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_maybe(t):\n",
    "  if isinstance(t, tensor_array_ops.TensorArray):\n",
    "    raise TypeError(\n",
    "        \"TensorArray state is not supported by BeamSearchDecoder: %s\" % t.name)\n",
    "  if t.shape.ndims is None:\n",
    "    raise ValueError(\n",
    "        \"Expected tensor (%s) to have known rank, but ndims == None.\" % t)\n",
    "\n",
    "def _maybe_tensor_gather_helper(gather_indices, gather_from, batch_size,\n",
    "                                range_size, gather_shape):\n",
    "  \"\"\"Maybe applies _tensor_gather_helper.\n",
    "  This applies _tensor_gather_helper when the gather_from dims is at least as\n",
    "  big as the length of gather_shape. This is used in conjunction with nest so\n",
    "  that we don't apply _tensor_gather_helper to inapplicable values like scalars.\n",
    "  Args:\n",
    "    gather_indices: The tensor indices that we use to gather.\n",
    "    gather_from: The tensor that we are gathering from.\n",
    "    batch_size: The batch size.\n",
    "    range_size: The number of values in each range. Likely equal to beam_width.\n",
    "    gather_shape: What we should reshape gather_from to in order to preserve the\n",
    "      correct values. An example is when gather_from is the attention from an\n",
    "      AttentionWrapperState with shape [batch_size, beam_width, attention_size].\n",
    "      There, we want to preserve the attention_size elements, so gather_shape is\n",
    "      [batch_size * beam_width, -1]. Then, upon reshape, we still have the\n",
    "      attention_size as desired.\n",
    "  Returns:\n",
    "    output: Gathered tensor of shape tf.shape(gather_from)[:1+len(gather_shape)]\n",
    "      or the original tensor if its dimensions are too small.\n",
    "  \"\"\"\n",
    "  _check_maybe(gather_from)\n",
    "  if gather_from.shape.ndims >= len(gather_shape):\n",
    "    return _tensor_gather_helper(\n",
    "        gather_indices=gather_indices,\n",
    "        gather_from=gather_from,\n",
    "        batch_size=batch_size,\n",
    "        range_size=range_size,\n",
    "        gather_shape=gather_shape)\n",
    "  else:\n",
    "    return gather_from\n",
    "\n",
    "def _tensor_gather_helper(gather_indices, gather_from, batch_size,\n",
    "                          range_size, gather_shape):\n",
    "  \"\"\"Helper for gathering the right indices from the tensor.\n",
    "  This works by reshaping gather_from to gather_shape (e.g. [-1]) and then\n",
    "  gathering from that according to the gather_indices, which are offset by\n",
    "  the right amounts in order to preserve the batch order.\n",
    "  Args:\n",
    "    gather_indices: The tensor indices that we use to gather.\n",
    "    gather_from: The tensor that we are gathering from.\n",
    "    batch_size: The input batch size.\n",
    "    range_size: The number of values in each range. Likely equal to beam_width.\n",
    "    gather_shape: What we should reshape gather_from to in order to preserve the\n",
    "      correct values. An example is when gather_from is the attention from an\n",
    "      AttentionWrapperState with shape [batch_size, beam_width, attention_size].\n",
    "      There, we want to preserve the attention_size elements, so gather_shape is\n",
    "      [batch_size * beam_width, -1]. Then, upon reshape, we still have the\n",
    "      attention_size as desired.\n",
    "  Returns:\n",
    "    output: Gathered tensor of shape tf.shape(gather_from)[:1+len(gather_shape)]\n",
    "  \"\"\"\n",
    "  range_ = array_ops.expand_dims(math_ops.range(batch_size) * range_size, 1)\n",
    "  gather_indices = array_ops.reshape(gather_indices + range_, [-1])\n",
    "  output = array_ops.gather(\n",
    "      array_ops.reshape(gather_from, gather_shape), gather_indices)\n",
    "  final_shape = array_ops.shape(gather_from)[:1 + len(gather_shape)]\n",
    "  static_batch_size = tensor_util.constant_value(batch_size)\n",
    "  final_static_shape = (tensor_shape.TensorShape([static_batch_size])\n",
    "                        .concatenate(\n",
    "                            gather_from.shape[1:1 + len(gather_shape)]))\n",
    "  output = array_ops.reshape(output, final_shape)\n",
    "  output.set_shape(final_static_shape)\n",
    "  return output\n",
    "#Have to understand how this works!\n",
    "def _mask_probs(probs, eos_token, finished):\n",
    "  \"\"\"Masks log probabilities.\n",
    "  The result is that finished beams allocate all probability mass to eos and\n",
    "  unfinished beams remain unchanged.\n",
    "  Args:\n",
    "    probs: Log probabiltiies of shape `[batch_size, beam_width, vocab_size]`\n",
    "    eos_token: An int32 id corresponding to the EOS token to allocate\n",
    "      probability to.\n",
    "    finished: A boolean tensor of shape `[batch_size, beam_width]` that\n",
    "      specifies which\n",
    "      elements in the beam are finished already.\n",
    "  Returns:\n",
    "    A tensor of shape `[batch_size, beam_width, vocab_size]`, where unfinished\n",
    "    beams stay unchanged and finished beams are replaced with a tensor with all\n",
    "    probability on the EOS token.\n",
    "  \"\"\"\n",
    "  vocab_size = array_ops.shape(probs)[2]\n",
    "  finished_mask = array_ops.expand_dims(\n",
    "      math_ops.to_float(1. - math_ops.to_float(finished)), 2)\n",
    "  # These examples are not finished and we leave them\n",
    "  non_finished_examples = finished_mask * probs\n",
    "  # All finished examples are replaced with a vector that has all\n",
    "  # probability on EOS\n",
    "  finished_row = array_ops.one_hot(\n",
    "      eos_token,\n",
    "      vocab_size,\n",
    "      dtype=probs.dtype,\n",
    "      on_value=0.,\n",
    "      off_value=probs.dtype.min)\n",
    "  finished_examples = (1. - finished_mask) * finished_row\n",
    "  return finished_examples + non_finished_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = cell_outputs\n",
    "step_log_probs = nn_ops.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_15:0' shape=(?, 3, 17323) dtype=float32>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_14:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(step_log_probs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step_log_probs = _mask_probs(step_log_probs, end_token, previously_finished)\n",
    "total_probs = array_ops.expand_dims(initial_state.log_probs, 2) + step_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = logits.shape[-1].value or array_ops.shape(logits)[-1]\n",
    "\n",
    "lengths_to_add = array_ops.one_hot(\n",
    "      indices=array_ops.tile(\n",
    "          array_ops.reshape(end_token, [1, 1]), [batch_size, beam_width]),\n",
    "      depth=vocab_size,\n",
    "      on_value=0,\n",
    "      off_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run([total_probs, array_ops.expand_dims(initial_state.log_probs, 2), step_log_probs], \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-30.88870049, -10.23493576, -46.66865921, ..., -40.9866333 ,\n",
       "       -40.04515076, -29.67861557], dtype=float32)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[2][0][2] + q_0[1][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-39.28241348, -10.5441494 , -41.33394623, ..., -33.4271698 ,\n",
       "        -35.07487869, -32.33093262],\n",
       "       [-19.30293655,  -7.79374218, -34.72539139, ..., -33.93330765,\n",
       "        -27.5273056 , -22.93684769],\n",
       "       [-30.88870049, -10.23493576, -46.66865921, ..., -40.9866333 ,\n",
       "        -40.04515076, -29.67861557]], dtype=float32)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_mask = (1 - math_ops.to_int32(previously_finished))\n",
    "\n",
    "lengths_to_add = array_ops.expand_dims(add_mask, 2) * lengths_to_add\n",
    "\n",
    "new_prediction_lengths = (lengths_to_add + array_ops.expand_dims(prediction_lengths, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = total_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_shape = array_ops.shape(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ops.convert_to_tensor(time, name=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_flat = control_flow_ops.cond(time > 0,\n",
    "      lambda: array_ops.reshape(scores, [batch_size, -1]), lambda: scores[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_available_beam = control_flow_ops.cond(\n",
    "      time > 0, lambda: math_ops.reduce_prod(scores_shape[1:]),\n",
    "      lambda: math_ops.reduce_prod(scores_shape[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the next beams according to the specified successors function\n",
    "next_beam_size = math_ops.minimum(\n",
    "  ops.convert_to_tensor(beam_width, dtype=dtypes.int32, name=\"beam_width\"),\n",
    "  num_available_beam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_beam_scores, word_indices = nn_ops.top_k(scores_flat, k=next_beam_size)\n",
    "next_beam_scores.set_shape([static_batch_size, beam_width])\n",
    "word_indices.set_shape([static_batch_size, beam_width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_beam_probs = _tensor_gather_helper(\n",
    "      gather_indices=word_indices,\n",
    "      gather_from=scores,\n",
    "      batch_size=batch_size,\n",
    "      range_size=beam_width * vocab_size,\n",
    "      gather_shape=[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, word indices represents the positions of the flattened list of beams, which goes from 1 to beam_width*vocab_size\n",
    "#thus the word indices obtained are not the word indices of the 1 to N vocab but the flattened out list of logprobs from 1 to \n",
    "#beam_width*vocab_size\n",
    "next_word_ids = math_ops.to_int32(word_indices % vocab_size)\n",
    "#The beam_ids represent which beams these word indices (and thereby log prob values belong to), as\n",
    "#we have word indices extracted out of 1 to beam_width*vocab_size\n",
    "next_beam_ids = math_ops.to_int32(word_indices / vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run([next_word_ids, next_beam_ids, scores, scores_flat, next_beam_scores, next_beam_probs,\n",
    "                    word_indices, next_beam_size], \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-39.2824173 , -10.54414368, -41.33394623, ..., -33.4271698 ,\n",
       "        -35.07487488, -32.33093262],\n",
       "       [-19.30294037,  -7.7937398 , -34.72540283, ..., -33.93331528,\n",
       "        -27.52731133, -22.93685341],\n",
       "       [-30.88870049, -10.23493862, -46.66866684, ..., -40.98663712,\n",
       "        -40.04515076, -29.67861748]], dtype=float32)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.55750656, -1.63940859, -1.51259661], dtype=float32)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[3][0][np.argsort(q_0[3][0])[-3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-39.2824173 , -10.54414368, -41.33394623, ..., -33.4271698 ,\n",
       "        -35.07487488, -32.33093262], dtype=float32),\n",
       " array([-19.30294037,  -7.7937398 , -34.72540283, ..., -33.93331528,\n",
       "        -27.52731133, -22.93685341], dtype=float32),\n",
       " array([-30.88870049, -10.23493862, -46.66866684, ..., -40.98663712,\n",
       "        -40.04515076, -29.67861748], dtype=float32))"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[3][0][:vocab_size], q_0[3][0][vocab_size:vocab_size*2], q_0[3][0][vocab_size*2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.51259661, -1.63940859, -2.55750656]], dtype=float32),\n",
       " array([[-1.51259661, -1.63940859, -2.55750656]], dtype=float32),\n",
       " array([[-1.51259661, -1.63940859, -2.55750656]], dtype=float32))"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[4], q_0[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[15014,  3722, 16662]]), array([[0, 0, 1]]))"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[0], q_0[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_finished = _tensor_gather_helper(\n",
    "      gather_indices=next_beam_ids,\n",
    "      gather_from=previously_finished,\n",
    "      batch_size=batch_size,\n",
    "      range_size=beam_width,\n",
    "      gather_shape=[-1])\n",
    "\n",
    "next_finished = math_ops.logical_or(previously_finished,\n",
    "                                      math_ops.equal(next_word_ids, end_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_finished = math_ops.logical_or(previously_finished, math_ops.equal(next_word_ids, end_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_to_add = math_ops.to_int32(math_ops.not_equal(next_word_ids, end_token))\n",
    "lengths_to_add = (1 - math_ops.to_int32(next_finished)) * lengths_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run([previously_finished, next_finished, lengths_to_add], \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[False, False, False]], dtype=bool),\n",
       " array([[False, False, False]], dtype=bool),\n",
       " array([[1, 1, 1]])]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_prediction_len = _tensor_gather_helper(\n",
    "      gather_indices=next_beam_ids,\n",
    "      gather_from=initial_state.lengths,\n",
    "      batch_size=batch_size,\n",
    "      range_size=beam_width,\n",
    "      gather_shape=[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_prediction_len += lengths_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_1 = sess.run([next_prediction_len],\n",
    "                     feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1, 1, 1]])]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_cell_state = nest.map_structure(\n",
    "      lambda gather_from: _maybe_tensor_gather_helper(\n",
    "          gather_indices=next_beam_ids,\n",
    "          gather_from=gather_from,\n",
    "          batch_size=batch_size,\n",
    "          range_size=beam_width,\n",
    "          gather_shape=[batch_size * beam_width, -1]),\n",
    "      next_cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_1 = sess.run([next_cell_state],\n",
    "                     feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the returns, next_state and output to the step function, but how exactly do we transpose this back and push it\n",
    "#into cell? Moreover what is teh mechanics that determines when the beams become different? I.E. are no longer identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state = tf.contrib.seq2seq.BeamSearchDecoderState(\n",
    "      cell_state=next_cell_state,\n",
    "      log_probs=next_beam_probs,\n",
    "      lengths=next_prediction_len,\n",
    "      finished=next_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.contrib.seq2seq.BeamSearchDecoderOutput(\n",
    "      scores=next_beam_scores,\n",
    "      predicted_ids=next_word_ids,\n",
    "      parent_ids=next_beam_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_1 = sess.run([next_state, output, next_beam_scores],\n",
    "                     feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_state = next_state\n",
    "beam_search_output = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished = beam_search_state.finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = beam_search_output.predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_inputs = control_flow_ops.cond(math_ops.reduce_all(finished), \n",
    "                                    lambda: inf_model_2.inference_decoder._start_inputs,\n",
    "                                    lambda: inf_model_2.inference_decoder._embedding_fn(sample_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_1 = sess.run([next_inputs, beam_search_state],\n",
    "                     feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 512), (3, 512))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[0].shape, q_1[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3), (1, 3))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[1][3].shape, q_1[1][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3), (1, 3))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[1][2].shape, q_1[1][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3), (1, 3))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[1][1].shape, q_1[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q_0[1][0]), len(q_1[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next_inputs\n",
    "#These are the chosen word ids which we have performed the embedding look up for\n",
    "first_inputs = next_inputs[0]\n",
    "\n",
    "#next_states\n",
    "#these are the next states of the decoder network which pass back through the process\n",
    "initial_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished = finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "time =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
