{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import preprocessing\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "def stripLineBreak(x):\n",
    "    return x.replace('\\n', '')\n",
    "\n",
    "def createConversations(all_text):\n",
    "    conv = [[all_text[idx_text], all_text[idx_text+1]] for idx_text, text in enumerate(all_text[:-1])]\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = 'enron'\n",
    "#df_enron = pd.read_csv('../enron/Enron_Email_Threads_Clean_v2.csv')\n",
    "\n",
    "#df_enron_convs = df_enron.groupby('Subject')['CleanBody'].agg({'count', 'unique'}).reset_index()\n",
    "\n",
    "#df_enron_convs = df_enron_convs[df_enron_convs['count']>1]\n",
    "\n",
    "#conv_text = df_enron_convs['unique'].values\n",
    "\n",
    "#text_conversations = [pd.DataFrame(createConversations(all_text), columns=['Pair_0', 'Pair_1']) for all_text in conv_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = 'de_en'\n",
    "#t_enc = open(r'C:\\Users\\euix\\nmt_data\\wmt16_de_en\\train.en', 'r', encoding='utf8')\n",
    "#t_dec = open(r'C:\\Users\\euix\\nmt_data\\wmt16_de_en\\train.de', 'r', encoding='utf8')\n",
    "\n",
    "#t_enc = t_enc.readlines()\n",
    "#t_dec = t_dec.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'twitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = open('../../datasets/chat.txt', 'r', encoding='utf-8')\n",
    "\n",
    "twitter_lines = twitter_data.readlines()\n",
    "\n",
    "twitter_convs = [[line for idx, line in enumerate(twitter_lines) if idx%2==1], \n",
    "              [line for idx, line in enumerate(twitter_lines) if idx%2==0]]\n",
    "\n",
    "df_all = pd.DataFrame(list(zip(*twitter_convs)), columns=['Pair_0', 'Pair_1']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = open('../../datasets/chat_corpus/cleaned_corpus_en.txt', 'r', encoding='utf-8')\n",
    "\n",
    "twitter_lines = twitter_data.readlines()\n",
    "\n",
    "twitter_convs = [[line for idx, line in enumerate(twitter_lines) if idx%2==1], \n",
    "              [line for idx, line in enumerate(twitter_lines) if idx%2==0]]\n",
    "\n",
    "df_all = pd.DataFrame(list(zip(*twitter_convs)), columns=['Pair_0', 'Pair_1']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2601244, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab questions only\n",
    "df_all = df_all[(df_all['Pair_0'].str.contains('\\?'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all = pd.DataFrame(list(zip(t_enc, t_dec)), columns=['Pair_0', 'Pair_1'])\n",
    "#df_all = pd.concat(text_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all = df_all.sample(frac=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_all['Pair_0'] = df_all['Pair_0'].apply(preprocessing.remove_non_ascii)\n",
    "\n",
    "df_all['Pair_1'] = df_all['Pair_1'].apply(preprocessing.remove_non_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_all['Pair_0'] = df_all['Pair_0'].apply(stripLineBreak)\n",
    "\n",
    "df_all['Pair_1'] = df_all['Pair_1'].apply(stripLineBreak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 55541 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done 255853 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 333769 out of 333769 | elapsed:   14.4s finished\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 53356 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 230148 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done 333769 out of 333769 | elapsed:   13.2s finished\n"
     ]
    }
   ],
   "source": [
    "Pair_0_char_checked = Parallel(n_jobs=-1, verbose=8)(delayed(preprocessing.checkChars)(i) for i in df_all['Pair_0'].values)\n",
    "Pair_1_char_checked = Parallel(n_jobs=-1, verbose=8)(delayed(preprocessing.checkChars)(i) for i in df_all['Pair_1'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((333769, 2), 333769)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape, len(Pair_0_char_checked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_all['Pair_0_char_checked'] = Pair_0_char_checked\n",
    "\n",
    "df_all['Pair_1_char_checked'] = Pair_1_char_checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pair_0_char_checked = []\n",
    "Pair_1_char_checked = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 11351 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 37909 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 69887 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done 106201 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=-1)]: Done 147935 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done 194005 tasks      | elapsed:   29.5s\n",
      "[Parallel(n_jobs=-1)]: Done 245495 tasks      | elapsed:   35.1s\n",
      "[Parallel(n_jobs=-1)]: Done 301321 tasks      | elapsed:   40.9s\n",
      "[Parallel(n_jobs=-1)]: Done 333769 out of 333769 | elapsed:   43.9s finished\n"
     ]
    }
   ],
   "source": [
    "alpha_Pair_0_tokens = Parallel(n_jobs=-1, verbose=8)(\\\n",
    "                            delayed(preprocessing.checkAlphaLower)(i) for i in df_all['Pair_0_char_checked'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 10996 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 40788 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 76660 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 117396 tasks      | elapsed:   19.6s\n",
      "[Parallel(n_jobs=-1)]: Done 164212 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=-1)]: Done 215892 tasks      | elapsed:   28.7s\n",
      "[Parallel(n_jobs=-1)]: Done 273652 tasks      | elapsed:   34.4s\n",
      "[Parallel(n_jobs=-1)]: Done 333769 out of 333769 | elapsed:   39.7s finished\n"
     ]
    }
   ],
   "source": [
    "alpha_Pair_1_tokens = Parallel(n_jobs=-1, verbose=8)(\\\n",
    "                            delayed(preprocessing.checkAlphaLower)(i) for i in df_all['Pair_1_char_checked'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_all['alpha_Pair_0_tokens'] = list(zip(*alpha_Pair_0_tokens))[0]\n",
    "\n",
    "df_all['alpha_Pair_1_tokens'] = list(zip(*alpha_Pair_1_tokens))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_all['n_alpha_Pair_0_tokens'] = list(zip(*alpha_Pair_0_tokens))[1]\n",
    "\n",
    "df_all['n_alpha_Pair_1_tokens'] = list(zip(*alpha_Pair_1_tokens))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ \"if you are attacked maliciously, don't you want to defend yourself? if not, you're crazy.\",\n",
       "       'donald trump shames ex-miss universe for her sexual past in late-night twitter barrage:',\n",
       "       'if you are attacked maliciously  don t you want to defend yourself? if not  you re crazy.',\n",
       "       'donald trump shames ex miss universe for her sexual past in late night twitter barrage ',\n",
       "       list(['if', 'you', 'are', 'attacked', 'maliciously', 'don', 't', 'you', 'want', 'to', 'defend', 'yourself', '?', 'if', 'not', 'you', 're', 'crazy', '<EOS>']),\n",
       "       list(['donald', 'trump', 'shames', 'ex', 'miss', 'universe', 'for', 'her', 'sexual', 'past', 'in', 'late', 'night', 'twitter', 'barrage', '<EOS>']),\n",
       "       19, 16], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head().values[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_quote_tokens = []\n",
    "alpha_mcp_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[df_all['n_alpha_Pair_0_tokens'] != 0]\n",
    "\n",
    "df_all = df_all[df_all['n_alpha_Pair_1_tokens'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 15085 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done 63399 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 121573 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 187635 tasks      | elapsed:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done 263557 tasks      | elapsed:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done 333769 out of 333769 | elapsed:   29.5s finished\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 12637 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 50955 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 97093 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 149487 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=-1)]: Done 209701 tasks      | elapsed:   24.8s\n",
      "[Parallel(n_jobs=-1)]: Done 276171 tasks      | elapsed:   29.2s\n",
      "[Parallel(n_jobs=-1)]: Done 333769 out of 333769 | elapsed:   33.8s finished\n"
     ]
    }
   ],
   "source": [
    "alpha_stem_Pair_0_tokens = Parallel(n_jobs=-1, verbose=8)(\\\n",
    "                            delayed(preprocessing.nltkStem)(i) for i in df_all['alpha_Pair_0_tokens'].values)\n",
    "\n",
    "df_all['alpha_stem_Pair_0_tokens'] = list(zip(*alpha_stem_Pair_0_tokens))[0]\n",
    "df_all['n_alpha_stem_Pair_0_tokens'] = list(zip(*alpha_stem_Pair_0_tokens))[1]\n",
    "\n",
    "alpha_stem_Pair_1_tokens = Parallel(n_jobs=-1, verbose=8)(\\\n",
    "                            delayed(preprocessing.nltkStem)(i) for i in df_all['alpha_Pair_1_tokens'].values)\n",
    "\n",
    "df_all['alpha_stem_Pair_1_tokens'] = list(zip(*alpha_stem_Pair_1_tokens))[0]\n",
    "\n",
    "df_all['n_alpha_stem_Pair_1_tokens'] = list(zip(*alpha_stem_Pair_1_tokens))[1]\n",
    "\n",
    "alpha_stem_quote_tokens = []\n",
    "alpha_stem_quote_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 58148 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done 182373 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 333769 out of 333769 | elapsed:   17.6s finished\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 57728 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176769 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 333769 out of 333769 | elapsed:   18.1s finished\n"
     ]
    }
   ],
   "source": [
    "alpha_lem_Pair_0_tokens = Parallel(n_jobs=-1, verbose=8)(\\\n",
    "                            delayed(preprocessing.nltkLem)(i) for i in df_all['alpha_Pair_0_tokens'].values)\n",
    "\n",
    "df_all['alpha_lem_Pair_0_tokens'] = list(zip(*alpha_lem_Pair_0_tokens))[0]\n",
    "df_all['n_alpha_lem_Pair_0_tokens'] = list(zip(*alpha_lem_Pair_0_tokens))[1]\n",
    "\n",
    "alpha_lem_Pair_1_tokens = Parallel(n_jobs=-1, verbose=8)(\\\n",
    "                            delayed(preprocessing.nltkLem)(i) for i in df_all['alpha_Pair_1_tokens'].values)\n",
    "\n",
    "df_all['alpha_lem_Pair_1_tokens'] = list(zip(*alpha_lem_Pair_1_tokens))[0]\n",
    "\n",
    "df_all['n_alpha_lem_Pair_1_tokens'] = list(zip(*alpha_lem_Pair_1_tokens))[1]\n",
    "\n",
    "alpha_lem_quote_tokens = []\n",
    "alpha_lem_quote_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_pickle(\n",
    "                '../../datasets/%s_preprocess_pickle_stage_0_sample_%d_questions_only.pkl' % (dataset, df_all.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all[:int(df_all.shape[0]/2)].to_pickle(\n",
    "                '../../datasets/%s_preprocess_pickle_stage_0_sample_%d_part_1.pkl' % (dataset, df_all.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all[int(df_all.shape[0]/2):].to_pickle(\n",
    "                '../../datasets/%s_preprocess_pickle_stage_0_sample_%d_part_2.pkl' % (dataset, df_all.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_pickle('../../datasets/twitter_preprocess_pickle_stage_0_sample_333769_questions_only.pkl')\n",
    "\n",
    "df_all_1 = pd.read_pickle('../../datasets/twitter_preprocess_pickle_stage_0_sample_2601244_part_1.pkl')\n",
    "\n",
    "df_all_2 = pd.read_pickle('../../datasets/twitter_preprocess_pickle_stage_0_sample_2601244_part_2.pkl')\n",
    "\n",
    "df_all = pd.concat([df_all_1, df_all_2])\n",
    "\n",
    "df_all_1 = []\n",
    "df_all_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all = df_all.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2601244, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_pair_0 = [item for sublist in df_all['alpha_lem_Pair_0_tokens'].values for item in sublist]\n",
    "\n",
    "flat_list_pair_1 = [item for sublist in df_all['alpha_lem_Pair_1_tokens'].values for item in sublist]\n",
    "\n",
    "flat_list = flat_list_pair_0 + flat_list_pair_1\n",
    "\n",
    "fdist_quote = FreqDist(flat_list)\n",
    "\n",
    "words = fdist_quote.keys()\n",
    "\n",
    "word_counts = fdist_quote.values()\n",
    "\n",
    "df_all_word_counts = pd.DataFrame(list(zip(words, word_counts)), columns=['word', 'word_counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word_counts.sort_values(by=['word_counts'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_counts</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word\n",
       "word_counts      \n",
       "1              15\n",
       "2              21\n",
       "3              28\n",
       "4              40\n",
       "5              82"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_word_counts.groupby('word_counts').count().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.86600000e+04,   4.60000000e+01,   1.30000000e+01,\n",
       "          1.10000000e+01,   2.00000000e+00,   2.00000000e+00,\n",
       "          1.00000000e+00,   4.00000000e+00,   3.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          0.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   1.00000000e+00]),\n",
       " array([  1.00000000e+00,   7.05851400e+04,   1.41169280e+05,\n",
       "          2.11753420e+05,   2.82337560e+05,   3.52921700e+05,\n",
       "          4.23505840e+05,   4.94089980e+05,   5.64674120e+05,\n",
       "          6.35258260e+05,   7.05842400e+05,   7.76426540e+05,\n",
       "          8.47010680e+05,   9.17594820e+05,   9.88178960e+05,\n",
       "          1.05876310e+06,   1.12934724e+06,   1.19993138e+06,\n",
       "          1.27051552e+06,   1.34109966e+06,   1.41168380e+06,\n",
       "          1.48226794e+06,   1.55285208e+06,   1.62343622e+06,\n",
       "          1.69402036e+06,   1.76460450e+06,   1.83518864e+06,\n",
       "          1.90577278e+06,   1.97635692e+06,   2.04694106e+06,\n",
       "          2.11752520e+06,   2.18810934e+06,   2.25869348e+06,\n",
       "          2.32927762e+06,   2.39986176e+06,   2.47044590e+06,\n",
       "          2.54103004e+06,   2.61161418e+06,   2.68219832e+06,\n",
       "          2.75278246e+06,   2.82336660e+06,   2.89395074e+06,\n",
       "          2.96453488e+06,   3.03511902e+06,   3.10570316e+06,\n",
       "          3.17628730e+06,   3.24687144e+06,   3.31745558e+06,\n",
       "          3.38803972e+06,   3.45862386e+06,   3.52920800e+06]),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD8CAYAAACW/ATfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVID/B81XgA6MMAAyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e3b8ff2390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_all_word_counts['word_counts'].values, bins=50, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all_word_counts.to_pickle('%s_word_occurrence_sample_%d_lem_questions.pkl' % (dataset,  df_all.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_lengths = sum([list(df_all['n_alpha_lem_Pair_0_tokens'].values), list(df_all['n_alpha_lem_Pair_1_tokens'].values)], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(sent_lengths, log=True, bins=[i for i in range(70)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mcp_rare_words = Parallel(n_jobs=-1, verbose=8)(delayed(preprocessing.countWordOccurence)(i) \\\n",
    "#                                                for i in df_all['alpha_Pair_0_tokens'].values)\n",
    "\n",
    "#df_all['Pair_0_rare_words'] = mcp_rare_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mcp_rare_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_word_pair_0 = 'alpha_lem_Pair_0_tokens'\n",
    "check_word_pair_1 = 'alpha_lem_Pair_1_tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_type='lem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20, 1):\n",
    "    #func = lambda x: preprocessing.checkWordOccurrence(x, i)\n",
    "    func_length = lambda x: preprocessing.checkWordOccurrenceLength(x, i)\n",
    "\n",
    "    #df_all['alpha_%s_Pair_0_tokens_%d_word' % (processing_type, i+1)] = df_all['alpha_%s_Pair_0_tokens' % processing_type].apply(func)\n",
    "        \n",
    "    #df_all['alpha_%s_Pair_1_tokens_%d_word' % (processing_type, i+1)] = df_all['alpha_%s_Pair_1_tokens' % processing_type].apply(func)\n",
    "\n",
    "    df_all['n_alpha_%s_Pair_0_tokens_%d_word' % (processing_type, i+1)] = \\\n",
    "                df_all['alpha_%s_Pair_0_tokens' % processing_type].apply(func_length)\n",
    "\n",
    "    df_all['n_alpha_%s_Pair_1_tokens_%d_word' % (processing_type, i+1)] = \\\n",
    "               df_all['alpha_%s_Pair_1_tokens' % processing_type].apply(func_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[(df_all['n_alpha_lem_Pair_0_tokens']>2) & (df_all['n_alpha_lem_Pair_1_tokens']>2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[(df_all['n_alpha_lem_Pair_0_tokens']<26) & (df_all['n_alpha_lem_Pair_1_tokens']<26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2292441, 56)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[\n",
    "\t\t(df_all['n_alpha_lem_Pair_0_tokens_1_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_1_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_0_tokens_2_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_2_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_0_tokens_3_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_3_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_0_tokens_4_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_4_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_0_tokens_5_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_5_word']==0)  &\n",
    "\t\t(df_all['n_alpha_lem_Pair_0_tokens_6_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_6_word']==0)  &\n",
    "\t\t(df_all['n_alpha_lem_Pair_0_tokens_7_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_7_word']==0)  &\n",
    "    \t(df_all['n_alpha_lem_Pair_0_tokens_8_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_8_word']==0)  &\n",
    "    \t(df_all['n_alpha_lem_Pair_0_tokens_9_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_9_word']==0) & \n",
    "    \t(df_all['n_alpha_lem_Pair_0_tokens_10_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_10_word']==0)  &\n",
    "    \t(df_all['n_alpha_lem_Pair_0_tokens_11_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_11_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_0_tokens_12_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_12_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_0_tokens_13_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_13_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_0_tokens_14_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_14_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_0_tokens_15_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_15_word']==0)  &\n",
    "\t\t(df_all['n_alpha_lem_Pair_0_tokens_16_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_16_word']==0)  &\n",
    "\t\t(df_all['n_alpha_lem_Pair_0_tokens_17_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_17_word']==0)  &\n",
    "    \t(df_all['n_alpha_lem_Pair_0_tokens_18_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_18_word']==0)  &\n",
    "    \t(df_all['n_alpha_lem_Pair_0_tokens_19_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_19_word']==0) & \n",
    "    \t(df_all['n_alpha_lem_Pair_0_tokens_20_word']==0) &\n",
    "\t\t(df_all['n_alpha_lem_Pair_1_tokens_20_word']==0)  \n",
    "                   ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(df_all[df_all['Pair_0_rel_rare_word_count']>=0]['Pair_0_rel_rare_word_count'].values, log=True, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all[df_all['Pair_0_rel_rare_word_count']<0.0001].shape[0], df_all.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#float(df_all[df_all['Pair_0_rel_rare_word_count']<0.001].shape[0])/df_all.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all_no_rare = df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 25\n"
     ]
    }
   ],
   "source": [
    "seq_min = min(df_all['n_alpha_lem_Pair_0_tokens'].min(), df_all['n_alpha_lem_Pair_1_tokens'].min())\n",
    "seq_max = min(df_all['n_alpha_lem_Pair_0_tokens'].max(), df_all['n_alpha_lem_Pair_1_tokens'].max())\n",
    "\n",
    "print (seq_min, seq_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_counts = df_all_word_counts['word_counts'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(word_counts, log=True, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pair_0_words = set.union(*[set(i) for i in df_all['alpha_lem_Pair_0_tokens'].values])\n",
    "\n",
    "Pair_1_words = set.union(*[set(i) for i in df_all['alpha_lem_Pair_1_tokens'].values])\n",
    "\n",
    "vocab_set = list(Pair_0_words.union(Pair_1_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38749, (1764604, 56))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_set), df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done 10822 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 38749 out of 38749 | elapsed:   17.3s finished\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = Parallel(n_jobs=-1, verbose=8)(\\\n",
    "                    delayed(preprocessing.createDict)(word, word_idx) for word_idx, word in enumerate(vocab_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = { k: v for d in vocab_dict for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict['<PAD>'] = 0\n",
    "vocab_dict['<EOS>'] = 1\n",
    "vocab_dict['<UNK>'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeSent(sent):\n",
    "\n",
    "    if type(sent) == str: sent = sent.split(' ')\n",
    "    \n",
    "    return [vocab_dict[word] if word in vocab_dict else 2 for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['alpha_Pair_1_encoding'] =  df_all['alpha_lem_Pair_0_tokens'].apply(encodeSent)\n",
    "df_final['alpha_Pair_0_encoding'] = df_all['alpha_lem_Pair_1_tokens'].apply(encodeSent)\n",
    "\n",
    "df_final['Index'] = df_all.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_pickle(\\\n",
    "    '../processed_data/processed_data_v02_%s_py35_seq_length_%d_%d_sample_%d_questions.pkl' % (dataset, seq_min, seq_max, df_all.shape[0]))\n",
    "\n",
    "pickle.dump(vocab_dict, \\\n",
    "    open('../processed_data/word_dict_v02_%s_py35_seq_length_%d_%d_sample_%d_questions.pkl' % (dataset, seq_min, seq_max, df_all.shape[0]) , 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
