{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import helpers\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "def make_train_inputs(input_seq, target_seq):\n",
    "    inputs_, inputs_length_ = helpers.batch(input_seq)\n",
    "    targets_, targets_length_ = helpers.batch(target_seq)\n",
    "    \n",
    "    return {\n",
    "        'encoder_inputs:0': inputs_,\n",
    "        'encoder_inputs_length:0': inputs_length_,\n",
    "        'decoder_targets:0': targets_,\n",
    "        'decoder_targets_length:0': targets_length_,\n",
    "    }\n",
    "\n",
    "def make_inference_inputs(input_seq):\n",
    "    inputs_, inputs_length_ = helpers.batch(input_seq)\n",
    "\n",
    "    return {\n",
    "        'encoder_inputs:0': inputs_,\n",
    "        'encoder_inputs_length:0': inputs_length_,\n",
    "    }\n",
    "\n",
    "def numericEncode(sent):\n",
    "\n",
    "    if type(sent) == str: sent = sent.split(' ')\n",
    "    \n",
    "    return [vocab_dict[word] if word in vocab_dict else 2 for word in sent]\n",
    "\n",
    "def decodeSent(sent):\n",
    "    return [inv_map[i] for i in sent]\n",
    "\n",
    "df_all = pd.read_pickle('processed_data\\processed_data_v01_enron_py35_seq_length_3_49_sample_4256_limited_vocab.pkl')\n",
    "vocab_dict = pickle.load(open('dicts\\word_dict_v01_enron_py35_seq_length_3_49_sample_4256_limited_vocab.pkl', 'rb'))\n",
    "\n",
    "#Encode sequences\n",
    "df_all['alpha_Pair_1_encoding'] =  df_all['alpha_Pair_1_tokens'].apply(numericEncode)\n",
    "df_all['alpha_Pair_0_encoding'] = df_all['alpha_Pair_0_tokens'].apply(numericEncode)\n",
    "\n",
    "df_all['Index'] = df_all.index.values\n",
    "\n",
    "df_all_train = df_all.sample(frac=0.90, random_state=0)\n",
    "\n",
    "df_all_dev = df_all[df_all['Index'].isin(df_all_train['Index'].values) == False]\n",
    "\n",
    "inv_map = {v: k for k, v in vocab_dict.items()}\n",
    "\n",
    "dev_encoded_text = df_all_dev['alpha_Pair_0_encoding'].values\n",
    "dev_decoded_text = df_all_dev['alpha_Pair_1_encoding'].values\n",
    "\n",
    "\n",
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = len(vocab_dict) \n",
    "input_embedding_size = 256\n",
    "\n",
    "length_from = 3\n",
    "length_to = 49\n",
    "vocab_lower = 0\n",
    "vocab_upper = vocab_size\n",
    "n_batch_size = 32\n",
    "\n",
    "batches_in_epoch = df_all_train.shape[0]/n_batch_size\n",
    "\n",
    "n_cells = 128\n",
    "num_layers = 2\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "filename = 'seq2seq_enron_encode_128_decode_256_vocab_7239_embedding_256_seq_3_49_batch_32_layers_2_enkeep_10_dekeep_10-28679'\n",
    "tf.reset_default_graph()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\n",
    "session=tf.Session(config=tf.ConfigProto(\n",
    "  allow_soft_placement=True, log_device_placement=True))    \n",
    "#First let's load meta graph and restore weights\n",
    "# We import the meta graph in the current default Graph\n",
    "saver = tf.train.import_meta_graph('d:\\coding\\seq2seq\\chkpt\\\\' + filename + '.meta')\n",
    "\n",
    "# We restore the weights\n",
    "saver.restore(session, 'd:\\coding\\seq2seq\\chkpt\\\\' + filename)\n",
    "\n",
    "# Now, let's access and create placeholders variables and\n",
    "# create feed-dict to feed new data\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "#x0 = graph.get_tensor_by_name('encoder_inputs:0')\n",
    "#x1 = graph.get_tensor_by_name('encoder_inputs_length:0')\n",
    "#x2 = graph.get_tensor_by_name('decoder_targets:0')\n",
    "#x3 = graph.get_tensor_by_name('decoder_targets_length:0')\n",
    "\n",
    "\n",
    "#Now, access the op that you want to run. \n",
    "#op_to_restore = graph.get_tensor_by_name('Adam:0')\n",
    "op_to_restore1 = graph.get_tensor_by_name('Decoder/decoder_prediction_inference:0')\n",
    "op_to_restore2 = graph.get_tensor_by_name('Decoder/decoder_prediction_prob_inference:0')\n",
    "\n",
    "#print (session.run(op_to_restore, feed_dict))\n",
    "\n",
    "result = session.run([op_to_restore1, op_to_restore2], \\\n",
    "                make_inference_inputs([numericEncode(df_all['alpha_Pair_0_tokens'].values[0])]))\n",
    "\n",
    "\n",
    "session.close()\n",
    "\n",
    "[n.name for n in tf.get_default_graph().as_graph_def().node if 'inference' in n.name]\n",
    "\n",
    "df_all[['alpha_Pair_0_tokens', 'alpha_Pair_1_tokens']].to_csv('enron_token_pairs.csv', index=False, sep='|')\n",
    "\n",
    "q = ['i', 'may', 'have', 'to', 'start', 'training', 'now', 'for', 'that', 'and', 'show', 'you', 'a', 'little', 'what', 'partying', 'drinking', 'and', 'going', 'all', 'out', 'is', 'all', 'about', '<EOS>']\n",
    "\n",
    "' '.join(q)\n",
    "\n",
    "#make a POST request\n",
    "import requests\n",
    "dictToSend = {'string':' '.join(q)}\n",
    "res = requests.post('http://54.158.64.8:5000/api/predict', json=dictToSend)\n",
    "\n",
    "print ('response from server:', res.text)\n",
    "\n",
    "dictFromServer = res.json()\n",
    "\n",
    "dictFromServer\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    _ = tf.contrib.seq2seq\n",
    "\n",
    "    # We import the meta graph in the current default Graph\n",
    "    saver = tf.train.import_meta_graph('d:\\coding\\seq2seq\\chkpt\\seq2seq_enron_encode_128_decode_256_vocab_7239_embedding_256_seq_3_49_batch_32_layers_2_enkeep_10_dekeep_10-12019.meta')\n",
    "\n",
    "    # We restore the weights\n",
    "    saver.restore(sess, 'd:\\coding\\seq2seq\\chkpt\\seq2seq_enron_encode_128_decode_256_vocab_7239_embedding_256_seq_3_49_batch_32_layers_2_enkeep_10_dekeep_10-12019')\n",
    "    #[print(n.name) for n in tf.get_default_graph().as_graph_def().node]\n",
    "\n",
    "\n",
    "#    # We use a built-in TF helper to export variables to constants\n",
    "    output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "        sess, # The session is used to retrieve the weights\n",
    "        tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n",
    "        ['Decoder/dynamic_rnn_decoder/Decoder/while/attention_decoder/Softmax', \n",
    "         'Decoder/dynamic_rnn_decoder_1/Decoder/while/attention_decoder_fn_inference/Softmax'])\n",
    "                # The output node names are used to select the usefull nodes\n",
    " #   ) \n",
    "    \n",
    "    # Finally we serialize and dump the output graph to the filesystem\n",
    "    with tf.gfile.GFile('frozen_model.pb', \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "    \n",
    "    print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n",
    "    \n",
    "  #  with tf.gfile.GFile('frozen_model.pb', \"rb\") as f:\n",
    "        #graph_def = tf.GraphDef()\n",
    "        #graph_def.ParseFromString(f.read())\n",
    "        \n",
    "    #with tf.Graph().as_default() as graph:\n",
    "        # The name var will prefix every op/nodes in your graph\n",
    "        # Since we load everything in a new graph, this is not needed\n",
    "        #tf.import_graph_def(graph_def)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
