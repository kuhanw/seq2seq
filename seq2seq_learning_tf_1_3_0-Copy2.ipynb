{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kuhan Wang 17-09-15\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import helpers\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "def make_train_inputs(input_seq, target_seq):\n",
    "    inputs_, inputs_length_ = helpers.batch(input_seq)\n",
    "    targets_, targets_length_ = helpers.batch(target_seq)\n",
    "    \n",
    "    return {\n",
    "        encoder_inputs: inputs_,\n",
    "        encoder_inputs_length: inputs_length_,\n",
    "        decoder_targets: targets_,\n",
    "        decoder_targets_length: targets_length_,\n",
    "    }\n",
    "\n",
    "def make_inference_inputs(input_seq):\n",
    "    inputs_, inputs_length_ = helpers.batch(input_seq)\n",
    "\n",
    "    return {\n",
    "        encoder_inputs: inputs_,\n",
    "        encoder_inputs_length: inputs_length_,\n",
    "    }\n",
    "\n",
    "def encodeSent(sent):\n",
    "\n",
    "    if type(sent) == str: sent = sent.split(' ')\n",
    "    \n",
    "    return [vocab_dict[word] if word in vocab_dict else 2 for word in sent]\n",
    "\n",
    "def decodeSent(sent):\n",
    "    return [inv_map[i] for i in sent]\n",
    "\n",
    "def prepare_batch(seqs_x, maxlen=None):\n",
    "    # seqs_x: a list of sentences\n",
    "    lengths_x = [len(s) for s in seqs_x]\n",
    "    if maxlen is not None:\n",
    "        new_seqs_x = []\n",
    "        new_lengths_x = []\n",
    "        for l_x, s_x in zip(lengths_x, seqs_x):\n",
    "            if l_x <= maxlen:\n",
    "                new_seqs_x.append(s_x)\n",
    "                new_lengths_x.append(l_x)\n",
    "        lengths_x = new_lengths_x\n",
    "        seqs_x = new_seqs_x\n",
    "        \n",
    "        if len(lengths_x) < 1:\n",
    "            return None, None\n",
    "\n",
    "    batch_size = len(seqs_x)\n",
    "    x_lengths = np.array(lengths_x)\n",
    "    maxlen_x = np.max(x_lengths)\n",
    "    x = np.ones((batch_size, maxlen_x)).astype('int32') * PAD\n",
    "    for idx, s_x in enumerate(seqs_x):\n",
    "        x[idx, :lengths_x[idx]] = s_x\n",
    "    return x, x_lengths\n",
    "\n",
    "def prepare_train_batch(seqs_x, seqs_y, maxlen=None):\n",
    "    # seqs_x, seqs_y: a list of sentences\n",
    "    lengths_x = [len(s) for s in seqs_x]\n",
    "    lengths_y = [len(s) for s in seqs_y]\n",
    "\n",
    "    if maxlen is not None:\n",
    "        new_seqs_x = []\n",
    "        new_seqs_y = []\n",
    "        new_lengths_x = []\n",
    "        new_lengths_y = []\n",
    "        for l_x, s_x, l_y, s_y in zip(lengths_x, seqs_x, lengths_y, seqs_y):\n",
    "            if l_x <= maxlen and l_y <= maxlen:\n",
    "                new_seqs_x.append(s_x)\n",
    "                new_lengths_x.append(l_x)\n",
    "                new_seqs_y.append(s_y)\n",
    "                new_lengths_y.append(l_y)\n",
    "        lengths_x = new_lengths_x\n",
    "        seqs_x = new_seqs_x\n",
    "        lengths_y = new_lengths_y\n",
    "        seqs_y = new_seqs_y\n",
    "\n",
    "        if len(lengths_x) < 1 or len(lengths_y) < 1:\n",
    "            return None, None, None, None\n",
    "\n",
    "    batch_size = len(seqs_x)\n",
    "    \n",
    "    x_lengths = np.array(lengths_x)\n",
    "    y_lengths = np.array(lengths_y)\n",
    "\n",
    "    maxlen_x = np.max(x_lengths)\n",
    "    maxlen_y = np.max(y_lengths)\n",
    "\n",
    "    x = np.ones((batch_size, maxlen_x)).astype('int32') * PAD\n",
    "    y = np.ones((batch_size, maxlen_y)).astype('int32') * PAD\n",
    "    \n",
    "    for idx, [s_x, s_y] in enumerate(zip(seqs_x, seqs_y)):\n",
    "        x[idx, :lengths_x[idx]] = s_x\n",
    "        y[idx, :lengths_y[idx]] = s_y\n",
    "    return x, x_lengths, y, y_lengths\n",
    "\n",
    "def generateRandomSeqBatchMajor(length_from, length_to, vocab_lower, vocab_upper, batch_size):\n",
    "    return [\n",
    "            [random.randint(vocab_lower, vocab_upper-2) for digit in range(random.randint(length_from, length_to))] + [1]\n",
    "                for batch in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra vocabulary symbols\n",
    "_GO = '_GO'\n",
    "EOS = '_EOS' # also function as PAD\n",
    "UNK = '_UNK'\n",
    "\n",
    "extra_tokens = [_GO, EOS, UNK]\n",
    "\n",
    "start_token = extra_tokens.index(_GO)\t# start_token = 0\n",
    "end_token = extra_tokens.index(EOS)\t# end_token = 1\n",
    "unk_token = extra_tokens.index(UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all = pd.read_pickle('processed_data_v01_EN-DE_py35_seq_length_5_15_sample_540659_limited_vocab.pkl')\n",
    "#vocab_dict = pickle.load(open('word_dict_v01_EN-DE_py35_seq_length_5_15_sample_540659_limited_vocab.pkl', 'rb'))\n",
    "\n",
    "#df_all = pd.read_pickle('../processed_data/processed_data_v02_enron_py35_seq_length_3_49_sample_4256_limited_vocab.pkl')\n",
    "#vocab_dict = pickle.load(open('../processed_data/word_dict_v02_enron_py35_seq_length_3_49_sample_4256_limited_vocab.pkl', 'rb'))\n",
    "\n",
    "dataset = 'twitter'\n",
    "\n",
    "df_all = pd.read_pickle('../processed_data/processed_data_v02_twitter_py35_seq_length_3_19_sample_22028_lem.pkl')\n",
    "vocab_dict = pickle.load(open('../processed_data/word_dict_v02_twitter_py35_seq_length_3_19_sample_22028_lem.pkl', 'rb'))\n",
    "\n",
    "#Encode sequences\n",
    "#df_all['alpha_Pair_1_encoding'] =  df_all['alpha_Pair_1_tokens'].apply(numericEncode)\n",
    "#df_all['alpha_Pair_0_encoding'] = df_all['alpha_Pair_0_tokens'].apply(numericEncode)\n",
    "\n",
    "df_all['alpha_Pair_1_encoding'] =  df_all['alpha_Pair_1_tokens'].apply(encodeSent)\n",
    "df_all['alpha_Pair_0_encoding'] = df_all['alpha_Pair_0_tokens'].apply(encodeSent)\n",
    "\n",
    "df_all['Index'] = df_all.index.values\n",
    "\n",
    "df_all_train = df_all.sample(frac=0.90, random_state=0)\n",
    "\n",
    "df_all_dev = df_all[df_all['Index'].isin(df_all_train['Index'].values) == False]\n",
    "\n",
    "df_all_test = df_all_dev.sample(frac=0.10, random_state=0)\n",
    "\n",
    "df_all_dev = df_all_dev[df_all_dev['Index'].isin(df_all_test['Index'].values) == False]\n",
    "\n",
    "inv_map = {v: k for k, v in vocab_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_encoded_text = df_all_dev['alpha_Pair_0_encoding'].values\n",
    "dev_decoded_text = df_all_dev['alpha_Pair_1_encoding'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded_text = df_all_test['alpha_Pair_0_encoding'].values\n",
    "test_decoded_text = df_all_test['alpha_Pair_1_encoding'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15898, 5), (1590, 5), (177, 5), 17181)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_train.shape, df_all_dev.shape, df_all_test.shape, len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496.8125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "PAD = 0\n",
    "EOS = 1\n",
    "UNK = 2\n",
    "vocab_size = len(vocab_dict) + 1\n",
    "#vocab_size = 30\n",
    "input_embedding_size = 512\n",
    "\n",
    "length_from = 3\n",
    "length_to = 19\n",
    "vocab_lower = 0\n",
    "vocab_upper = vocab_size\n",
    "n_batch_size = 32\n",
    "\n",
    "batches_in_epoch = df_all_train.shape[0]/n_batch_size\n",
    "#batches_in_epoch=100\n",
    "n_cells = 128\n",
    "num_layers = 2\n",
    "\n",
    "n_epochs = 1000\n",
    "n_beam_width = 1\n",
    "\n",
    "encoder_output_keep = 1\n",
    "decoder_output_keep = 1\n",
    "\n",
    "batches_in_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_batches = helpers.random_sequences(length_from=length_from, length_to=length_to,\n",
    "                                       vocab_lower=vocab_lower, vocab_upper=vocab_size,\n",
    "                                       batch_size=n_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create handles for encoder and decoders\n",
    "encoder_inputs = tf.placeholder(\n",
    "            shape=(None, None),\n",
    "            dtype=tf.int32,\n",
    "            name='encoder_inputs',\n",
    "        )\n",
    "\n",
    "encoder_inputs_length = tf.placeholder(\n",
    "            shape=(None,),\n",
    "            dtype=tf.int32,\n",
    "            name='encoder_inputs_length',\n",
    "        )\n",
    "\n",
    "# required for training, not required for testing\n",
    "decoder_targets = tf.placeholder(\n",
    "            shape=(None, None),\n",
    "            dtype=tf.int32,\n",
    "            name='decoder_targets'\n",
    "        )\n",
    "\n",
    "decoder_targets_length = tf.placeholder(\n",
    "            shape=(None,),\n",
    "            dtype=tf.int32,\n",
    "            name='decoder_targets_length',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = tf.shape(encoder_inputs)[0]\n",
    "\n",
    "#Make EOS and PAD matrices to concatenate with targets\n",
    "EOS_SLICE = tf.ones([batch_size, 1], dtype=tf.int32) * EOS\n",
    "PAD_SLICE = tf.ones([batch_size, 1], dtype=tf.int32) * PAD\n",
    "\n",
    "#Adding EOS to the beginning of the decoder targets\n",
    "decoder_train_inputs = tf.concat([EOS_SLICE, decoder_targets], axis=1, name='decoder_train_inputs_concat')\n",
    "#[1,10], [10, 16]\n",
    "decoder_train_length = decoder_targets_length + 1\n",
    "\n",
    "decoder_train_targets = tf.concat([decoder_targets, PAD_SLICE], axis=1, name='decoder_train_targets')\n",
    "\n",
    "max_decoder_length = tf.reduce_max(decoder_train_length)\n",
    "\n",
    "#Create word embeddings\n",
    "sqrt3 = math.sqrt(3)\n",
    "initializer = tf.random_uniform_initializer(-sqrt3, sqrt3)\n",
    "\n",
    "#Randomly initialize a embedding vector for each term in the vocabulary\n",
    "embedding_matrix = tf.get_variable(name=\"embedding_matrix\", shape=[vocab_size, input_embedding_size],\n",
    "                                   initializer=initializer, \n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "#Map each input unit to a column in the embedding matrix\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embedding_matrix, encoder_inputs)\n",
    "\n",
    "decoder_train_inputs_embedded = tf.nn.embedding_lookup(embedding_matrix, decoder_train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_layer = Dense(n_cells, name='input_projection')\n",
    "\n",
    "# Output projection layer to convert cell_outputs to logits\n",
    "output_layer = Dense(vocab_size, name='output_projection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a bi-directional encoder, encoding the forward and backward states\n",
    "#The core abstraction is in tf.nn.bidirectional_dynamic_rnn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_cell_list = []\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(n_cells, state_is_tuple=True), input_keep_prob=1, \n",
    "                                            output_keep_prob=encoder_output_keep)\n",
    "    encoder_cell_list.append(cell)\n",
    "\n",
    "encoder_cell =  tf.contrib.rnn.MultiRNNCell(encoder_cell_list)\n",
    "\n",
    "encoder_outputs, encoder_last_state = tf.nn.dynamic_rnn(\n",
    "        cell=encoder_cell, inputs=encoder_inputs_embedded,\n",
    "        sequence_length=encoder_inputs_length, dtype=tf.float32,\n",
    "        time_major=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "    num_units=n_cells, \n",
    "    memory=encoder_outputs, \n",
    "    memory_sequence_length=encoder_inputs_length) \n",
    "\n",
    "decoder_cell_list = []\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(n_cells, state_is_tuple=True), input_keep_prob=1, \n",
    "                                            output_keep_prob=decoder_output_keep)\n",
    "    decoder_cell_list.append(cell)\n",
    "\n",
    "#decoder_initial_state = encoder_last_state\n",
    "\n",
    "decoder_cell_list[-1] = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            cell=decoder_cell_list[-1],\n",
    "            attention_mechanism=attention_mechanism,\n",
    "            attention_layer_size=n_cells,\n",
    "          #  cell_input_fn=attn_decoder_input_fn,\n",
    "            #initial_cell_state=encoder_last_state[-1],\n",
    "            initial_cell_state=encoder_last_state[-1],                   \n",
    "            alignment_history=False,\n",
    "            name='Attention_Wrapper')\n",
    "\n",
    "initial_state = [state for state in encoder_last_state]\n",
    "\n",
    "initial_state[-1] = decoder_cell_list[-1].zero_state(batch_size=batch_size*n_beam_width, dtype=tf.float32)\n",
    "\n",
    "decoder_initial_state = tuple(initial_state)\n",
    "\n",
    "decoder_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list)\n",
    "\n",
    "#encoder_cell.state_size, encoder_inputs_embedded, encoder_inputs_length\n",
    "\n",
    "# Helper to feed inputs for training: read inputs from dense ground truth vectors\n",
    "training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_train_inputs_embedded,\n",
    "                                   sequence_length=decoder_train_length,\n",
    "                                   time_major=False,\n",
    "                                   name='training_helper')\n",
    "\n",
    "training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell,\n",
    "                                   helper=training_helper,\n",
    "                                   initial_state=decoder_initial_state, \n",
    "                                   output_layer=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                                    num_units=n_cells, \n",
    "                                    memory=encoder_outputs, \n",
    "                                    memory_sequence_length=encoder_inputs_length)\n",
    "\n",
    "decoder_cell_list = []\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(n_cells, state_is_tuple=True), input_keep_prob=1, \n",
    "                                                    output_keep_prob=decoder_output_keep)\n",
    "    decoder_cell_list.append(cell)\n",
    "\n",
    "#decoder_initial_state = encoder_last_state\n",
    "\n",
    "#Last layer of decoders is wrapped in attention\n",
    "decoder_cell_list[-1] = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                                 cell=decoder_cell_list[-1],\n",
    "                                 attention_mechanism=attention_mechanism,\n",
    "                                 attention_layer_size=n_cells,\n",
    "                                 #  cell_input_fn=attn_decoder_input_fn,\n",
    "                                 initial_cell_state=encoder_last_state[-1],                   \n",
    "                                 alignment_history=False,\n",
    "                                 name='Attention_Wrapper')\n",
    "\n",
    "initial_state = [state for state in encoder_last_state]\n",
    "\n",
    "initial_state[-1] = decoder_cell_list[-1].zero_state(batch_size=batch_size*n_beam_width, dtype=tf.float32)\n",
    "\n",
    "decoder_initial_state = tuple(initial_state)\n",
    "\n",
    "decoder_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list)\n",
    "\n",
    "training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_train_inputs_embedded,\n",
    "                           sequence_length=decoder_train_length,\n",
    "                           time_major=False,\n",
    "                           name='training_helper')\n",
    "\n",
    "training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell,\n",
    "                                                   helper=training_helper,\n",
    "                                                   initial_state=decoder_initial_state, \n",
    "                                                   output_layer=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(decoder_outputs_train, decoder_last_state_train, decoder_outputs_length_train) = \\\n",
    "      (tf.contrib.seq2seq.dynamic_decode(\n",
    "                                        decoder=training_decoder,\n",
    "                                        output_time_major=False,\n",
    "                                        impute_finished=True,\n",
    "                                        maximum_iterations=max_decoder_length\n",
    "                                        )\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "    num_units=n_cells, \n",
    "    memory=encoder_outputs, \n",
    "    memory_sequence_length=encoder_inputs_length) \n",
    "\n",
    "decoder_cell_list = []\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(n_cells, state_is_tuple=True), input_keep_prob=1, \n",
    "                                            output_keep_prob=decoder_output_keep)\n",
    "    decoder_cell_list.append(cell)\n",
    "\n",
    "decoder_initial_state = encoder_last_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_input_feeding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_decoder_input_fn(inputs, attention):\n",
    "    if not attn_input_feeding:\n",
    "        return inputs\n",
    "\n",
    "    # Essential when use_residual=True\n",
    "    _input_layer = Dense(n_cells, dtype=tf.float32,\n",
    "                         name='attn_input_feeding')\n",
    "    return _input_layer(array_ops.concat([inputs, attention], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_cell_list[-1] = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            cell=decoder_cell_list[-1],\n",
    "            attention_mechanism=attention_mechanism,\n",
    "            attention_layer_size=n_cells,\n",
    "          #  cell_input_fn=attn_decoder_input_fn,\n",
    "            #initial_cell_state=encoder_last_state[-1],\n",
    "            initial_cell_state=encoder_last_state[-1],                   \n",
    "            alignment_history=False,\n",
    "            name='Attention_Wrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = [state for state in encoder_last_state]\n",
    "\n",
    "initial_state[-1] = decoder_cell_list[-1].zero_state(batch_size=batch_size*n_beam_width, dtype=tf.float32)\n",
    "\n",
    "decoder_initial_state = tuple(initial_state)\n",
    "\n",
    "decoder_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_cell.state_size, encoder_inputs_embedded, encoder_inputs_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to feed inputs for training: read inputs from dense ground truth vectors\n",
    "training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_train_inputs_embedded,\n",
    "                                   sequence_length=decoder_train_length,\n",
    "                                   time_major=False,\n",
    "                                   name='training_helper')\n",
    "\n",
    "training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell,\n",
    "                                   helper=training_helper,\n",
    "                                   initial_state=decoder_initial_state, \n",
    "                                   output_layer=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "(decoder_outputs_train, decoder_last_state_train, decoder_outputs_length_train) = \\\n",
    "              (tf.contrib.seq2seq.dynamic_decode(\n",
    "                                                decoder=training_decoder,\n",
    "                                                output_time_major=False,\n",
    "                                                impute_finished=True,\n",
    "                                                maximum_iterations=max_decoder_length\n",
    "                                                )\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More efficient to do the projection on the batch-time-concatenated tensor\n",
    "# logits_train: [batch_size, max_time_step + 1, num_decoder_symbols]\n",
    "# self.decoder_logits_train = output_layer(self.decoder_outputs_train.rnn_output)\n",
    "decoder_logits_train = tf.identity(decoder_outputs_train.rnn_output) \n",
    "\n",
    "# Use argmax to extract decoder symbols to emit\n",
    "decoder_pred_train = tf.argmax(decoder_logits_train, axis=-1, name='decoder_pred_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.ones([batch_size*n_beam_width,], tf.int32) * EOS\n",
    "end_token = EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to feed inputs for greedy decoding: uses the argmax of the output\n",
    "decoding_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(start_tokens=start_tokens,\n",
    "                                                end_token=end_token,\n",
    "                                                embedding=embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic decoder performs greedy decoding at each time step\n",
    "inference_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell,\n",
    "                                         helper=decoding_helper,\n",
    "                                         initial_state=decoder_initial_state,\n",
    "                                         output_layer=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell=decoder_cell,\n",
    "#                                                               embedding=embedding_matrix,\n",
    "#                                                               start_tokens=start_tokens,\n",
    "#                                                               end_token=end_token,\n",
    "#                                                               initial_state=decoder_initial_state,\n",
    "#                                                               beam_width=n_beam_width,\n",
    "#                                                               output_layer=output_layer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_decode_step = tf.reduce_max(encoder_inputs_length) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "(decoder_outputs_decode, decoder_last_state_decode,\n",
    "         decoder_outputs_length_decode) = (tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder=inference_decoder,\n",
    "            output_time_major=False,\n",
    "            #impute_finished=True,\t# error occurs --why?\n",
    "            maximum_iterations=max_decode_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_pred_decode = tf.argmax(decoder_outputs_decode.rnn_output, axis=-1, name='decoder_pred_decode')\n",
    "\n",
    "decoder_pred_decode_prob = tf.nn.softmax(decoder_outputs_decode.rnn_output, name='decoder_pred_decode_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder_pred_decode_prob = tf.nn.softmax(decoder_outputs_decode.rnn_output, name='decoder_pred_decode_prob')\n",
    "#decoder_pred_decode_prob = decoder_outputs_decode.beam_search_decoder_output.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks: masking for valid and padded time steps, [batch_size, max_time_step + 1]\n",
    "masks = tf.sequence_mask(lengths=decoder_train_length, \n",
    "                         maxlen=max_decoder_length, dtype=tf.float32, name='masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes per word average cross-entropy over a batch\n",
    "# Internally calls 'nn_ops.sparse_softmax_cross_entropy_with_logits' by default\n",
    "loss = tf.contrib.seq2seq.sequence_loss(logits=decoder_logits_train, \n",
    "                                  targets=decoder_train_targets,\n",
    "                                  weights=masks,\n",
    "                                  average_across_timesteps=True,\n",
    "                                  average_across_batch=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=decoder_train_targets,\n",
    "    logits=decoder_logits_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = df_all_train['alpha_Pair_0_encoding'].values\n",
    "decoded_text = df_all_train['alpha_Pair_1_encoding'].values\n",
    "text_index = df_all_train['Index'].values\n",
    "\n",
    "input_batches = ([encoded_text[block_idx*n_batch_size:(block_idx+1)*n_batch_size], \n",
    "         decoded_text[block_idx*n_batch_size:(block_idx+1)*n_batch_size], \n",
    "                 text_index[block_idx*n_batch_size:(block_idx+1)*n_batch_size]]\\\n",
    "            for block_idx in range(len(encoded_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_seq = generateRandomSeqBatchMajor(length_from=length_from, length_to=length_to,\n",
    "                                       vocab_lower=2, vocab_upper=vocab_upper,\n",
    "                                       batch_size=n_batch_size)\n",
    "input_batch_data = ran_seq\n",
    "target_batch_data = input_batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17182"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fd = make_train_inputs(input_batch_data, target_batch_data)\n",
    "fd = prepare_train_batch([[4, 5, 7, 2, 5, 1], [4, 5, 7, 4, 5, 1]], [[4, 5, 7, 2, 30, 1],[4, 5, 7, 9, 5, 1]])\n",
    "feed_dict = {encoder_inputs: fd[0],\n",
    "        encoder_inputs_length: fd[1],\n",
    "        decoder_targets: fd[2],\n",
    "        decoder_targets_length: fd[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    for i in range(1):\n",
    "        print (i)\n",
    "        #epoch_batches = next(input_batches)\n",
    "\n",
    "        #input_batch_data = epoch_batches[0]\n",
    "        #target_batch_data = epoch_batches[1]\n",
    "        #batch_data_index = epoch_batches[2]\n",
    "        #print ([inv_map[i] for i in input_batch_data[0]])\n",
    "       #fd = prepare_train_batch(input_batch_data, target_batch_data)\n",
    "        \n",
    "        #feed_dict = {encoder_inputs: fd[0],\n",
    "        #encoder_inputs_length: fd[1],\n",
    "        #decoder_targets: fd[2],\n",
    "        #decoder_targets_length: fd[3]}\n",
    "        \n",
    "        t = session.run([loss], feed_dict)\n",
    "        #y  = session.run([encoder_outputs_original, encoder_outputs], feed_dict)\n",
    "        #if t[1] !=t[1]: \n",
    "        #    print (loss)\n",
    "         #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.001\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, \\\n",
    "                                           n_epochs*int(batches_in_epoch), 0.0001, staircase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_task = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 1\n",
      "learning rate 0.000999982\n",
      "epoch 0\n",
      "batch 0\n",
      "training minibatch loss: 9.750731468200684\n",
      "  sample 1:\n",
      "    enc input           > i think they gon na go straight to the it s the th anniversary of the iphone <EOS>\n",
      "    dec input           > i m not sure if i want the or to wait for s <EOS>\n",
      "    dec train predicted > \n",
      "dev minibatch loss: 9.74228286743164\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > just think in month he will be off the stage thank you lord <EOS>\n",
      "    DEV dec input           > on stage on the importance of investing in africa amp inspiring entrepreneur in africa u <EOS>\n",
      "    DEV dec train predicted > plant plant is desean is advice advice advice alright dutch shimmering shimmering tinder newsworthy tinder quesadilla\n",
      "    DEV dec train infer > plant plant plant a capitalist capitalist capitalist capitalist alright alright socal socal praised tinder tinder tinder audi audi audi establishes\n",
      "global_step: 51\n",
      "learning rate 0.000999053\n",
      "epoch 0\n",
      "batch 50\n",
      "training minibatch loss: 6.624457359313965\n",
      "  sample 1:\n",
      "    enc input           > lol i m just messing with you <EOS>\n",
      "    dec input           > that s my gov name though <EOS>\n",
      "    dec train predicted > <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.594655990600586\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > gold plated fixture installers <EOS>\n",
      "    DEV dec input           > i think marble installers is the nail in the coffin tonight <EOS>\n",
      "    DEV dec train predicted > \n",
      "    DEV dec train infer > \n",
      "global_step: 101\n",
      "learning rate 0.000998126\n",
      "epoch 0\n",
      "batch 100\n",
      "training minibatch loss: 6.226045608520508\n",
      "  sample 1:\n",
      "    enc input           > sorry that wa me <EOS>\n",
      "    dec input           > why are these scene kid making whale noise a they walk down main street ? ? ? ? <EOS>\n",
      "    dec train predicted > i i <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.119852066040039\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you were supposed to be on electronic blackout tonight what happened ? <EOS>\n",
      "    DEV dec input           > turn out he actually didnt prepare pity these thing arent thirty minute longer <EOS>\n",
      "    DEV dec train predicted > i i i i <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i i i i <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 151\n",
      "learning rate 0.0009972\n",
      "epoch 0\n",
      "batch 150\n",
      "training minibatch loss: 6.110386848449707\n",
      "  sample 1:\n",
      "    enc input           > even though i am much younger than she is <EOS>\n",
      "    dec input           > is the kris jenner of wine critic ? <EOS>\n",
      "    dec train predicted > i i i <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.3192057609558105\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > but that s how he roll being the dumbass <EOS>\n",
      "    DEV dec input           > damn right we all know it except him <EOS>\n",
      "    DEV dec train predicted > i i i <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i i i <EOS>\n",
      "global_step: 201\n",
      "learning rate 0.000996275\n",
      "epoch 0\n",
      "batch 200\n",
      "training minibatch loss: 6.176146030426025\n",
      "  sample 1:\n",
      "    enc input           > ta da another retweet amp true coolness abounds thanks so much grady visit u for some goody <EOS>\n",
      "    dec input           > grazie gracias danka merci spoceba amp thanks for the retweet grady we have music book <EOS>\n",
      "    dec train predicted > i the the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.956169605255127\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you should close your account because they got busted for fraud <EOS>\n",
      "    DEV dec input           > because i bank with them haha <EOS>\n",
      "    DEV dec train predicted > i i the <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i i the the <EOS>\n",
      "global_step: 251\n",
      "learning rate 0.00099535\n",
      "epoch 0\n",
      "batch 250\n",
      "training minibatch loss: 5.8444390296936035\n",
      "  sample 1:\n",
      "    enc input           > which mean absolutely nothing <EOS>\n",
      "    dec input           > better start for team usa in the second <EOS>\n",
      "    dec train predicted > i the the the <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.309217929840088\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > ah i see kind of reminds me of those people mover at the airport fun to ride <EOS>\n",
      "    DEV dec input           > yi they are guided with guide wheel and the bus just flow along like going through rail <EOS>\n",
      "    DEV dec train predicted > i you the the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i the the the <EOS>\n",
      "global_step: 301\n",
      "learning rate 0.000994426\n",
      "epoch 0\n",
      "batch 300\n",
      "training minibatch loss: 5.94015645980835\n",
      "  sample 1:\n",
      "    enc input           > you were robbed it s ok denis we all know how fantastic is let s dance <EOS>\n",
      "    dec input           > what i do when the forget to invite me to the party <EOS>\n",
      "    dec train predicted > i you the you you the <EOS> the <EOS> <EOS> the <EOS> <EOS>\n",
      "dev minibatch loss: 6.076848030090332\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > pandering clinton she s a joke <EOS>\n",
      "    DEV dec input           > this is a man who ha called woman pig and liar <EOS>\n",
      "    DEV dec train predicted > i the the the <EOS> the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i the the the <EOS>\n",
      "global_step: 351\n",
      "learning rate 0.000993503\n",
      "epoch 0\n",
      "batch 350\n",
      "training minibatch loss: 5.901059627532959\n",
      "  sample 1:\n",
      "    enc input           > not that hard <EOS>\n",
      "    dec input           > how would you suggest i try to verify that claim <EOS>\n",
      "    dec train predicted > i i the the <EOS> the <EOS> the <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.733013153076172\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the power of the moon is real very no denying it <EOS>\n",
      "    DEV dec input           > when strange behavior become worse and you look at the calendar and this in two day <EOS>\n",
      "    DEV dec train predicted > i i the the <EOS> <EOS> the the <EOS> <EOS> the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i the the the <EOS>\n",
      "global_step: 401\n",
      "learning rate 0.000992581\n",
      "epoch 0\n",
      "batch 400\n",
      "training minibatch loss: 6.239396572113037\n",
      "  sample 1:\n",
      "    enc input           > so many weird sunburn forthcoming <EOS>\n",
      "    dec input           > it is glorious day sunscreen those bare butt my folsom bound friend <EOS>\n",
      "    dec train predicted > i i the <EOS> <EOS> <EOS> the <EOS> <EOS> the <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.737138271331787\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > this is one for black jesus <EOS>\n",
      "    DEV dec input           > yep and somehow that wa made okay come on jesus <EOS>\n",
      "    DEV dec train predicted > i i the the i the <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m the the the the <EOS>\n",
      "global_step: 451\n",
      "learning rate 0.00099166\n",
      "epoch 0\n",
      "batch 450\n",
      "training minibatch loss: 5.558946132659912\n",
      "  sample 1:\n",
      "    enc input           > i came in with legging before honestly but idk fam lp is starting to be more extra <EOS>\n",
      "    dec input           > they re getting worse with the checkup though <EOS>\n",
      "    dec train predicted > i m the the <EOS> the the <EOS> <EOS>\n",
      "dev minibatch loss: 5.840991497039795\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i just buy myself stuff from the guy section in nike <EOS>\n",
      "    DEV dec input           > i just want a boyfriend so i can wear his sweat shirt and sweat pant <EOS>\n",
      "    DEV dec train predicted > i m the to the <EOS> the m s the the <EOS> <EOS> the <EOS> <EOS>\n",
      "    DEV dec train infer > i m the the the the <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 497\n",
      "learning rate 0.000990814\n",
      "epoch 1\n",
      "batch 0\n",
      "training minibatch loss: 5.383382320404053\n",
      "  sample 1:\n",
      "    enc input           > wow u r right on brother <EOS>\n",
      "    dec input           > the church and homosexuality <EOS>\n",
      "    dec train predicted > i the to i <EOS>\n",
      "dev minibatch loss: 6.0622944831848145\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you just said it <EOS>\n",
      "    DEV dec input           > so i guess half of american are idiot then ? or a hillary like to call them deplorables <EOS>\n",
      "    DEV dec train predicted > i is m to <EOS> the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> the <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m the the the the <EOS>\n",
      "global_step: 547\n",
      "learning rate 0.000989894\n",
      "epoch 1\n",
      "batch 50\n",
      "training minibatch loss: 5.7381205558776855\n",
      "  sample 1:\n",
      "    enc input           > thank you <EOS>\n",
      "    dec input           > happy birthday johnny <EOS>\n",
      "    dec train predicted > i birthday to <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 5.753232479095459\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > no fool you are not that lit <EOS>\n",
      "    DEV dec input           > my birthday is the day project x come out coincidence ? ? <EOS>\n",
      "    DEV dec train predicted > i the to the the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m the the the the the the <EOS>\n",
      "global_step: 597\n",
      "learning rate 0.000988975\n",
      "epoch 1\n",
      "batch 100\n",
      "training minibatch loss: 5.988624572753906\n",
      "  sample 1:\n",
      "    enc input           > we are doomed then ugh goodnight <EOS>\n",
      "    dec input           > total resistance came from the people the offered no resistance and the court went along <EOS>\n",
      "    dec train predicted > i the <EOS> <EOS> the the <EOS> the <EOS> <EOS> <EOS> the the <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.713786602020264\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > thanks for the rt <EOS>\n",
      "    DEV dec input           > rt reach for the sky <EOS>\n",
      "    DEV dec train predicted > i is the the the <EOS>\n",
      "    DEV dec train infer > i m the the the the <EOS> <EOS>\n",
      "global_step: 647\n",
      "learning rate 0.000988058\n",
      "epoch 1\n",
      "batch 150\n",
      "training minibatch loss: 5.430532455444336\n",
      "  sample 1:\n",
      "    enc input           > worse playoff race ever <EOS>\n",
      "    dec input           > and we re already in the giant bullpen <EOS>\n",
      "    dec train predicted > i is m a <EOS> the <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.871220588684082\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > but then navas and san i mean whatever pep s got this <EOS>\n",
      "    DEV dec input           > lm probably <EOS>\n",
      "    DEV dec train predicted > i is a\n",
      "    DEV dec train infer > i m a a a a a a a a <EOS>\n",
      "global_step: 697\n",
      "learning rate 0.000987141\n",
      "epoch 1\n",
      "batch 200\n",
      "training minibatch loss: 5.489614486694336\n",
      "  sample 1:\n",
      "    enc input           > i feared that people would think san fran but it wa too good a name to pas up <EOS>\n",
      "    dec input           > i ll allow it <EOS>\n",
      "    dec train predicted > i m be to to\n",
      "dev minibatch loss: 5.774694442749023\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > wake county and nc can t afford not investing in transit infrastructure <EOS>\n",
      "    DEV dec input           > more use sale tax amp fee that ask more from folk of many who can not afford <EOS>\n",
      "    DEV dec train predicted > i is a a <EOS> a <EOS> s <EOS> <EOS> <EOS> <EOS> the <EOS> wa t be <EOS>\n",
      "    DEV dec train infer > i m a a a a a a a a a a <EOS> <EOS> <EOS>\n",
      "global_step: 747\n",
      "learning rate 0.000986225\n",
      "epoch 1\n",
      "batch 250\n",
      "training minibatch loss: 5.327145099639893\n",
      "  sample 1:\n",
      "    enc input           > good luck baby <EOS>\n",
      "    dec input           > im literally at the dmv by myself amp i have to uber to school <EOS>\n",
      "    dec train predicted > i the a the time <EOS> <EOS> <EOS> <EOS> m a be <EOS> the <EOS>\n",
      "dev minibatch loss: 5.782663345336914\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > is it like deleting confidential email ? <EOS>\n",
      "    DEV dec input           > yet they turn video off or destroy tape to no consequence ? <EOS>\n",
      "    DEV dec train predicted > i is s to <EOS> <EOS> the <EOS> <EOS> the <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a good time <EOS>\n",
      "global_step: 797\n",
      "learning rate 0.000985309\n",
      "epoch 1\n",
      "batch 300\n",
      "training minibatch loss: 5.296725273132324\n",
      "  sample 1:\n",
      "    enc input           > better get terry s sword <EOS>\n",
      "    dec input           > did i hear zombie <EOS>\n",
      "    dec train predicted > i you m to the\n",
      "dev minibatch loss: 5.596807956695557\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > haha i don t wan na school you kid on the old cod man <EOS>\n",
      "    DEV dec input           > shitttt snag that hoe back mw is coming up next <EOS>\n",
      "    DEV dec train predicted > i is is s <EOS> <EOS> <EOS> the <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a good <EOS>\n",
      "global_step: 847\n",
      "learning rate 0.000984395\n",
      "epoch 1\n",
      "batch 350\n",
      "training minibatch loss: 5.350354194641113\n",
      "  sample 1:\n",
      "    enc input           > no way he doesn t watch porn he rape kid <EOS>\n",
      "    dec input           > trump s recommended video section on pornhub is definitely hentai <EOS>\n",
      "    dec train predicted > i s a of <EOS> <EOS> the <EOS> the a <EOS>\n",
      "dev minibatch loss: 5.9164719581604\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i lied i read back to and then read <EOS>\n",
      "    DEV dec input           > you never fail to make me laugh i love that xx <EOS>\n",
      "    DEV dec train predicted > i s be to be the <EOS> <EOS> m a <EOS> <EOS>\n",
      "    DEV dec train infer > i m a a good day <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 897\n",
      "learning rate 0.000983481\n",
      "epoch 1\n",
      "batch 400\n",
      "training minibatch loss: 5.48546838760376\n",
      "  sample 1:\n",
      "    enc input           > no the form is if i don t file them someone else will which is a different idea <EOS>\n",
      "    dec input           > the people want frivolous lawsuit so i have to file them or someone else will <EOS>\n",
      "    dec train predicted > i new is a <EOS> is i m a be a <EOS> the s <EOS> be\n",
      "dev minibatch loss: 6.028444290161133\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > club banger <EOS>\n",
      "    DEV dec input           > how about love love love ? lol <EOS>\n",
      "    DEV dec train predicted > i you the <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a best <EOS> <EOS>\n",
      "global_step: 947\n",
      "learning rate 0.000982569\n",
      "epoch 1\n",
      "batch 450\n",
      "training minibatch loss: 5.345386505126953\n",
      "  sample 1:\n",
      "    enc input           > nyc nyc happy to be home for a sec come over p <EOS>\n",
      "    dec input           > i miss you too where in the world are you now ? <EOS>\n",
      "    dec train predicted > i m you a <EOS> i the new <EOS> the <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.9676127433776855\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > ugh it s taking forever to find one <EOS>\n",
      "    DEV dec input           > lmfao omg so me but we all have one i just haven t found mine either <EOS>\n",
      "    DEV dec train predicted > i i i i <EOS> you ll a the <EOS> m be t be <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a new time <EOS>\n",
      "global_step: 993\n",
      "learning rate 0.00098173\n",
      "epoch 2\n",
      "batch 0\n",
      "training minibatch loss: 5.3803606033325195\n",
      "  sample 1:\n",
      "    enc input           > dont give any seed to kiefer <EOS>\n",
      "    dec input           > you can say it i don t mind <EOS>\n",
      "    dec train predicted > i re t you <EOS> m t be to\n",
      "dev minibatch loss: 5.523123264312744\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i applaud them <EOS>\n",
      "    DEV dec input           > i personally know hillary supporter who said they may no longer vote for her <EOS>\n",
      "    DEV dec train predicted > i m a you s <EOS> s <EOS> s be a <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m you to be a new day <EOS>\n",
      "global_step: 1043\n",
      "learning rate 0.000980819\n",
      "epoch 2\n",
      "batch 50\n",
      "training minibatch loss: 4.996707916259766\n",
      "  sample 1:\n",
      "    enc input           > and manson is responsible for fewer death <EOS>\n",
      "    dec input           > so would i he s sane compared to her <EOS>\n",
      "    dec train predicted > i i you have have a to <EOS> be <EOS>\n",
      "dev minibatch loss: 5.487663745880127\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we should go to this <EOS>\n",
      "    DEV dec input           > come party with amp paper this thursday in bk rsvp here <EOS>\n",
      "    DEV dec train predicted > i is to the <EOS> <EOS> <EOS> <EOS> the <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a good day <EOS> <EOS>\n",
      "global_step: 1093\n",
      "learning rate 0.000979909\n",
      "epoch 2\n",
      "batch 100\n",
      "training minibatch loss: 5.3877763748168945\n",
      "  sample 1:\n",
      "    enc input           > it s a pho craving morning here too <EOS>\n",
      "    dec input           > mama need pho and she need it now <EOS>\n",
      "    dec train predicted > i to to <EOS> i s to to <EOS>\n",
      "dev minibatch loss: 5.507596969604492\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > in i want a tank on governor island and horse too <EOS>\n",
      "    DEV dec input           > sat sept <EOS>\n",
      "    DEV dec train predicted > i to the\n",
      "    DEV dec train infer > i m a good one of the debate <EOS> <EOS>\n",
      "global_step: 1143\n",
      "learning rate 0.000978999\n",
      "epoch 2\n",
      "batch 150\n",
      "training minibatch loss: 5.314604759216309\n",
      "  sample 1:\n",
      "    enc input           > thank you carly for endorsing trump <EOS>\n",
      "    dec input           > after more than a year of bashing trump ted cruz supporter carly fiorina is finally aboard the <EOS>\n",
      "    dec train predicted > i the of the good of the <EOS> s <EOS> s <EOS> <EOS> <EOS> the <EOS> <EOS> world\n",
      "dev minibatch loss: 5.527829170227051\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i don t need surgery lol <EOS>\n",
      "    DEV dec input           > hope your surgery go well dude lt <EOS>\n",
      "    DEV dec train predicted > i you favorite is <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a good day <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 1193\n",
      "learning rate 0.000978091\n",
      "epoch 2\n",
      "batch 200\n",
      "training minibatch loss: 5.159919738769531\n",
      "  sample 1:\n",
      "    enc input           > not when ur taking a dump lol <EOS>\n",
      "    dec input           > someone s always watching everything you do <EOS>\n",
      "    dec train predicted > i is a be the <EOS> re the\n",
      "dev minibatch loss: 5.705637454986572\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i would like to know where my country is going still thank you <EOS>\n",
      "    DEV dec input           > why do nothing but bitch about the candidate when you can t even vote ? <EOS>\n",
      "    DEV dec train predicted > i is you to i <EOS> the time <EOS> you re have be be <EOS> <EOS>\n",
      "    DEV dec train infer > i m a good day <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1243\n",
      "learning rate 0.000977183\n",
      "epoch 2\n",
      "batch 250\n",
      "training minibatch loss: 5.298300743103027\n",
      "  sample 1:\n",
      "    enc input           > how do you create that illusion ? lol <EOS>\n",
      "    dec input           > if you don t know how to twerk you have to create the illusion that you do know <EOS>\n",
      "    dec train predicted > i you re t be it i be i re a be the day <EOS> s <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.9574055671691895\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > confirmed train is halted at th and oakland <EOS>\n",
      "    DEV dec input           > stuck on at th st possible person on track nothing from so far <EOS>\n",
      "    DEV dec train predicted > i to the the <EOS> <EOS> <EOS> <EOS> the <EOS> <EOS> the <EOS> <EOS>\n",
      "    DEV dec train infer > i m a new york <EOS> <EOS> <EOS>\n",
      "global_step: 1293\n",
      "learning rate 0.000976276\n",
      "epoch 2\n",
      "batch 300\n",
      "training minibatch loss: 5.3240861892700195\n",
      "  sample 1:\n",
      "    enc input           > why is it when i see a mushroom i see rupert murdoch s face ? <EOS>\n",
      "    dec input           > one can make rotten an entire package one can poison a harvest of mushroom and ag <EOS>\n",
      "    dec train predicted > i of you a a na thing thing <EOS> have <EOS> good of the <EOS> i <EOS>\n",
      "dev minibatch loss: 5.379581928253174\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the towel is in the bidet <EOS>\n",
      "    DEV dec input           > no that s the sink that european use to clean their as w every private bathroom ha one <EOS>\n",
      "    DEV dec train predicted > i the s a new <EOS> s <EOS> <EOS> be the own <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a a new york <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1343\n",
      "learning rate 0.00097537\n",
      "epoch 2\n",
      "batch 350\n",
      "training minibatch loss: 5.1049418449401855\n",
      "  sample 1:\n",
      "    enc input           > carrying the gospel with me because <EOS>\n",
      "    dec input           > i m on my way to the representing at the digital medium lounge <EOS>\n",
      "    dec train predicted > i m a the time to be same of the time <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.530754566192627\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > yeah that s hilarious you should go on the road with that act <EOS>\n",
      "    DEV dec input           > she s probably taking a nap don t expect you ll hear back <EOS>\n",
      "    DEV dec train predicted > i s a be the good <EOS> t know to re be the <EOS>\n",
      "    DEV dec train infer > i m a good one to be a good time <EOS> <EOS>\n",
      "global_step: 1393\n",
      "learning rate 0.000974465\n",
      "epoch 2\n",
      "batch 400\n",
      "training minibatch loss: 5.468960285186768\n",
      "  sample 1:\n",
      "    enc input           > actually a v good idea <EOS>\n",
      "    dec input           > throw a bunch of your other stuff away <EOS>\n",
      "    dec train predicted > i the new of the birthday <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.507020950317383\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > to the rioter in charlotte north carolina each to vote for <EOS>\n",
      "    DEV dec input           > charlotte police shooting victim wa armed with a gun not a book chief say <EOS>\n",
      "    DEV dec train predicted > the is is of of a <EOS> the new <EOS> a new <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > the new york is a a a trump is a new york <EOS> <EOS> <EOS>\n",
      "global_step: 1443\n",
      "learning rate 0.00097356\n",
      "epoch 2\n",
      "batch 450\n",
      "training minibatch loss: 5.364781379699707\n",
      "  sample 1:\n",
      "    enc input           > great to see you happy my friend be safe and take care your friend dan <EOS>\n",
      "    dec input           > congrats on the new location well deserved <EOS>\n",
      "    dec train predicted > i to the day day <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.5908203125\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > have a nice night <EOS>\n",
      "    DEV dec input           > lol where wa all this outrage the last year ? <EOS>\n",
      "    DEV dec train predicted > i you you the <EOS> <EOS> <EOS> day <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a great <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1489\n",
      "learning rate 0.000972729\n",
      "epoch 3\n",
      "batch 0\n",
      "training minibatch loss: 4.949897766113281\n",
      "  sample 1:\n",
      "    enc input           > oh yes you will i m back next week and again the week after <EOS>\n",
      "    dec input           > i m never going to actually meet you in person am i ? <EOS>\n",
      "    dec train predicted > i m not be to be a you <EOS> the <EOS> i m <EOS>\n",
      "dev minibatch loss: 5.563443183898926\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > yes i m confident quietly confident bush endorsement will be interesting <EOS>\n",
      "    DEV dec input           > hey still confident ? can t imagine how you guy would feel losing to trump <EOS>\n",
      "    DEV dec train predicted > i trump just i <EOS> t be it it have have be a <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m not a new york <EOS> <EOS> <EOS>\n",
      "global_step: 1539\n",
      "learning rate 0.000971826\n",
      "epoch 3\n",
      "batch 50\n",
      "training minibatch loss: 5.1256585121154785\n",
      "  sample 1:\n",
      "    enc input           > if not what in the world is he referencing ? <EOS>\n",
      "    dec input           > did trump just say drug are a very very big factor in the protest in charlotte ? <EOS>\n",
      "    dec train predicted > i you campaign been the to a trump na na debate to the debate <EOS> the <EOS> <EOS>\n",
      "dev minibatch loss: 5.704196453094482\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > like obama did to thousand of coal miner <EOS>\n",
      "    DEV dec input           > don t forget how many small business he s bankrupted by contracting them then refusing to pay <EOS>\n",
      "    DEV dec train predicted > trump t be the the birther campaign <EOS> s not a the <EOS> <EOS> <EOS> <EOS> control <EOS>\n",
      "    DEV dec train infer > trump is a new york <EOS> is the new york <EOS>\n",
      "global_step: 1589\n",
      "learning rate 0.000970925\n",
      "epoch 3\n",
      "batch 100\n",
      "training minibatch loss: 5.073665618896484\n",
      "  sample 1:\n",
      "    enc input           > can we start w s immigrant ancestor please ? <EOS>\n",
      "    dec input           > well there it is <EOS>\n",
      "    dec train predicted > the you s is a\n",
      "dev minibatch loss: 5.789559841156006\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > nope it s your dad and we ve seen proof more than once <EOS>\n",
      "    DEV dec input           > eric trump bill clinton is maybe the worst sexist that ever lived <EOS>\n",
      "    DEV dec train predicted > i a is <EOS> is a you same <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a good thing <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1639\n",
      "learning rate 0.000970024\n",
      "epoch 3\n",
      "batch 150\n",
      "training minibatch loss: 4.9785075187683105\n",
      "  sample 1:\n",
      "    enc input           > martin please calm down <EOS>\n",
      "    dec input           > young thug there are no gender progressive a fuck pac probably thought there were only gender sexist pig <EOS>\n",
      "    dec train predicted > this news are is a new <EOS> <EOS> new <EOS> i is he is <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.438430309295654\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > did just call these post game interview ? <EOS>\n",
      "    DEV dec input           > the first u presidential debate is now underway watch live on <EOS>\n",
      "    DEV dec train predicted > the new day is debate <EOS> a <EOS> <EOS> <EOS> <EOS> the\n",
      "    DEV dec train infer > the new york <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1689\n",
      "learning rate 0.000969123\n",
      "epoch 3\n",
      "batch 200\n",
      "training minibatch loss: 5.033872127532959\n",
      "  sample 1:\n",
      "    enc input           > of course dude a glorified real estate agent beating out career politician <EOS>\n",
      "    dec input           > honestly think trump whole presidential race will lead to a new tv series for him win or lose <EOS>\n",
      "    dec train predicted > the is i s debate debate is be to the new york <EOS> <EOS> the <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 6.0095930099487305\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > how adorable have a wonderful day <EOS>\n",
      "    DEV dec input           > seeing mommy properly for the first time beautiful <EOS>\n",
      "    DEV dec train predicted > i you for <EOS> the great day <EOS> <EOS>\n",
      "    DEV dec train infer > i m just just just to be a great day <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1739\n",
      "learning rate 0.000968224\n",
      "epoch 3\n",
      "batch 250\n",
      "training minibatch loss: 5.198957920074463\n",
      "  sample 1:\n",
      "    enc input           > they also got divorced when i wa but you can t prove those thing are directly linked <EOS>\n",
      "    dec input           > who even are you guy ? <EOS>\n",
      "    dec train predicted > i s like it a <EOS> <EOS>\n",
      "dev minibatch loss: 5.628173351287842\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > why would you talk trash on your gfs school lol bye <EOS>\n",
      "    DEV dec input           > supporting my girl on jv dawgggg screw the varsity <EOS>\n",
      "    DEV dec train predicted > i the favorite <EOS> the <EOS> <EOS> <EOS> time of\n",
      "    DEV dec train infer > i m a good thing to the same <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1789\n",
      "learning rate 0.000967325\n",
      "epoch 3\n",
      "batch 300\n",
      "training minibatch loss: 5.207512855529785\n",
      "  sample 1:\n",
      "    enc input           > bet this come up in the next debate <EOS>\n",
      "    dec input           > and finger includes petitioning to make sure homeless veteran couldn t sit near trump tower <EOS>\n",
      "    dec train predicted > i i is in to be the you <EOS> is t be <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.648268699645996\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > whaaaat i thought you were the one who gave out weeb certification <EOS>\n",
      "    DEV dec input           > me studying to become a certified weeb <EOS>\n",
      "    DEV dec train predicted > i to <EOS> be a good <EOS> <EOS>\n",
      "    DEV dec train infer > i m not a good thing <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1839\n",
      "learning rate 0.000966428\n",
      "epoch 3\n",
      "batch 350\n",
      "training minibatch loss: 5.239618301391602\n",
      "  sample 1:\n",
      "    enc input           > started my day with a laugh and smile thank you ricky <EOS>\n",
      "    dec input           > old tree going for a walk <EOS>\n",
      "    dec train predicted > i one is to the good <EOS>\n",
      "dev minibatch loss: 5.76487922668457\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > well done you will have to teach your niece <EOS>\n",
      "    DEV dec input           > since this afternoon <EOS>\n",
      "    DEV dec train predicted > i i is <EOS>\n",
      "    DEV dec train infer > i m not a good day <EOS> <EOS> <EOS>\n",
      "global_step: 1889\n",
      "learning rate 0.000965531\n",
      "epoch 3\n",
      "batch 400\n",
      "training minibatch loss: 5.092404842376709\n",
      "  sample 1:\n",
      "    enc input           > ok lol thanks <EOS>\n",
      "    dec input           > happy birthday craig <EOS>\n",
      "    dec train predicted > happy birthday <EOS> <EOS>\n",
      "dev minibatch loss: 5.820908069610596\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > your so pretty <EOS>\n",
      "    DEV dec input           > trip on love but never fall <EOS>\n",
      "    DEV dec train predicted > i to the you i want <EOS>\n",
      "    DEV dec train infer > i m so good <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1939\n",
      "learning rate 0.000964635\n",
      "epoch 3\n",
      "batch 450\n",
      "training minibatch loss: 5.076944351196289\n",
      "  sample 1:\n",
      "    enc input           > i think she s cute <EOS>\n",
      "    dec input           > this is so wrong why do y all do her like this <EOS>\n",
      "    dec train predicted > i is a much <EOS> i you you do you <EOS> you <EOS>\n",
      "dev minibatch loss: 5.643598556518555\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > so true lmfao <EOS>\n",
      "    DEV dec input           > the sign a joanne the scammer gifs aquarius <EOS>\n",
      "    DEV dec train predicted > i best of good <EOS> day love <EOS> <EOS>\n",
      "    DEV dec train infer > i m going to be a good day <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 1985\n",
      "learning rate 0.000963811\n",
      "epoch 4\n",
      "batch 0\n",
      "training minibatch loss: 4.87235164642334\n",
      "  sample 1:\n",
      "    enc input           > long stream tomorrow <EOS>\n",
      "    dec input           > alright bro see you tomorrow on twitch <EOS>\n",
      "    dec train predicted > happy i <EOS> you <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.789539813995361\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > maybe backroom shady shit <EOS>\n",
      "    DEV dec input           > i just read something that say no public appearance til the debate wtf ? <EOS>\n",
      "    DEV dec train predicted > i m want the to s i <EOS> <EOS> <EOS> <EOS> new <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m not a good one <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2035\n",
      "learning rate 0.000962917\n",
      "epoch 4\n",
      "batch 50\n",
      "training minibatch loss: 4.623201370239258\n",
      "  sample 1:\n",
      "    enc input           > now i m gon na drive myself off a bridge <EOS>\n",
      "    dec input           > i refuse to let a stanford fan hit me with their car <EOS>\n",
      "    dec train predicted > i m to see you good thing i you <EOS> the phone <EOS>\n",
      "dev minibatch loss: 6.238796710968018\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > for a cooperative solution i mean <EOS>\n",
      "    DEV dec input           > i think the subsidy are in place and many city have a long history of cooperative housing scenario <EOS>\n",
      "    DEV dec train predicted > i m i whole a a the to i time <EOS> to good thing <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m not a good thing to be a bit <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2085\n",
      "learning rate 0.000962023\n",
      "epoch 4\n",
      "batch 100\n",
      "training minibatch loss: 4.877383232116699\n",
      "  sample 1:\n",
      "    enc input           > if trump is not making any profit out of it then why don t u release your tax <EOS>\n",
      "    dec input           > the clinton have perfected the politics of profit <EOS>\n",
      "    dec train predicted > trump debate is been a debate of the <EOS>\n",
      "dev minibatch loss: 5.82274866104126\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > how much longer i got ta be a side piece for ? <EOS>\n",
      "    DEV dec input           > also not my husband <EOS>\n",
      "    DEV dec train predicted > i i the favorite is\n",
      "    DEV dec train infer > i m just just just just just just just just just just just like it <EOS> <EOS> <EOS>\n",
      "global_step: 2135\n",
      "learning rate 0.00096113\n",
      "epoch 4\n",
      "batch 150\n",
      "training minibatch loss: 4.86746072769165\n",
      "  sample 1:\n",
      "    enc input           > sad state of affair no respect taught child <EOS>\n",
      "    dec input           > public school in america latino teacher fighting with african american student people wonder about white flight ? <EOS>\n",
      "    dec train predicted > trump campaign is trump trump is a a trump debate <EOS> clinton have he his tax <EOS> <EOS>\n",
      "dev minibatch loss: 5.876559257507324\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > it s a brain axe <EOS>\n",
      "    DEV dec input           > you have my axe <EOS>\n",
      "    DEV dec train predicted > i re a question ?\n",
      "    DEV dec train infer > i m not a good thing to be a a of a trump supporter <EOS> <EOS> <EOS>\n",
      "global_step: 2185\n",
      "learning rate 0.000960238\n",
      "epoch 4\n",
      "batch 200\n",
      "training minibatch loss: 4.749421119689941\n",
      "  sample 1:\n",
      "    enc input           > it okay thanks for being so quick to respond is this dan or tom ? <EOS>\n",
      "    dec input           > ok great sorry for the trouble <EOS>\n",
      "    dec train predicted > i i time to the best <EOS>\n",
      "dev minibatch loss: 5.926612377166748\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we need a new theology of money <EOS>\n",
      "    DEV dec input           > thought on this thread ? <EOS>\n",
      "    DEV dec train predicted > i i the <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m so much a a a a a a a a a a it <EOS> <EOS>\n",
      "global_step: 2235\n",
      "learning rate 0.000959347\n",
      "epoch 4\n",
      "batch 250\n",
      "training minibatch loss: 4.984419345855713\n",
      "  sample 1:\n",
      "    enc input           > white men being personally offended gt gt gt woman being actually physically tortured and brutalized <EOS>\n",
      "    dec input           > it s not like he s quietly sitting during a song like a monster <EOS>\n",
      "    dec train predicted > the s a a the s a a than the new <EOS> the a <EOS>\n",
      "dev minibatch loss: 5.967220306396484\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > my president <EOS>\n",
      "    DEV dec input           > most people who are bully grow out of it donald trump hasn t <EOS>\n",
      "    DEV dec train predicted > i on got have a <EOS> <EOS> <EOS> my <EOS> i s t <EOS>\n",
      "    DEV dec train infer > i m very very good <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 2285\n",
      "learning rate 0.000958457\n",
      "epoch 4\n",
      "batch 300\n",
      "training minibatch loss: 4.9669880867004395\n",
      "  sample 1:\n",
      "    enc input           > yes it is haha thank you <EOS>\n",
      "    dec input           > you are so beautiful is this you ? xd <EOS>\n",
      "    dec train predicted > i re a much <EOS> it <EOS> re <EOS> <EOS>\n",
      "dev minibatch loss: 5.623543739318848\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > like every day <EOS>\n",
      "    DEV dec input           > how often are you in manhattan ? <EOS>\n",
      "    DEV dec train predicted > i is i you <EOS> the <EOS> <EOS>\n",
      "    DEV dec train infer > i m just going to be a bit <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2335\n",
      "learning rate 0.000957567\n",
      "epoch 4\n",
      "batch 350\n",
      "training minibatch loss: 4.903368949890137\n",
      "  sample 1:\n",
      "    enc input           > i originally read that a bug catching <EOS>\n",
      "    dec input           > friday night is for bug writing <EOS>\n",
      "    dec train predicted > i of is a a the <EOS>\n",
      "dev minibatch loss: 5.651825904846191\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > sure common respect you ll never hear a coach say anything negative about a rival player or team <EOS>\n",
      "    DEV dec input           > there is a reason d coordinator say he is one of the best <EOS>\n",
      "    DEV dec train predicted > i s a good that <EOS> <EOS> you s a of the debate time\n",
      "    DEV dec train infer > i don t think it s a good one and trump is a lot of american of a thing <EOS>\n",
      "global_step: 2385\n",
      "learning rate 0.000956679\n",
      "epoch 4\n",
      "batch 400\n",
      "training minibatch loss: 4.906494140625\n",
      "  sample 1:\n",
      "    enc input           > this is from march <EOS>\n",
      "    dec input           > message continues to resonate with american people <EOS>\n",
      "    dec train predicted > this the to be to the <EOS> <EOS>\n",
      "dev minibatch loss: 5.870491981506348\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > xx dj al emo xx on the s and s <EOS>\n",
      "    DEV dec input           > lmao just walk around cry into the microphone all night <EOS>\n",
      "    DEV dec train predicted > i i a a for <EOS> the new <EOS> the <EOS>\n",
      "    DEV dec train infer > i m just excited to be a new day <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2435\n",
      "learning rate 0.000955791\n",
      "epoch 4\n",
      "batch 450\n",
      "training minibatch loss: 4.827979564666748\n",
      "  sample 1:\n",
      "    enc input           > or let the woman she fight use steroid <EOS>\n",
      "    dec input           > please let her fight men sick of seeing her bully beat down these woman <EOS>\n",
      "    dec train predicted > i do the know a to of the the own <EOS> the <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.682351112365723\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > u fishing for straight people <EOS>\n",
      "    DEV dec input           > i want a boo thang too let go fishing <EOS>\n",
      "    DEV dec train predicted > i m to good <EOS> to <EOS> you <EOS> <EOS>\n",
      "    DEV dec train infer > i m not a good one to be a good one <EOS> <EOS> <EOS>\n",
      "global_step: 2481\n",
      "learning rate 0.000954975\n",
      "epoch 5\n",
      "batch 0\n",
      "training minibatch loss: 4.5676960945129395\n",
      "  sample 1:\n",
      "    enc input           > your racist also you should stop lying for trump you re trapped in his bull <EOS>\n",
      "    dec input           > bloody weekend in nyc put rising murder rate in spotlight via <EOS>\n",
      "    dec train predicted > i news is the s a and and to the <EOS> <EOS>\n",
      "dev minibatch loss: 5.428252696990967\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i knew it ditto also love freaky friday w jodie amp barbara d <EOS>\n",
      "    DEV dec input           > love and of course <EOS>\n",
      "    DEV dec train predicted > i it i the i\n",
      "    DEV dec train infer > i m sorry i m sorry i m not a good day <EOS> <EOS> <EOS>\n",
      "global_step: 2531\n",
      "learning rate 0.000954089\n",
      "epoch 5\n",
      "batch 50\n",
      "training minibatch loss: 4.287290573120117\n",
      "  sample 1:\n",
      "    enc input           > let s put roast beef on our belly button <EOS>\n",
      "    dec input           > you re the dog of this city <EOS>\n",
      "    dec train predicted > i re a new <EOS> the <EOS> <EOS>\n",
      "dev minibatch loss: 5.935810565948486\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i ll try it next time <EOS>\n",
      "    DEV dec input           > the pork spicy taco across from tasty pot is pretty fuckin fire too <EOS>\n",
      "    DEV dec train predicted > i whole is as <EOS> my <EOS> game <EOS> my good friend <EOS> <EOS>\n",
      "    DEV dec train infer > i m so much good <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2581\n",
      "learning rate 0.000953203\n",
      "epoch 5\n",
      "batch 100\n",
      "training minibatch loss: 4.569470405578613\n",
      "  sample 1:\n",
      "    enc input           > awwww nice hey are you going to see bodyguard in theater on december nd ? ? ? <EOS>\n",
      "    dec input           > my bieber shirt black jean and converse or something haha <EOS>\n",
      "    dec train predicted > my fav team in park <EOS> i <EOS> the i <EOS>\n",
      "dev minibatch loss: 6.246344566345215\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > well playing banjo doe build up some serious arm strength <EOS>\n",
      "    DEV dec input           > fyi boxing steve martin back in the ring after absence of a year <EOS>\n",
      "    DEV dec train predicted > i the sniffing is <EOS> to nyc th <EOS> i <EOS> reddit th of\n",
      "    DEV dec train infer > i m just going to be able to get to be a bit <EOS> <EOS>\n",
      "global_step: 2631\n",
      "learning rate 0.000952319\n",
      "epoch 5\n",
      "batch 150\n",
      "training minibatch loss: 4.48891019821167\n",
      "  sample 1:\n",
      "    enc input           > winning made my day <EOS>\n",
      "    dec input           > and thus wrap another which win made you happiest tonight ? <EOS>\n",
      "    dec train predicted > i i sniffing <EOS> night wa <EOS> me <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.388086318969727\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > glad you guy are ok <EOS>\n",
      "    DEV dec input           > i wa walking down that block just last night this mean nothing but is still weird <EOS>\n",
      "    DEV dec train predicted > the m gon by and the <EOS> not night <EOS> week is is the <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > the news is gross <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2681\n",
      "learning rate 0.000951435\n",
      "epoch 5\n",
      "batch 200\n",
      "training minibatch loss: 4.394584655761719\n",
      "  sample 1:\n",
      "    enc input           > i must live in a diff world <EOS>\n",
      "    dec input           > na deadass tho <EOS>\n",
      "    dec train predicted > the have of <EOS>\n",
      "dev minibatch loss: 5.729007720947266\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > happy birth date <EOS>\n",
      "    DEV dec input           > it my birthday today time to get baked and eat some sushi <EOS>\n",
      "    DEV dec train predicted > hey s birthday <EOS> <EOS> you be to of you your of <EOS>\n",
      "    DEV dec train infer > hey you are a good day for your birthday <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2731\n",
      "learning rate 0.000950552\n",
      "epoch 5\n",
      "batch 250\n",
      "training minibatch loss: 4.693996906280518\n",
      "  sample 1:\n",
      "    enc input           > she may be your type but she look like a pig so move on and stop pushing her <EOS>\n",
      "    dec input           > follow the perfect queen look how beautiful she is and look at them lip <EOS>\n",
      "    dec train predicted > i the best thing is a you <EOS> s <EOS> much <EOS> the <EOS> <EOS>\n",
      "dev minibatch loss: 5.832935810089111\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > s ionnidis ioannidis sigh <EOS>\n",
      "    DEV dec input           > also read up on just about anything written by john ionnidis about bad study <EOS>\n",
      "    DEV dec train predicted > i i the <EOS> the a the a <EOS> the <EOS> <EOS> the <EOS> <EOS>\n",
      "    DEV dec train infer > i m a new day <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2781\n",
      "learning rate 0.00094967\n",
      "epoch 5\n",
      "batch 300\n",
      "training minibatch loss: 4.722751140594482\n",
      "  sample 1:\n",
      "    enc input           > did anderson cooper dye his hair ? <EOS>\n",
      "    dec input           > this show moved me to tear excited for animal lover to watch on october th <EOS>\n",
      "    dec train predicted > the is are to a be a to the head to be <EOS> nyc night <EOS>\n",
      "dev minibatch loss: 5.775148391723633\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > awwww they thought the little effort that they put in wa enough girl how bout now <EOS>\n",
      "    DEV dec input           > they still up right now but they suck right now <EOS>\n",
      "    DEV dec train predicted > i re have in to <EOS> i re <EOS> ? <EOS>\n",
      "    DEV dec train infer > i m not a lot of a a a a a a a a a a one <EOS> <EOS> <EOS>\n",
      "global_step: 2831\n",
      "learning rate 0.000948788\n",
      "epoch 5\n",
      "batch 350\n",
      "training minibatch loss: 4.637340545654297\n",
      "  sample 1:\n",
      "    enc input           > for real i be so mad listening to ppl argue bout it <EOS>\n",
      "    dec input           > whoever said there not is stupid <EOS>\n",
      "    dec train predicted > i is i is a a <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 6.1120285987854\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > wait both of these should be not <EOS>\n",
      "    DEV dec input           > then the bridge is maybe <EOS>\n",
      "    DEV dec train predicted > the you best is getting you\n",
      "    DEV dec train infer > the new headphone is a great time for a great time of <EOS> <EOS> <EOS>\n",
      "global_step: 2881\n",
      "learning rate 0.000947908\n",
      "epoch 5\n",
      "batch 400\n",
      "training minibatch loss: 4.413819789886475\n",
      "  sample 1:\n",
      "    enc input           > bad idea wouldn t recommend <EOS>\n",
      "    dec input           > maybe i should light myself on fire too <EOS>\n",
      "    dec train predicted > this the m be the to the <EOS> <EOS>\n",
      "dev minibatch loss: 5.825194835662842\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you spelled shitbag of humanity wrong <EOS>\n",
      "    DEV dec input           > lt oh look another ignorant racist let s have twitter follower block him <EOS>\n",
      "    DEV dec train predicted > the the the a the a in s be a own have <EOS> <EOS>\n",
      "    DEV dec train infer > the latest is a new york a a you re a great time <EOS> <EOS> <EOS>\n",
      "global_step: 2931\n",
      "learning rate 0.000947028\n",
      "epoch 5\n",
      "batch 450\n",
      "training minibatch loss: 4.517518043518066\n",
      "  sample 1:\n",
      "    enc input           > nobody is doing anything about it what action can be taken ? none ? <EOS>\n",
      "    dec input           > the fbi ha prosecuted american for compromising information far le classified than what clinton and her staff <EOS>\n",
      "    dec train predicted > the new is been new in million in in a than than <EOS> a <EOS> not fault <EOS>\n",
      "dev minibatch loss: 5.878880977630615\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i just need these in my life omg please <EOS>\n",
      "    DEV dec input           > rt to win black rose gold zoeva brush set must be following me so i can dm winner <EOS>\n",
      "    DEV dec train predicted > my this be my <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> be a <EOS> <EOS> much m t <EOS> <EOS>\n",
      "    DEV dec train infer > my favorite team is a bunch of my life <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2977\n",
      "learning rate 0.00094622\n",
      "epoch 6\n",
      "batch 0\n",
      "training minibatch loss: 3.9912467002868652\n",
      "  sample 1:\n",
      "    enc input           > show their true color cash over anything <EOS>\n",
      "    dec input           > whoever is attending this money grab birthday need to be shot off into space <EOS>\n",
      "    dec train predicted > is is a the is is favorite need to be a to the the <EOS>\n",
      "dev minibatch loss: 6.374699115753174\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what s the breakdown of the indian immigrant population in term of religion ? <EOS>\n",
      "    DEV dec input           > indian are the fastest growing illegal immigrant population in the u via <EOS>\n",
      "    DEV dec train predicted > and the you other of in than <EOS> <EOS> the debate <EOS> <EOS>\n",
      "    DEV dec train infer > and the debate are a lot of a lot of a lot of a public <EOS> <EOS>\n",
      "global_step: 3027\n",
      "learning rate 0.000945342\n",
      "epoch 6\n",
      "batch 50\n",
      "training minibatch loss: 4.051975250244141\n",
      "  sample 1:\n",
      "    enc input           > wtf you think i m doing ? ? ? ? lol <EOS>\n",
      "    dec input           > you got me lol you best catch up though <EOS>\n",
      "    dec train predicted > i re me i <EOS> <EOS> <EOS> to <EOS> <EOS>\n",
      "dev minibatch loss: 6.519482135772705\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > can i be excited for the what remains or nah ? <EOS>\n",
      "    DEV dec input           > well go look at the summary that wa posted and you ll know why i m excited <EOS>\n",
      "    DEV dec train predicted > the the a a the game ? s a <EOS> i are be <EOS> <EOS> am <EOS> <EOS>\n",
      "    DEV dec train infer > the mets app is the street <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3077\n",
      "learning rate 0.000944464\n",
      "epoch 6\n",
      "batch 100\n",
      "training minibatch loss: 4.137512683868408\n",
      "  sample 1:\n",
      "    enc input           > dare i even ask what s drug these people are on that made this a good idea ? <EOS>\n",
      "    dec input           > trump will propose nationwide stop and frisk to address violence in black community nite on hannity <EOS>\n",
      "    dec train predicted > trump is be people trump for frisk and trump the to american community <EOS> of american <EOS>\n",
      "dev minibatch loss: 6.537962436676025\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > folk who work on open source still got ta pay the bill somehow <EOS>\n",
      "    DEV dec input           > just feel like a great open sourced project for motivated people <EOS>\n",
      "    DEV dec train predicted > just want not the new yorker in show <EOS> the <EOS> <EOS>\n",
      "    DEV dec train infer > just want to be fair the website is a new yorker <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3127\n",
      "learning rate 0.000943588\n",
      "epoch 6\n",
      "batch 150\n",
      "training minibatch loss: 4.179434776306152\n",
      "  sample 1:\n",
      "    enc input           > happy to you too <EOS>\n",
      "    dec input           > to my forever family happy to u all xo <EOS>\n",
      "    dec train predicted > i all day friend birthday to the <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.578279495239258\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > they are ya man in nyc <EOS>\n",
      "    DEV dec input           > these moment with jay <EOS>\n",
      "    DEV dec train predicted > and twitter i the of\n",
      "    DEV dec train infer > and i m a dazzle and the knicks are begun and chocolate <EOS> <EOS> <EOS>\n",
      "global_step: 3177\n",
      "learning rate 0.000942712\n",
      "epoch 6\n",
      "batch 200\n",
      "training minibatch loss: 4.32712984085083\n",
      "  sample 1:\n",
      "    enc input           > you called a bigot are these latino bigot too ? <EOS>\n",
      "    dec input           > hey politifact also say you and don flat out lie <EOS>\n",
      "    dec train predicted > wow trump look make trump to she t a people <EOS>\n",
      "dev minibatch loss: 6.558627128601074\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > are you representing him in the divorce ? haha <EOS>\n",
      "    DEV dec input           > he s screwed either way i already went through the financials with babs <EOS>\n",
      "    DEV dec train predicted > don s not to <EOS> to m don to in cereal of the <EOS>\n",
      "    DEV dec train infer > don t be doing that ? ? ? ? ? ? ? ? ? ? ? <EOS>\n",
      "global_step: 3227\n",
      "learning rate 0.000941837\n",
      "epoch 6\n",
      "batch 250\n",
      "training minibatch loss: 3.967472791671753\n",
      "  sample 1:\n",
      "    enc input           > nope i am here for this too <EOS>\n",
      "    dec input           > what if i m the only one listening when you talk dawson s creek i m so intrigued <EOS>\n",
      "    dec train predicted > i s you m going best thing i to i re about s a it m not cute <EOS>\n",
      "dev minibatch loss: 6.16062593460083\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we need a new theology of money <EOS>\n",
      "    DEV dec input           > thought on this thread ? <EOS>\n",
      "    DEV dec train predicted > can of the <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > can t wait to be a a a a a a a a a it <EOS> <EOS>\n",
      "global_step: 3277\n",
      "learning rate 0.000940963\n",
      "epoch 6\n",
      "batch 300\n",
      "training minibatch loss: 4.090489864349365\n",
      "  sample 1:\n",
      "    enc input           > gee i wonder why ? <EOS>\n",
      "    dec input           > sen chris coon donald trump passed on a meeting with the ukrainian president <EOS>\n",
      "    dec train predicted > why clinton frisk trump trump is in the republican in the debate police <EOS>\n",
      "dev minibatch loss: 6.4284162521362305\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > is your driving really that bad ? <EOS>\n",
      "    DEV dec input           > the face wa bc i know yo as gon na be scared of my driving <EOS>\n",
      "    DEV dec train predicted > new only of a i m it a <EOS> na be a <EOS> the day <EOS>\n",
      "    DEV dec train infer > new news in nyc <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3327\n",
      "learning rate 0.00094009\n",
      "epoch 6\n",
      "batch 350\n",
      "training minibatch loss: 4.357487678527832\n",
      "  sample 1:\n",
      "    enc input           > oh yeah ? that s awesome this is wonderful to hear we d appreciate any feedback you have <EOS>\n",
      "    dec input           > just watched really enjoyed <EOS>\n",
      "    dec train predicted > i got my a <EOS>\n",
      "dev minibatch loss: 6.001753807067871\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > wow you re disgusting <EOS>\n",
      "    DEV dec input           > trickle down like hillary s catheter <EOS>\n",
      "    DEV dec train predicted > i for now the s popularity in\n",
      "    DEV dec train infer > i m not a lot of a bit of a of sandwich you re a bit of them <EOS>\n",
      "global_step: 3377\n",
      "learning rate 0.000939217\n",
      "epoch 6\n",
      "batch 400\n",
      "training minibatch loss: 4.380509376525879\n",
      "  sample 1:\n",
      "    enc input           > think before you tweet <EOS>\n",
      "    dec input           > because you can vote more than once like voter fraud conservative complain <EOS>\n",
      "    dec train predicted > i i re t for <EOS> i can his welfare she kill <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 6.365240573883057\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > very important info to figure out very confused amp a speechless over here cc <EOS>\n",
      "    DEV dec input           > i m with some gentleman who need an education in <EOS>\n",
      "    DEV dec train predicted > is think not the problem conference s to apology via the\n",
      "    DEV dec train infer > is you been tired and trump is a lot of rapist to wx <EOS> <EOS> <EOS>\n",
      "global_step: 3427\n",
      "learning rate 0.000938346\n",
      "epoch 6\n",
      "batch 450\n",
      "training minibatch loss: 4.123628616333008\n",
      "  sample 1:\n",
      "    enc input           > the public see right through the establishment fluff after year of keeping u down stop it <EOS>\n",
      "    dec input           > thanks for watching <EOS>\n",
      "    dec train predicted > the for the <EOS>\n",
      "dev minibatch loss: 5.823311805725098\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > old black english teacher hello ? vampire student fuck me daddy <EOS>\n",
      "    DEV dec input           > what s up do you have game on your phone <EOS>\n",
      "    DEV dec train predicted > my to a for you have a <EOS> <EOS> hair <EOS>\n",
      "    DEV dec train infer > my birthday <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 3473\n",
      "learning rate 0.000937545\n",
      "epoch 7\n",
      "batch 0\n",
      "training minibatch loss: 3.5178864002227783\n",
      "  sample 1:\n",
      "    enc input           > the first episode not the second <EOS>\n",
      "    dec input           > have you played tell tale s batman yet ? <EOS>\n",
      "    dec train predicted > can you got got me ? time is ? <EOS>\n",
      "dev minibatch loss: 6.131705284118652\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > have a great time i ll want a review please like it could be anything but awesome <EOS>\n",
      "    DEV dec input           > i am so excited can t wait to see tonight thanks <EOS>\n",
      "    DEV dec train predicted > i m just excited to you wait to see you on for\n",
      "    DEV dec train infer > i m so proud of the morning you are not a good day on the first <EOS> <EOS>\n",
      "global_step: 3523\n",
      "learning rate 0.000936675\n",
      "epoch 7\n",
      "batch 50\n",
      "training minibatch loss: 3.6353721618652344\n",
      "  sample 1:\n",
      "    enc input           > other desk ? <EOS>\n",
      "    dec input           > what the hell ? <EOS>\n",
      "    dec train predicted > i ? fuck is <EOS>\n",
      "dev minibatch loss: 6.369766712188721\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we may take a break somewhere between la and san diego <EOS>\n",
      "    DEV dec input           > who is coming to ? we ll have prize from pm edt <EOS>\n",
      "    DEV dec train predicted > we to the for the <EOS> need be a than a on <EOS>\n",
      "    DEV dec train infer > we re surprised to be a responsibility responsibility to the killer of a relationship ? <EOS> <EOS> <EOS>\n",
      "global_step: 3573\n",
      "learning rate 0.000935805\n",
      "epoch 7\n",
      "batch 100\n",
      "training minibatch loss: 3.7133967876434326\n",
      "  sample 1:\n",
      "    enc input           > key win the lottery lol <EOS>\n",
      "    dec input           > where to get grand lol <EOS>\n",
      "    dec train predicted > i to the the ? <EOS>\n",
      "dev minibatch loss: 6.899381637573242\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > so true lmfao <EOS>\n",
      "    DEV dec input           > the sign a joanne the scammer gifs aquarius <EOS>\n",
      "    DEV dec train predicted > the best thing good a best decided of in\n",
      "    DEV dec train infer > the best thing i m a bit of bit and the best thing i am <EOS> <EOS>\n",
      "global_step: 3623\n",
      "learning rate 0.000934937\n",
      "epoch 7\n",
      "batch 150\n",
      "training minibatch loss: 3.770155906677246\n",
      "  sample 1:\n",
      "    enc input           > i can t watch this anymore wake me when we win the world series <EOS>\n",
      "    dec input           > blown save by committee <EOS>\n",
      "    dec train predicted > wait ta for <EOS> <EOS>\n",
      "dev minibatch loss: 6.218925952911377\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > actually read this <EOS>\n",
      "    DEV dec input           > here are some actual fact stop making it worse the myth of black life matter via <EOS>\n",
      "    DEV dec train predicted > the s the of bomb <EOS> now a on by new of the th <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > the giant is losing the background <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3673\n",
      "learning rate 0.000934069\n",
      "epoch 7\n",
      "batch 200\n",
      "training minibatch loss: 3.9725170135498047\n",
      "  sample 1:\n",
      "    enc input           > i wonder what he would do for ? <EOS>\n",
      "    dec input           > sudbury s finest <EOS>\n",
      "    dec train predicted > what s debate <EOS>\n",
      "dev minibatch loss: 6.608432292938232\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > congrats recognition well deserved <EOS>\n",
      "    DEV dec input           > i m so excited to be recognised a one of the by amp thank you <EOS>\n",
      "    DEV dec train predicted > recommend recommend getting excited to see selected <EOS> counter <EOS> the night bid <EOS> you <EOS>\n",
      "    DEV dec train infer > recommend best <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3723\n",
      "learning rate 0.000933202\n",
      "epoch 7\n",
      "batch 250\n",
      "training minibatch loss: 3.8088276386260986\n",
      "  sample 1:\n",
      "    enc input           > i m a firm believer he created said epidemic <EOS>\n",
      "    dec input           > but he also ignored an epidemic that killed off black people so that wa a benefit for them <EOS>\n",
      "    dec train predicted > i you wa won an opera in s for the black in i s a great <EOS> the <EOS>\n",
      "dev minibatch loss: 6.561863899230957\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > this will be the second time tony will miss <EOS>\n",
      "    DEV dec input           > join these handsome men sunday a they discus this week s episode of <EOS>\n",
      "    DEV dec train predicted > first the day to are cancel good need listening s <EOS> time is his\n",
      "    DEV dec train infer > first time to make a good time on amp s agenda and i will help <EOS> <EOS> <EOS>\n",
      "global_step: 3773\n",
      "learning rate 0.000932336\n",
      "epoch 7\n",
      "batch 300\n",
      "training minibatch loss: 3.565480947494507\n",
      "  sample 1:\n",
      "    enc input           > can we talk about growing the game during the lockout ? <EOS>\n",
      "    dec input           > eh that s a separate conversation until it isn t in the meantime gim me some money <EOS>\n",
      "    dec train predicted > surveillance i s a hired town on the s t the the meantime who me <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.5066399574279785\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > thought position would refer to something else tbh <EOS>\n",
      "    DEV dec input           > wow at my page for pride on the office of lgbt life website <EOS>\n",
      "    DEV dec train predicted > how i least mom <EOS> a <EOS> the world <EOS> the <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > how are you to be rid of that morning ? <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3823\n",
      "learning rate 0.000931471\n",
      "epoch 7\n",
      "batch 350\n",
      "training minibatch loss: 3.9637346267700195\n",
      "  sample 1:\n",
      "    enc input           > do the thing you can do it <EOS>\n",
      "    dec input           > doing something way out of my comfort zone tonight amp procrastinating by tweeting bc i m scared <EOS>\n",
      "    dec train predicted > saw the bad to of the life reality <EOS> <EOS> procrastinating for <EOS> on i m not <EOS>\n",
      "dev minibatch loss: 6.86889123916626\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > pure envy this sob is killing my back <EOS>\n",
      "    DEV dec input           > like my chair swoon <EOS>\n",
      "    DEV dec train predicted > this this whole on since\n",
      "    DEV dec train infer > this is a lot i m not a lot <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3873\n",
      "learning rate 0.000930607\n",
      "epoch 7\n",
      "batch 400\n",
      "training minibatch loss: 4.194057464599609\n",
      "  sample 1:\n",
      "    enc input           > i tweeted this too someone will recognize him surely how are you feeling ? <EOS>\n",
      "    dec input           > just in new photo of the shooting suspect in the can you help police identify him ? <EOS>\n",
      "    dec train predicted > a to the york of the debate say <EOS> the debate you say <EOS> say it <EOS> <EOS>\n",
      "dev minibatch loss: 6.979313850402832\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > yeah i saw it the day after it came out <EOS>\n",
      "    DEV dec input           > yeah i remember i went to the movie to watch believe movie when it came <EOS>\n",
      "    DEV dec train predicted > i it m my m to the same <EOS> pack <EOS> i is you s <EOS>\n",
      "    DEV dec train infer > i m so good to see it <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3923\n",
      "learning rate 0.000929743\n",
      "epoch 7\n",
      "batch 450\n",
      "training minibatch loss: 3.942885637283325\n",
      "  sample 1:\n",
      "    enc input           > this is a thing you can see in the window of the amsterdam red light district in use <EOS>\n",
      "    dec input           > it s like riding a bike and a dick at the same time <EOS>\n",
      "    dec train predicted > you s been a a spade <EOS> a spade in the city team <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 6.294200420379639\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > yea that wa rough <EOS>\n",
      "    DEV dec input           > oh god help you i hated that movie <EOS>\n",
      "    DEV dec train predicted > i i i you have had the <EOS> <EOS>\n",
      "    DEV dec train infer > i m not a lot of a year of a bit of a of job <EOS> <EOS>\n",
      "global_step: 3969\n",
      "learning rate 0.000928949\n",
      "epoch 8\n",
      "batch 0\n",
      "training minibatch loss: 3.4951188564300537\n",
      "  sample 1:\n",
      "    enc input           > that s hideous <EOS>\n",
      "    dec input           > if you have a weak stomach look away <EOS>\n",
      "    dec train predicted > i you re a good thing like in <EOS>\n",
      "dev minibatch loss: 6.508508205413818\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > dude seriously read his debate he should not be your favorite writer <EOS>\n",
      "    DEV dec input           > funny how my fav author get into quarrel with the other thinker i like such a pinker and <EOS>\n",
      "    DEV dec train predicted > just the the campaign opinion <EOS> the the and the debate <EOS> <EOS> ve <EOS> <EOS> white <EOS> <EOS>\n",
      "    DEV dec train infer > just want to be a new job <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4019\n",
      "learning rate 0.000928087\n",
      "epoch 8\n",
      "batch 50\n",
      "training minibatch loss: 3.1957881450653076\n",
      "  sample 1:\n",
      "    enc input           > my heart is broken i wanted to see you pitch <EOS>\n",
      "    dec input           > don t worry ppl can t get teammate sick taking precautionary measure at home and in clubhouse <EOS>\n",
      "    dec train predicted > just t get what do t get to candy described precautionary measure in home and it clubhouse <EOS>\n",
      "dev minibatch loss: 6.53085470199585\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that shit wa a bitch for me too <EOS>\n",
      "    DEV dec input           > it about time i passed my license <EOS>\n",
      "    DEV dec train predicted > i s me for m to soulmate iphones\n",
      "    DEV dec train infer > i m so hungry i m so hungry i m like that shit <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4069\n",
      "learning rate 0.000927226\n",
      "epoch 8\n",
      "batch 100\n",
      "training minibatch loss: 3.228706121444702\n",
      "  sample 1:\n",
      "    enc input           > or a faggot lol <EOS>\n",
      "    dec input           > or just responsible <EOS>\n",
      "    dec train predicted > i the going <EOS>\n",
      "dev minibatch loss: 6.6927571296691895\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > remember when folk tried to argue that fighter under couldn t be star ? <EOS>\n",
      "    DEV dec input           > yeah he s by far the biggest in ufc history i just did a whole thing on this <EOS>\n",
      "    DEV dec train predicted > she i ha a the a same jeeze my hmu <EOS> m wan <EOS> racist biggest <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > she said primary card <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4119\n",
      "learning rate 0.000926365\n",
      "epoch 8\n",
      "batch 150\n",
      "training minibatch loss: 3.5060739517211914\n",
      "  sample 1:\n",
      "    enc input           > good luck you got this <EOS>\n",
      "    dec input           > ready to sing my heart out <EOS>\n",
      "    dec train predicted > life to be the life <EOS> <EOS>\n",
      "dev minibatch loss: 6.893258094787598\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i would love to give you mine <EOS>\n",
      "    DEV dec input           > noooo feckless are so cute i wish i had some <EOS>\n",
      "    DEV dec train predicted > i you <EOS> you good <EOS> m i m to old\n",
      "    DEV dec train infer > i m going to be a headache so i m so proud of the best girl <EOS> <EOS>\n",
      "global_step: 4169\n",
      "learning rate 0.000925506\n",
      "epoch 8\n",
      "batch 200\n",
      "training minibatch loss: 3.530766248703003\n",
      "  sample 1:\n",
      "    enc input           > probably break up with him too right <EOS>\n",
      "    dec input           > ur right i would be <EOS>\n",
      "    dec train predicted > the a now m have a\n",
      "dev minibatch loss: 7.206848621368408\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > well rn i m the rightful owner so <EOS>\n",
      "    DEV dec input           > i just give the child to it s rightful owner <EOS>\n",
      "    DEV dec train predicted > my m had him cheesecake <EOS> get <EOS> bad <EOS> <EOS>\n",
      "    DEV dec train infer > my catfish in my buffoon go so i m a rented motion <EOS> <EOS> <EOS>\n",
      "global_step: 4219\n",
      "learning rate 0.000924647\n",
      "epoch 8\n",
      "batch 250\n",
      "training minibatch loss: 3.716610908508301\n",
      "  sample 1:\n",
      "    enc input           > word shit dick <EOS>\n",
      "    dec input           > there is truly nothing worse than chapped lip <EOS>\n",
      "    dec train predicted > i is like nothing a than chapped <EOS> <EOS>\n",
      "dev minibatch loss: 6.5948381423950195\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > so is global warming and all the other b that democrat spew <EOS>\n",
      "    DEV dec input           > superbug the greatest and most urgent global risk <EOS>\n",
      "    DEV dec train predicted > i people bribe republic not size idea intervened of\n",
      "    DEV dec train infer > i m not offended but she is not worse and identify the same <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4269\n",
      "learning rate 0.000923789\n",
      "epoch 8\n",
      "batch 300\n",
      "training minibatch loss: 3.5532195568084717\n",
      "  sample 1:\n",
      "    enc input           > the sauce not you pardon me <EOS>\n",
      "    dec input           > i refuse to believe that can taste good <EOS>\n",
      "    dec train predicted > i don to see my i fuck one <EOS>\n",
      "dev minibatch loss: 6.575474739074707\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > reason why school is satan <EOS>\n",
      "    DEV dec input           > i fell down stair at school and re sprained the same ankle i already sprained <EOS>\n",
      "    DEV dec train predicted > what just to to what least is he doing and steelers <EOS> <EOS> m <EOS> <EOS>\n",
      "    DEV dec train infer > what s a christian is stirring <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4319\n",
      "learning rate 0.000922931\n",
      "epoch 8\n",
      "batch 350\n",
      "training minibatch loss: 3.468414068222046\n",
      "  sample 1:\n",
      "    enc input           > so happy you love them <EOS>\n",
      "    dec input           > i live for instagram story <EOS>\n",
      "    dec train predicted > and m in a girl <EOS>\n",
      "dev minibatch loss: 6.956050395965576\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > here is your assignment read stolen legacy then come back with an intelligent tweet <EOS>\n",
      "    DEV dec input           > behave shouldn t you be doing your homework ? <EOS>\n",
      "    DEV dec train predicted > the in t the be a this name <EOS> <EOS>\n",
      "    DEV dec train infer > the status of boycotting in the vet and order to <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4369\n",
      "learning rate 0.000922075\n",
      "epoch 8\n",
      "batch 400\n",
      "training minibatch loss: 3.642611265182495\n",
      "  sample 1:\n",
      "    enc input           > please hannah <EOS>\n",
      "    dec input           > i want to i ll be home <EOS>\n",
      "    dec train predicted > i m to have have be in <EOS>\n",
      "dev minibatch loss: 6.694223403930664\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > no i m not even starting on saturday morning cartoon i ve accepted those question go unanswered <EOS>\n",
      "    DEV dec input           > i don t know why the joes never shot the cobra either <EOS>\n",
      "    DEV dec train predicted > i m t know what i one is got <EOS> bad is <EOS>\n",
      "    DEV dec train infer > i m sorry i wa not a bad time to be a bad time <EOS> <EOS> <EOS>\n",
      "global_step: 4419\n",
      "learning rate 0.000921219\n",
      "epoch 8\n",
      "batch 450\n",
      "training minibatch loss: 3.6523873805999756\n",
      "  sample 1:\n",
      "    enc input           > yeah that s right home depot dave bill and sherman gon na f ck you up <EOS>\n",
      "    dec input           > so watch your back rude clerk at home depot <EOS>\n",
      "    dec train predicted > so you the <EOS> like clerk in high depot <EOS>\n",
      "dev minibatch loss: 6.852850914001465\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > ricky you still trying to convince people that lv is real just stop dude <EOS>\n",
      "    DEV dec input           > they have a site russell rd <EOS>\n",
      "    DEV dec train predicted > well have a well of action and\n",
      "    DEV dec train infer > well you are a well a well a you guy <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4465\n",
      "learning rate 0.000920433\n",
      "epoch 9\n",
      "batch 0\n",
      "training minibatch loss: 3.1187844276428223\n",
      "  sample 1:\n",
      "    enc input           > here come making his way through the italian market just in time to make the steelers eagle game <EOS>\n",
      "    dec input           > said he would walk home if the phillies lost after they were up let go mets <EOS>\n",
      "    dec train predicted > can i s be out yet they other would in i are out <EOS> do <EOS> <EOS>\n",
      "dev minibatch loss: 6.584978103637695\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i can t imagine all the people who have to drive on grand central daily during this hoopla <EOS>\n",
      "    DEV dec input           > note to self laguardia airport is simply <EOS>\n",
      "    DEV dec train predicted > i to be add opinion is not that\n",
      "    DEV dec train infer > i don t know what you think ? <EOS> ? <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 4515\n",
      "learning rate 0.000919578\n",
      "epoch 9\n",
      "batch 50\n",
      "training minibatch loss: 2.8853261470794678\n",
      "  sample 1:\n",
      "    enc input           > repeal and replace with what sir ? <EOS>\n",
      "    dec input           > but you definitely should not have to pay this tax after your co op fails <EOS>\n",
      "    dec train predicted > how the re will have have to tax the and ? the tv set fails <EOS>\n",
      "dev minibatch loss: 7.230550289154053\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that s le than an iphone dongle <EOS>\n",
      "    DEV dec input           > an issue these day ? ? ? <EOS>\n",
      "    DEV dec train predicted > you iphone ? ? <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > you re a sadness <EOS> ? <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4565\n",
      "learning rate 0.000918725\n",
      "epoch 9\n",
      "batch 100\n",
      "training minibatch loss: 2.9936718940734863\n",
      "  sample 1:\n",
      "    enc input           > omg i did that friday they wont wash out <EOS>\n",
      "    dec input           > i spilled popper on my fitted sheet so fuck my sleep tonight <EOS>\n",
      "    dec train predicted > i spilled popper in my fitted sheet in sweet my fitted <EOS> <EOS>\n",
      "dev minibatch loss: 8.026885986328125\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > no black for birther no latino for wall no female for end of pp <EOS>\n",
      "    DEV dec input           > i still won t tell u how to vote tho <EOS>\n",
      "    DEV dec train predicted > poll wonder have t think it that he end that <EOS>\n",
      "    DEV dec train infer > poll he s done in california ? wa the chinese of the constitution <EOS> <EOS>\n",
      "global_step: 4615\n",
      "learning rate 0.000917872\n",
      "epoch 9\n",
      "batch 150\n",
      "training minibatch loss: 3.190159559249878\n",
      "  sample 1:\n",
      "    enc input           > how much a piece <EOS>\n",
      "    dec input           > guy i have four ticket to chance in toronto tmrw somebody pls buy them off of me <EOS>\n",
      "    dec train predicted > watching is m a ticket to toronto on toronto tmrw toronto playing wait me to <EOS> the <EOS>\n",
      "dev minibatch loss: 7.597883701324463\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we need a new theology of money <EOS>\n",
      "    DEV dec input           > thought on this thread ? <EOS>\n",
      "    DEV dec train predicted > remember ? ? c ? <EOS>\n",
      "    DEV dec train infer > remember what ? <EOS> ? <EOS> ? <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4665\n",
      "learning rate 0.00091702\n",
      "epoch 9\n",
      "batch 200\n",
      "training minibatch loss: 3.0734856128692627\n",
      "  sample 1:\n",
      "    enc input           > ugh fat people in bed are so good at cyber <EOS>\n",
      "    dec input           > the hacker weren t russian they were fat people on their bed <EOS>\n",
      "    dec train predicted > no russian say t people people re not people in their own <EOS>\n",
      "dev minibatch loss: 7.825577735900879\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > greeting from new york when you watch go through the tweet hilarious thought provoking insightful <EOS>\n",
      "    DEV dec input           > i ll know what you re referring to when i watch later tweeting from taiwan <EOS>\n",
      "    DEV dec train predicted > survivor ll submit but happens think not to you trump ll the romney ? unseat <EOS>\n",
      "    DEV dec train infer > survivor warren s private than frisk leak <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4715\n",
      "learning rate 0.000916169\n",
      "epoch 9\n",
      "batch 250\n",
      "training minibatch loss: 3.120008945465088\n",
      "  sample 1:\n",
      "    enc input           > that moment plus his comment on housing <EOS>\n",
      "    dec input           > that moment is going to stick trump seemingly bragging about not paying federal income tax <EOS>\n",
      "    dec train predicted > he s is not to be out to sniffle to her not flower tax tax <EOS>\n",
      "dev minibatch loss: 7.42387056350708\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > smh don t disrespect me like that <EOS>\n",
      "    DEV dec input           > nope i m still convinced you went to class and got this from the internet <EOS>\n",
      "    DEV dec train predicted > i my know mad a a re in my <EOS> i me <EOS> <EOS> album <EOS>\n",
      "    DEV dec train infer > i know i m going to take a bit that s a bit <EOS> <EOS> <EOS>\n",
      "global_step: 4765\n",
      "learning rate 0.000915319\n",
      "epoch 9\n",
      "batch 300\n",
      "training minibatch loss: 3.0006093978881836\n",
      "  sample 1:\n",
      "    enc input           > how often did you get a failing grade for doing too much homework ? <EOS>\n",
      "    dec input           > exposed trump s lack of preparation but clinton seemed over prepared at time <EOS>\n",
      "    dec train predicted > exposed trump s lack of preparation and troll seemed in prepared in time <EOS>\n",
      "dev minibatch loss: 6.948620319366455\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you got that right <EOS>\n",
      "    DEV dec input           > the shouldve tried the men on the field approach much earlier in this game <EOS>\n",
      "    DEV dec train predicted > not struggle been to fuck to the first is in <EOS> <EOS> <EOS> <EOS> ?\n",
      "    DEV dec train infer > not about the world u even i m not a bit of the world you ever <EOS>\n",
      "global_step: 4815\n",
      "learning rate 0.00091447\n",
      "epoch 9\n",
      "batch 350\n",
      "training minibatch loss: 3.2692363262176514\n",
      "  sample 1:\n",
      "    enc input           > shut up broke boi <EOS>\n",
      "    dec input           > thanks loser <EOS>\n",
      "    dec train predicted > omg baby <EOS>\n",
      "dev minibatch loss: 7.5780510902404785\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i mean playing with your own boob can be quite amusing that guy know wassup <EOS>\n",
      "    DEV dec input           > oh the first gif <EOS>\n",
      "    DEV dec train predicted > founder congrats internet photo is\n",
      "    DEV dec train infer > founder say this is a great day <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4865\n",
      "learning rate 0.000913621\n",
      "epoch 9\n",
      "batch 400\n",
      "training minibatch loss: 3.304244041442871\n",
      "  sample 1:\n",
      "    enc input           > already ? it wa in theater just week ago <EOS>\n",
      "    dec input           > blazing saddle on right now cinemax <EOS>\n",
      "    dec train predicted > no cloud in in now cinemax <EOS>\n",
      "dev minibatch loss: 6.966726779937744\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i d forgotten how hot he wa <EOS>\n",
      "    DEV dec input           > here s the painting paid k for using other people s charity money <EOS>\n",
      "    DEV dec train predicted > i s the truth the ? <EOS> the the guy ? life in in\n",
      "    DEV dec train infer > i m not aware about the truth how you are a bad thing <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4915\n",
      "learning rate 0.000912773\n",
      "epoch 9\n",
      "batch 450\n",
      "training minibatch loss: 3.3462941646575928\n",
      "  sample 1:\n",
      "    enc input           > what are those called i ve never seen them ? <EOS>\n",
      "    dec input           > trivia ? which nba slam dunk champion wore this shoe in the white blue colorway ? <EOS>\n",
      "    dec train predicted > trivia ? what marlin slam commercial champion figure the game <EOS> the net interruption colorway ? <EOS>\n",
      "dev minibatch loss: 7.346278667449951\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the same thing u are <EOS>\n",
      "    DEV dec input           > what are you thinking about <EOS>\n",
      "    DEV dec train predicted > i i you going i the\n",
      "    DEV dec train infer > i hate censorship and i just hate myself <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 4961\n",
      "learning rate 0.000911994\n",
      "epoch 10\n",
      "batch 0\n",
      "training minibatch loss: 2.946162223815918\n",
      "  sample 1:\n",
      "    enc input           > fair enough come to sf and be sketchy w me <EOS>\n",
      "    dec input           > hahaha i can t put on twitter <EOS>\n",
      "    dec train predicted > ugh i m t wait to mah and\n",
      "dev minibatch loss: 7.002714157104492\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > omfg youre so cute i love you sm <EOS>\n",
      "    DEV dec input           > we gon na fail chem together but at least she s gon na look good while doing it <EOS>\n",
      "    DEV dec train predicted > you are na be you and <EOS> you all i are pretty na be <EOS> a you <EOS> <EOS>\n",
      "    DEV dec train infer > you re so happy and you are pretty happy and happy and love you <EOS> <EOS>\n",
      "global_step: 5011\n",
      "learning rate 0.000911148\n",
      "epoch 10\n",
      "batch 50\n",
      "training minibatch loss: 2.663841485977173\n",
      "  sample 1:\n",
      "    enc input           > if you re talking to me yes my profile pic is of my actual face during summer time <EOS>\n",
      "    dec input           > is that your actual picture ? pct a real a mine <EOS>\n",
      "    dec train predicted > is that a time time ? <EOS> a a time you <EOS>\n",
      "dev minibatch loss: 8.09421157836914\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > this is going to bang pun intended <EOS>\n",
      "    DEV dec input           > trump only want stop and frisk so he ha an excuse for feeling up ivanka <EOS>\n",
      "    DEV dec train predicted > is continues number a by butter the we already a original <EOS> the the above <EOS>\n",
      "    DEV dec train infer > is the new number and this is getting out of nyc <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 5061\n",
      "learning rate 0.000910302\n",
      "epoch 10\n",
      "batch 100\n",
      "training minibatch loss: 2.726698637008667\n",
      "  sample 1:\n",
      "    enc input           > receipt on that ? <EOS>\n",
      "    dec input           > sound just like hillary and her email while secretary of state <EOS>\n",
      "    dec train predicted > just a seen the and clinton debate secretary secretary of muslim ?\n",
      "dev minibatch loss: 7.703882217407227\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > before the season started i would have said siemian now i would say keenum <EOS>\n",
      "    DEV dec input           > who will be first qb to be benched ? blaine gabbert case keenum kirk cousin trevor siemian <EOS>\n",
      "    DEV dec train predicted > the i the a a to get a <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > the best thing wa not a good thing i can t have a good position <EOS> <EOS> <EOS>\n",
      "global_step: 5111\n",
      "learning rate 0.000909457\n",
      "epoch 10\n",
      "batch 150\n",
      "training minibatch loss: 2.8318440914154053\n",
      "  sample 1:\n",
      "    enc input           > thanks any regret ? would it have been possible to replace the router first instead of last ? <EOS>\n",
      "    dec input           > we did it a the last piece of the puzzle here the pr <EOS>\n",
      "    dec train predicted > i are the that a piece time of the pr for it pr <EOS>\n",
      "dev minibatch loss: 7.665948390960693\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > same so i guess they really meant freshman <EOS>\n",
      "    DEV dec input           > however i did lose pound <EOS>\n",
      "    DEV dec train predicted > i i m the the of\n",
      "    DEV dec train infer > i m just worried about to see it <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5161\n",
      "learning rate 0.000908613\n",
      "epoch 10\n",
      "batch 200\n",
      "training minibatch loss: 2.821960210800171\n",
      "  sample 1:\n",
      "    enc input           > she s actually disgusting like who tf go to that after someone hott like u <EOS>\n",
      "    dec input           > he s just upset he downgraded <EOS>\n",
      "    dec train predicted > i s so wit that downgraded <EOS>\n",
      "dev minibatch loss: 7.557650089263916\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what you mean lol <EOS>\n",
      "    DEV dec input           > your giveaway are definitely awesome wish i could figure em out lol <EOS>\n",
      "    DEV dec train predicted > i emotion <EOS> definitely af <EOS> <EOS> m be to <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m scared in my heart forever left <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5211\n",
      "learning rate 0.00090777\n",
      "epoch 10\n",
      "batch 250\n",
      "training minibatch loss: 2.8009214401245117\n",
      "  sample 1:\n",
      "    enc input           > great to see you <EOS>\n",
      "    dec input           > great meeting with alum at school of management <EOS>\n",
      "    dec train predicted > good back for <EOS> to pm <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 8.16182804107666\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > better than fap meat <EOS>\n",
      "    DEV dec input           > flap meat ? no i m good thanks <EOS>\n",
      "    DEV dec train predicted > i how and <EOS> <EOS> have on <EOS> on\n",
      "    DEV dec train infer > i m at the gym of my inbox <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5261\n",
      "learning rate 0.000906928\n",
      "epoch 10\n",
      "batch 300\n",
      "training minibatch loss: 3.1624338626861572\n",
      "  sample 1:\n",
      "    enc input           > make sense to visit the airport to speak to someone in person ? <EOS>\n",
      "    dec input           > our centralized baggage office can be contacted at bz <EOS>\n",
      "    dec train predicted > the centralized baggage customer are be contacted to bz <EOS>\n",
      "dev minibatch loss: 7.852435111999512\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > proud to be a trump supporter and reject racist genocidal hillary clinton because of this <EOS>\n",
      "    DEV dec input           > this is a day after st debate million in fundraising k attendance in melbourne fl <EOS>\n",
      "    DEV dec train predicted > the is a historic checker trump is obama supporter black vote news of trump racism are\n",
      "    DEV dec train infer > the people who want a woman who want a woman to vote for the debate obama <EOS> <EOS>\n",
      "global_step: 5311\n",
      "learning rate 0.000906086\n",
      "epoch 10\n",
      "batch 350\n",
      "training minibatch loss: 2.8706212043762207\n",
      "  sample 1:\n",
      "    enc input           > that dead as get me sooo fucking angry <EOS>\n",
      "    dec input           > when you could taste it nd get there nd it s gone <EOS>\n",
      "    dec train predicted > i i re get it movie a there movie that s not <EOS>\n",
      "dev minibatch loss: 6.915594100952148\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > im pretty sure it wa completely sarcastic at least i hope <EOS>\n",
      "    DEV dec input           > i read that this morning and didn t understand anything it said <EOS>\n",
      "    DEV dec train predicted > i m it i is <EOS> i t make i <EOS> s <EOS>\n",
      "    DEV dec train infer > i m not jealous of this morning <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5361\n",
      "learning rate 0.000905245\n",
      "epoch 10\n",
      "batch 400\n",
      "training minibatch loss: 2.835979461669922\n",
      "  sample 1:\n",
      "    enc input           > omg she is awesome we re all american how great is that <EOS>\n",
      "    dec input           > clinton is trying to get this video removed watch before lawyer get involved <EOS>\n",
      "    dec train predicted > can is tired to preach on video video video i losing play missing <EOS>\n",
      "dev minibatch loss: 7.53608512878418\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > story of our life <EOS>\n",
      "    DEV dec input           > i left oakland and the sun wa out i arrive at daly city amp it s dark <EOS>\n",
      "    DEV dec train predicted > where m a about trying successful will trying there m off south <EOS> a <EOS> s available <EOS>\n",
      "    DEV dec train infer > where are you going in the next double you are more eye ? <EOS> <EOS> ? <EOS>\n",
      "global_step: 5411\n",
      "learning rate 0.000904405\n",
      "epoch 10\n",
      "batch 450\n",
      "training minibatch loss: 2.868638038635254\n",
      "  sample 1:\n",
      "    enc input           > are you in new york ? i m here for unga <EOS>\n",
      "    dec input           > the ability to have cross sector conversation and collaboration is critical to expand <EOS>\n",
      "    dec train predicted > i ability to see representing sector version and collaboration is having to expand <EOS>\n",
      "dev minibatch loss: 7.381775379180908\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > pretty good <EOS>\n",
      "    DEV dec input           > how are you lady doing ? <EOS>\n",
      "    DEV dec train predicted > what is you doing ? the <EOS>\n",
      "    DEV dec train infer > what is the best thing i m a good day than the world <EOS> <EOS> <EOS>\n",
      "global_step: 5457\n",
      "learning rate 0.000903633\n",
      "epoch 11\n",
      "batch 0\n",
      "training minibatch loss: 2.4795095920562744\n",
      "  sample 1:\n",
      "    enc input           > lmfao omg yes <EOS>\n",
      "    dec input           > lol you this year <EOS>\n",
      "    dec train predicted > i you all <EOS> <EOS>\n",
      "dev minibatch loss: 8.061384201049805\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > are you talking about trump ? <EOS>\n",
      "    DEV dec input           > anyone else use to stare at this racist <EOS>\n",
      "    DEV dec train predicted > trump who hillary hillary trump for trump donald hillary\n",
      "    DEV dec train infer > trump created the consistently rigged campaign on lie <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5507\n",
      "learning rate 0.000902794\n",
      "epoch 11\n",
      "batch 50\n",
      "training minibatch loss: 2.492682695388794\n",
      "  sample 1:\n",
      "    enc input           > we re watching nothing without you whit so far so good <EOS>\n",
      "    dec input           > another great of see it today on amp <EOS>\n",
      "    dec train predicted > a great for nyc u <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 7.6866455078125\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we are walking through the city can t take off our clothes yet <EOS>\n",
      "    DEV dec input           > seems modest wardrobe from all the story i hear about folsom <EOS>\n",
      "    DEV dec train predicted > we you to with pc with above paid m paid refusing into\n",
      "    DEV dec train infer > we are experiencing two home with me planning and get paid with me <EOS> <EOS> <EOS>\n",
      "global_step: 5557\n",
      "learning rate 0.000901956\n",
      "epoch 11\n",
      "batch 100\n",
      "training minibatch loss: 2.4753479957580566\n",
      "  sample 1:\n",
      "    enc input           > mm in an hour holy crap <EOS>\n",
      "    dec input           > like this in i miss my rd floor apt <EOS>\n",
      "    dec train predicted > not the <EOS> the m the rd ny apt <EOS>\n",
      "dev minibatch loss: 8.284676551818848\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i wa on the way there before i saw youd already arrived and set up shop <EOS>\n",
      "    DEV dec input           > ty for the setup <EOS>\n",
      "    DEV dec train predicted > hey in the worst of\n",
      "    DEV dec train infer > hey god i love this show <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 5607\n",
      "learning rate 0.000901119\n",
      "epoch 11\n",
      "batch 150\n",
      "training minibatch loss: 2.5784432888031006\n",
      "  sample 1:\n",
      "    enc input           > that s the best tweet for the night <EOS>\n",
      "    dec input           > we really cant <EOS>\n",
      "    dec train predicted > i are s <EOS>\n",
      "dev minibatch loss: 8.986289978027344\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i didn t know what to call it <EOS>\n",
      "    DEV dec input           > i don t think of the console a the dash ? <EOS>\n",
      "    DEV dec train predicted > no m t have it you same trade single business whip <EOS>\n",
      "    DEV dec train infer > no wonder you re going to say it s in the franchise <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5657\n",
      "learning rate 0.000900283\n",
      "epoch 11\n",
      "batch 200\n",
      "training minibatch loss: 2.557586431503296\n",
      "  sample 1:\n",
      "    enc input           > beautiful picture <EOS>\n",
      "    dec input           > good evening it s how could i forget ? enjoy <EOS>\n",
      "    dec train predicted > good morning i s so i you follow <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 7.451931953430176\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the republican suggestion ha been to arm teacher <EOS>\n",
      "    DEV dec input           > don t worry trump s going to fix it all right ? <EOS>\n",
      "    DEV dec train predicted > and t do it will normal into lose to <EOS> effectively now <EOS>\n",
      "    DEV dec train infer > and if you re offended to the wall his wall u it to lose <EOS> <EOS>\n",
      "global_step: 5707\n",
      "learning rate 0.000899447\n",
      "epoch 11\n",
      "batch 250\n",
      "training minibatch loss: 2.7265217304229736\n",
      "  sample 1:\n",
      "    enc input           > really special thank you guy <EOS>\n",
      "    dec input           > celebrate the historic opening of the by learning about the legacy of african american at the white house <EOS>\n",
      "    dec train predicted > just the voice community and the of dump about the police of african american <EOS> the world market <EOS>\n",
      "dev minibatch loss: 8.109984397888184\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > they also said chad kelley potentially is the best passer in america <EOS>\n",
      "    DEV dec input           > espn s fpi say florida ha a chance to beat tennessee that s funny <EOS>\n",
      "    DEV dec train predicted > especially video presidential troll i all a joke and the people rudy troll poorly attack\n",
      "    DEV dec train infer > especially and cheating are racist on than the most joy and the constitution <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5757\n",
      "learning rate 0.000898613\n",
      "epoch 11\n",
      "batch 300\n",
      "training minibatch loss: 2.647693634033203\n",
      "  sample 1:\n",
      "    enc input           > that s what i said too when i saw it <EOS>\n",
      "    dec input           > it s so nice <EOS>\n",
      "    dec train predicted > i s so strange <EOS>\n",
      "dev minibatch loss: 8.882427215576172\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > lol omg stop <EOS>\n",
      "    DEV dec input           > grabbed an old ski jacket out of the closet and found a butterscotch candy in me pocket <EOS>\n",
      "    DEV dec train predicted > that my old bottle <EOS> <EOS> <EOS> day west <EOS> block a bottle bottle <EOS> school <EOS> <EOS>\n",
      "    DEV dec train infer > that s my friend and i m so much a bottle <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5807\n",
      "learning rate 0.000897779\n",
      "epoch 11\n",
      "batch 350\n",
      "training minibatch loss: 2.508692502975464\n",
      "  sample 1:\n",
      "    enc input           > man lol livermore suck on the weekday oakland is way better <EOS>\n",
      "    dec input           > try ubering at livermore bart if u haven t already <EOS>\n",
      "    dec train predicted > fuck ubering in livermore bart ? yall haven t like <EOS>\n",
      "dev minibatch loss: 7.875819683074951\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > some couple should come with an off switch <EOS>\n",
      "    DEV dec input           > didn t see context so i read that pair a meaning romantic couple <EOS>\n",
      "    DEV dec train predicted > why t worry the ? you think to the of word of wife <EOS>\n",
      "    DEV dec train infer > why are the word thing you re full of word ? <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5857\n",
      "learning rate 0.000896946\n",
      "epoch 11\n",
      "batch 400\n",
      "training minibatch loss: 2.7764530181884766\n",
      "  sample 1:\n",
      "    enc input           > also a few of tax change that primarily deal with investment income here are detail analysis <EOS>\n",
      "    dec input           > doe that mean a tax increase too ? ? i can t afford that <EOS>\n",
      "    dec train predicted > doe the pay a increase increase ? <EOS> ? ? know t believe it <EOS>\n",
      "dev minibatch loss: 7.8312668800354\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > trumpsplaining deliver birtherism x yr minus birtherism x hillary no apology needed <EOS>\n",
      "    DEV dec input           > fact check false like really false <EOS>\n",
      "    DEV dec train predicted > new john john th the reason truth\n",
      "    DEV dec train infer > new york th while th couch and rally for clinton s solution <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5907\n",
      "learning rate 0.000896113\n",
      "epoch 11\n",
      "batch 450\n",
      "training minibatch loss: 2.6466434001922607\n",
      "  sample 1:\n",
      "    enc input           > why postpone ? ? ? more cowardly congressman being bought off ? <EOS>\n",
      "    dec input           > gop leader postpone irs commissioner impeachment plan until after november election <EOS>\n",
      "    dec train predicted > gop leader field irs commissioner impeachment leader at now trump court <EOS>\n",
      "dev minibatch loss: 8.05434513092041\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > also someone mite put in effort for the wrong reason <EOS>\n",
      "    DEV dec input           > effort is the best indicator of interest paul carrick brunson <EOS>\n",
      "    DEV dec train predicted > s is the question question pairing the question ? to <EOS>\n",
      "    DEV dec train infer > s wk tell me to do the question ? <EOS> ? <EOS> ? <EOS> <EOS> <EOS>\n",
      "global_step: 5953\n",
      "learning rate 0.000895348\n",
      "epoch 12\n",
      "batch 0\n",
      "training minibatch loss: 2.2295944690704346\n",
      "  sample 1:\n",
      "    enc input           > literally same but i m so bored idk what s worse lol <EOS>\n",
      "    dec input           > work is so boring today and after last week i am so damn thankful for that <EOS>\n",
      "    dec train predicted > seeing is my boring and <EOS> i my night i m so damn thankful for you <EOS>\n",
      "dev minibatch loss: 7.10677433013916\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > right back at ya babe <EOS>\n",
      "    DEV dec input           > since i know you re a few cup deep already <EOS>\n",
      "    DEV dec train predicted > i wait guess man ve in word thing is on <EOS>\n",
      "    DEV dec train infer > i guess i guess i m forever whoop <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6003\n",
      "learning rate 0.000894517\n",
      "epoch 12\n",
      "batch 50\n",
      "training minibatch loss: 2.101531982421875\n",
      "  sample 1:\n",
      "    enc input           > i control what i want to do to you <EOS>\n",
      "    dec input           > do it then i want your mouth on me <EOS>\n",
      "    dec train predicted > can you you i m to mouth to <EOS> ?\n",
      "dev minibatch loss: 8.41179370880127\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > add the total from the male african american column divide unknown by proportion add to equation <EOS>\n",
      "    DEV dec input           > that wa a legit question <EOS>\n",
      "    DEV dec train predicted > tune crush in new government <EOS>\n",
      "    DEV dec train infer > tune news in in in new york and the government are written <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6053\n",
      "learning rate 0.000893687\n",
      "epoch 12\n",
      "batch 100\n",
      "training minibatch loss: 2.225071907043457\n",
      "  sample 1:\n",
      "    enc input           > harrison would have walked out on him st day <EOS>\n",
      "    dec input           > harrison ford would be great with hitch <EOS>\n",
      "    dec train predicted > harrison ford don be a to hitch <EOS>\n",
      "dev minibatch loss: 8.266407012939453\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > no i m saying the exact opposite <EOS>\n",
      "    DEV dec input           > so if yall hit u it s alright ? <EOS>\n",
      "    DEV dec train predicted > i i you see my i s awesome <EOS> <EOS>\n",
      "    DEV dec train infer > i m so happy i hope you have a good time <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6103\n",
      "learning rate 0.000892858\n",
      "epoch 12\n",
      "batch 150\n",
      "training minibatch loss: 2.276909351348877\n",
      "  sample 1:\n",
      "    enc input           > curious a to whether these number reflect self identifying jew or what what s a jew ? <EOS>\n",
      "    dec input           > jewish population million u israel france canada argentina russia <EOS>\n",
      "    dec train predicted > police say shooting the israel france canada argentina russia <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 9.231988906860352\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > u mean pregnant with his child again ? <EOS>\n",
      "    DEV dec input           > omg omg omg omg the girl who s baby father punched her in the stomach is pregnant <EOS>\n",
      "    DEV dec train predicted > followed anthony let i <EOS> <EOS> i didn <EOS> <EOS> ha <EOS> pm nyc <EOS> <EOS> now <EOS>\n",
      "    DEV dec train infer > followed yelling goat but i have <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6153\n",
      "learning rate 0.000892029\n",
      "epoch 12\n",
      "batch 200\n",
      "training minibatch loss: 2.5042667388916016\n",
      "  sample 1:\n",
      "    enc input           > it wa shameful to watch those question happens the people that asked them should feel ashamed <EOS>\n",
      "    dec input           > and no one care about your electric bus <EOS>\n",
      "    dec train predicted > and i one think about the own debate <EOS>\n",
      "dev minibatch loss: 8.666864395141602\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i think the whole off bway prod might be on youtube <EOS>\n",
      "    DEV dec input           > my kingdom for a ticket to this production <EOS>\n",
      "    DEV dec train predicted > don opinion is white white <EOS> change question or\n",
      "    DEV dec train infer > don t be going like a successful orange reagan i will be a conspiracy question ? <EOS>\n",
      "global_step: 6203\n",
      "learning rate 0.000891201\n",
      "epoch 12\n",
      "batch 250\n",
      "training minibatch loss: 2.4491264820098877\n",
      "  sample 1:\n",
      "    enc input           > i subscribed but now i m slightly worried that i m effectively vendor locked <EOS>\n",
      "    dec input           > both for me i refuse to use the subscription service i think i m being punished <EOS>\n",
      "    dec train predicted > both for the <EOS> m to stop the debate witch i m i m not punished <EOS>\n",
      "dev minibatch loss: 7.959414958953857\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > and your english teacher certainly failed you <EOS>\n",
      "    DEV dec input           > hillary s been failing for year in not getting the job done it will never change <EOS>\n",
      "    DEV dec train predicted > my get been six for music and a so a music <EOS> <EOS> <EOS> get get <EOS>\n",
      "    DEV dec train infer > my reply ha been lit up and cheese music and cheese and cheese and cheese <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6253\n",
      "learning rate 0.000890374\n",
      "epoch 12\n",
      "batch 300\n",
      "training minibatch loss: 2.4367613792419434\n",
      "  sample 1:\n",
      "    enc input           > the purge mcdonalds <EOS>\n",
      "    dec input           > im on a pilgrimage amp no one can stop me <EOS>\n",
      "    dec train predicted > i the the thing <EOS> the one do get to <EOS>\n",
      "dev minibatch loss: 8.609602928161621\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > pretty good <EOS>\n",
      "    DEV dec input           > how are you lady doing ? <EOS>\n",
      "    DEV dec train predicted > no is you doing ? the <EOS>\n",
      "    DEV dec train infer > no the way to make the way to get a new job <EOS> <EOS> <EOS>\n",
      "global_step: 6303\n",
      "learning rate 0.000889548\n",
      "epoch 12\n",
      "batch 350\n",
      "training minibatch loss: 2.4774534702301025\n",
      "  sample 1:\n",
      "    enc input           > probably trying to humanize himself and not being seen a sexist <EOS>\n",
      "    dec input           > what wa that ? wa he trying to be funny ? <EOS>\n",
      "    dec train predicted > but s that he she about s to be talking ? <EOS>\n",
      "dev minibatch loss: 9.23855972290039\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > may and also twitter please put it to where i can edit tweet not have to delete them <EOS>\n",
      "    DEV dec input           > yes please also why did you move setup if i kay ask ? <EOS>\n",
      "    DEV dec train predicted > or how politics be wonder you believe to of you would a themselves <EOS>\n",
      "    DEV dec train infer > or why you need a politician ? it would be shaped guessing you need to vote <EOS>\n",
      "global_step: 6353\n",
      "learning rate 0.000888722\n",
      "epoch 12\n",
      "batch 400\n",
      "training minibatch loss: 2.495029926300049\n",
      "  sample 1:\n",
      "    enc input           > link source for tw <EOS>\n",
      "    dec input           > chelsea manhattan explosion causing injury reported by local amp link in reply <EOS>\n",
      "    dec train predicted > africa panel explosion causing africa reported by africa game clinton on pm <EOS>\n",
      "dev minibatch loss: 8.218965530395508\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > she s biting her tongue so hard good the office reference <EOS>\n",
      "    DEV dec input           > hillary is straight up jim halpert right now <EOS>\n",
      "    DEV dec train predicted > i is a a and old and now <EOS>\n",
      "    DEV dec train infer > i love michael of michael nation debate et her life is due to manhattan <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6403\n",
      "learning rate 0.000887898\n",
      "epoch 12\n",
      "batch 450\n",
      "training minibatch loss: 2.6081316471099854\n",
      "  sample 1:\n",
      "    enc input           > you re a babe stop <EOS>\n",
      "    dec input           > my biggest regret in life is that i m too old and too ugly to appear on <EOS>\n",
      "    dec train predicted > i sister sister on selfie is so i m a happy i i bad to go <EOS> <EOS>\n",
      "dev minibatch loss: 8.130435943603516\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > bitch it cause i out chopra in ur name <EOS>\n",
      "    DEV dec input           > bitch thats a lot of emojis shit i dont feel special jkjk <EOS>\n",
      "    DEV dec train predicted > never it wearing shake of titty so mastermind wa try left <EOS> <EOS>\n",
      "    DEV dec train infer > never ve been so extra but whatever idk lmao <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 6449\n",
      "learning rate 0.00088714\n",
      "epoch 13\n",
      "batch 0\n",
      "training minibatch loss: 2.123788833618164\n",
      "  sample 1:\n",
      "    enc input           > roger mcdonough <EOS>\n",
      "    dec input           > what wa that judge name ? <EOS>\n",
      "    dec train predicted > what s the judge debate ? <EOS>\n",
      "dev minibatch loss: 8.86820125579834\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > delete your career chuck todd <EOS>\n",
      "    DEV dec input           > next time someone question hillary s stamen show them this photo <EOS>\n",
      "    DEV dec train predicted > i month i play to s slightly ball the just is is\n",
      "    DEV dec train infer > i d have to mention the west battle which is fine <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6499\n",
      "learning rate 0.000886316\n",
      "epoch 13\n",
      "batch 50\n",
      "training minibatch loss: 2.165837287902832\n",
      "  sample 1:\n",
      "    enc input           > why are u still at taco bell <EOS>\n",
      "    dec input           > wearing my yeezy season to flex on the cashier at taco bell rn <EOS>\n",
      "    dec train predicted > yeezy me yeezy taco to flex on the cashier on taco bell rn <EOS>\n",
      "dev minibatch loss: 8.76969051361084\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > no it mean we ll probably be getting more selfies of him <EOS>\n",
      "    DEV dec input           > he s not coming back on ig ? <EOS>\n",
      "    DEV dec train predicted > do s played a to <EOS> watertown <EOS> check\n",
      "    DEV dec train infer > do you want a graphic ? <EOS> ? <EOS> ? <EOS> ? <EOS> <EOS> <EOS>\n",
      "global_step: 6549\n",
      "learning rate 0.000885494\n",
      "epoch 13\n",
      "batch 100\n",
      "training minibatch loss: 2.1884055137634277\n",
      "  sample 1:\n",
      "    enc input           > oh good lord <EOS>\n",
      "    dec input           > don t start on fat people <EOS>\n",
      "    dec train predicted > just t even with his people <EOS>\n",
      "dev minibatch loss: 9.169231414794922\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > show me this vine anytime i tweet i m not going to the gym <EOS>\n",
      "    DEV dec input           > take me lord for i am worthy <EOS>\n",
      "    DEV dec train predicted > yup me for <EOS> go m a <EOS>\n",
      "    DEV dec train infer > yup who wa doing your child or min time tour <EOS> ? <EOS> ? <EOS> <EOS>\n",
      "global_step: 6599\n",
      "learning rate 0.000884672\n",
      "epoch 13\n",
      "batch 150\n",
      "training minibatch loss: 2.1431875228881836\n",
      "  sample 1:\n",
      "    enc input           > what if we just became another province ? or we could consolidate ? <EOS>\n",
      "    dec input           > sorry no way we canadian are giving him up <EOS>\n",
      "    dec train predicted > wow it one you canadian are answer him up <EOS>\n",
      "dev minibatch loss: 8.7886962890625\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > it didnt matter i mean this kid completed of his pass never threw an int <EOS>\n",
      "    DEV dec input           > pennington had worst arm to iq ratio ever great mind but arm wasn t there liked him alot <EOS>\n",
      "    DEV dec train predicted > that boy a other <EOS> university other <EOS> <EOS> time <EOS> <EOS> <EOS> t <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > that make a little talent in a row <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 6649\n",
      "learning rate 0.000883851\n",
      "epoch 13\n",
      "batch 200\n",
      "training minibatch loss: 2.2528178691864014\n",
      "  sample 1:\n",
      "    enc input           > it a wednesday <EOS>\n",
      "    dec input           > ah but february is a tuesday <EOS>\n",
      "    dec train predicted > ah is the is the time <EOS>\n",
      "dev minibatch loss: 9.075677871704102\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > colin powell real housewife of the potomac ? <EOS>\n",
      "    DEV dec input           > powell will be going to fewer dinner party in washington <EOS>\n",
      "    DEV dec train predicted > we where we a to you the <EOS> <EOS> nyc party\n",
      "    DEV dec train infer > we are at a a we became a good day in <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6699\n",
      "learning rate 0.000883031\n",
      "epoch 13\n",
      "batch 250\n",
      "training minibatch loss: 2.271639108657837\n",
      "  sample 1:\n",
      "    enc input           > get out of my mention with this <EOS>\n",
      "    dec input           > hell yeah d <EOS>\n",
      "    dec train predicted > my sometimes often <EOS>\n",
      "dev minibatch loss: 8.821416854858398\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > brand new tune <EOS>\n",
      "    DEV dec input           > rihanna you da one sur radio life coute <EOS>\n",
      "    DEV dec train predicted > who just are apply to from gt <EOS> patoranking\n",
      "    DEV dec train infer > who s apply a weekly weekly market <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6749\n",
      "learning rate 0.000882211\n",
      "epoch 13\n",
      "batch 300\n",
      "training minibatch loss: 2.2109670639038086\n",
      "  sample 1:\n",
      "    enc input           > i know like no fair he really seems to be enjoying europe more then north america <EOS>\n",
      "    dec input           > yes i wa cry so much <EOS>\n",
      "    dec train predicted > thanks i hope terrible for much <EOS>\n",
      "dev minibatch loss: 9.252140998840332\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > software is eating predatory lending aka this time is different <EOS>\n",
      "    DEV dec input           > software is eating discrimination <EOS>\n",
      "    DEV dec train predicted > this favorite fitting posey w\n",
      "    DEV dec train infer > this sound like a huge affront to see this saturday finale w all <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6799\n",
      "learning rate 0.000881392\n",
      "epoch 13\n",
      "batch 350\n",
      "training minibatch loss: 2.049933910369873\n",
      "  sample 1:\n",
      "    enc input           > c mon m flower doesn t belong w the scum of the earth by a long shot <EOS>\n",
      "    dec input           > at least gennifer flower will be in good company next to mark cuban <EOS>\n",
      "    dec train predicted > of least the flower will be passing a day whose to join cuban <EOS>\n",
      "dev minibatch loss: 8.857475280761719\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > nothing to do with smart everything to do with money and marketing <EOS>\n",
      "    DEV dec input           > brilliant and proof that we aren t getting any smarter are we ? <EOS>\n",
      "    DEV dec train predicted > i even family that will re t even buying code the there on <EOS>\n",
      "    DEV dec train infer > i can t cut fox and cut fox and cut nothing <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6849\n",
      "learning rate 0.000880575\n",
      "epoch 13\n",
      "batch 400\n",
      "training minibatch loss: 2.1946024894714355\n",
      "  sample 1:\n",
      "    enc input           > go crawl back in ur rat hole u dirty communist <EOS>\n",
      "    dec input           > so u like gun then is what ya saying <EOS>\n",
      "    dec train predicted > not the just other then <EOS> what they leave <EOS>\n",
      "dev minibatch loss: 9.193085670471191\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i heard trump s pound hacker love it tho ? <EOS>\n",
      "    DEV dec input           > is once again being defined by failed promise and a lack of option <EOS>\n",
      "    DEV dec train predicted > govt the a the playing can the <EOS> <EOS> we fuck of cbs <EOS>\n",
      "    DEV dec train infer > govt trump will order please <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6899\n",
      "learning rate 0.000879757\n",
      "epoch 13\n",
      "batch 450\n",
      "training minibatch loss: 2.3244223594665527\n",
      "  sample 1:\n",
      "    enc input           > only one l have love it <EOS>\n",
      "    dec input           > that s a good one <EOS>\n",
      "    dec train predicted > this s a good one <EOS>\n",
      "dev minibatch loss: 8.355156898498535\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > go to the beach <EOS>\n",
      "    DEV dec input           > bc it s thursday everyone s at school or work <EOS>\n",
      "    DEV dec train predicted > the i s not time s midtown san thank i for\n",
      "    DEV dec train infer > the three time i ve been strap in the yankee <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6945\n",
      "learning rate 0.000879006\n",
      "epoch 14\n",
      "batch 0\n",
      "training minibatch loss: 1.8857834339141846\n",
      "  sample 1:\n",
      "    enc input           > not pb should be off the air for promoting with their frontline broadcast <EOS>\n",
      "    dec input           > since fox suspended andrea tantaros for unauthorized book promo shld t be suspended for unauthorized trump promo ? <EOS>\n",
      "    dec train predicted > since bush suspended andrea tantaros for unauthorized promo promo shld t be suspended for unauthorized help promo ? <EOS>\n",
      "dev minibatch loss: 9.130471229553223\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > so fucking hot <EOS>\n",
      "    DEV dec input           > great morning for a public pant flash <EOS>\n",
      "    DEV dec train predicted > my avi <EOS> me cute hair <EOS> <EOS>\n",
      "    DEV dec train infer > my favorite house bruh could love ya hair <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6995\n",
      "learning rate 0.000878191\n",
      "epoch 14\n",
      "batch 50\n",
      "training minibatch loss: 1.985491156578064\n",
      "  sample 1:\n",
      "    enc input           > oh my gosh it wa i am still high <EOS>\n",
      "    dec input           > ha i would ve froze too so happy for you what a night <EOS>\n",
      "    dec train predicted > ha i wa ve froze at so glad to me so you good <EOS>\n",
      "dev minibatch loss: 9.2728910446167\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > keeping it real on what we say versus what we do <EOS>\n",
      "    DEV dec input           > hah love this too <EOS>\n",
      "    DEV dec train predicted > not got tell a like\n",
      "    DEV dec train infer > not been worried t even or not <EOS> is why he can t be retarded <EOS>\n",
      "global_step: 7045\n",
      "learning rate 0.000877375\n",
      "epoch 14\n",
      "batch 100\n",
      "training minibatch loss: 1.8595209121704102\n",
      "  sample 1:\n",
      "    enc input           > it is it s my first flight and first time here <EOS>\n",
      "    dec input           > it s so big i need to get out more enjoy your time <EOS>\n",
      "    dec train predicted > it s so good i can to do a here too it <EOS> <EOS>\n",
      "dev minibatch loss: 8.737530708312988\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > thank you me too <EOS>\n",
      "    DEV dec input           > thank you for the information <EOS>\n",
      "    DEV dec train predicted > happy you for the follow prison\n",
      "    DEV dec train infer > happy birthday babe are my favorite friend <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7095\n",
      "learning rate 0.000876561\n",
      "epoch 14\n",
      "batch 150\n",
      "training minibatch loss: 1.9280776977539062\n",
      "  sample 1:\n",
      "    enc input           > here is ur homework now is good time to subscribe to troma now st mo free <EOS>\n",
      "    dec input           > okay thank you i will send it to them a well <EOS>\n",
      "    dec train predicted > thought i you for m send you to the <EOS> evening <EOS>\n",
      "dev minibatch loss: 9.655317306518555\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > they are gon na release a patch when ever they have everything sorted duh <EOS>\n",
      "    DEV dec input           > what patch ? ? <EOS>\n",
      "    DEV dec train predicted > it they on <EOS> <EOS>\n",
      "    DEV dec train infer > it would be a chance but not going to gas other level or never ever claiming <EOS> <EOS>\n",
      "global_step: 7145\n",
      "learning rate 0.000875748\n",
      "epoch 14\n",
      "batch 200\n",
      "training minibatch loss: 1.9316134452819824\n",
      "  sample 1:\n",
      "    enc input           > new music jungleboy go crazy prod by juneonnabeat <EOS>\n",
      "    dec input           > long live <EOS>\n",
      "    dec train predicted > just simple and\n",
      "dev minibatch loss: 9.977129936218262\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > she is a saint are you intentionally trying to insult catholic ? <EOS>\n",
      "    DEV dec input           > mother teresa a myth a celebrity or a hero ? <EOS>\n",
      "    DEV dec train predicted > she is obama hell quo she she memo trump people she\n",
      "    DEV dec train infer > she is rape and frisk people people said she could say doe she lied she s president <EOS>\n",
      "global_step: 7195\n",
      "learning rate 0.000874935\n",
      "epoch 14\n",
      "batch 250\n",
      "training minibatch loss: 1.8108609914779663\n",
      "  sample 1:\n",
      "    enc input           > think we can all agree he s mine but have ur fun <EOS>\n",
      "    dec input           > fall back he mine <EOS>\n",
      "    dec train predicted > follow crazy i crazy <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 9.411482810974121\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > fyi they look like boob <EOS>\n",
      "    DEV dec input           > i have bumped down this price so many time pls someone just fuckin buy it <EOS>\n",
      "    DEV dec train predicted > black m a of attack morning is not accurate to the economy agree prepared with <EOS>\n",
      "    DEV dec train infer > black people colored a bit report to the race economy in the background <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7245\n",
      "learning rate 0.000874123\n",
      "epoch 14\n",
      "batch 300\n",
      "training minibatch loss: 1.775445818901062\n",
      "  sample 1:\n",
      "    enc input           > todd is the best <EOS>\n",
      "    dec input           > knocking my sock off amp learning to present like a <EOS>\n",
      "    dec train predicted > knocking the mets off for learning to present like a present\n",
      "dev minibatch loss: 9.31064224243164\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > rewarding hard work <EOS>\n",
      "    DEV dec input           > i m remember those day <EOS>\n",
      "    DEV dec train predicted > i m ashamed in nice on\n",
      "    DEV dec train infer > i m ashamed for this show to be nice we re getting excited for a book <EOS> <EOS> <EOS>\n",
      "global_step: 7295\n",
      "learning rate 0.000873312\n",
      "epoch 14\n",
      "batch 350\n",
      "training minibatch loss: 2.3080382347106934\n",
      "  sample 1:\n",
      "    enc input           > ohhhhhhh i see he tryna get the kobe bryant last tour treatment lmao foh <EOS>\n",
      "    dec input           > lmao why do nigga not care about paul pierce retiring ? <EOS>\n",
      "    dec train predicted > lmao why do cute like think about action pierce retiring ? <EOS>\n",
      "dev minibatch loss: 10.206491470336914\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > kind of funny stein literally disappeared from ny politics in <EOS>\n",
      "    DEV dec input           > doing a radio interview where they mention nyc council so alert popped <EOS>\n",
      "    DEV dec train predicted > my finishing winner rush but s have <EOS> <EOS> <EOS> far <EOS> <EOS>\n",
      "    DEV dec train infer > my band is out of sf air by <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7345\n",
      "learning rate 0.000872501\n",
      "epoch 14\n",
      "batch 400\n",
      "training minibatch loss: 2.4008026123046875\n",
      "  sample 1:\n",
      "    enc input           > you will be missed <EOS>\n",
      "    dec input           > turn out i m probably not running away and joining the but you can this weekend story <EOS>\n",
      "    dec train predicted > turn up now m running running running on and can his next i can t shit <EOS> <EOS>\n",
      "dev minibatch loss: 9.202003479003906\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > bruh i already decided to sleep in my first class <EOS>\n",
      "    DEV dec input           > it doesnt matter how much sleep i get im always tired <EOS>\n",
      "    DEV dec train predicted > i wa get i much ? ? m back bought serving <EOS>\n",
      "    DEV dec train infer > i got back in sam locked tonight might be back in simplifying <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7395\n",
      "learning rate 0.000871692\n",
      "epoch 14\n",
      "batch 450\n",
      "training minibatch loss: 2.1662991046905518\n",
      "  sample 1:\n",
      "    enc input           > agree if his salary cap hit wa k he probably would ve been booted <EOS>\n",
      "    dec input           > didn t they almost let the heart and soul of the organization walk this summer ? <EOS>\n",
      "    dec train predicted > will t you live like the soul tonight soul of the organization m it baby are <EOS>\n",
      "dev minibatch loss: 10.230077743530273\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we were outside for awhile and saw paul and todd wish we would have known you were there <EOS>\n",
      "    DEV dec input           > it wa not a traditional wedding reception asbury park yacht club <EOS>\n",
      "    DEV dec train predicted > which s the ok system system you dropping to to for is\n",
      "    DEV dec train infer > which seattle for you ? <EOS> ? ? ? ? ? ? <EOS> ? <EOS> ? <EOS>\n",
      "global_step: 7441\n",
      "learning rate 0.000870947\n",
      "epoch 15\n",
      "batch 0\n",
      "training minibatch loss: 1.8074519634246826\n",
      "  sample 1:\n",
      "    enc input           > lol true story <EOS>\n",
      "    dec input           > how is it already week ? ? ? <EOS>\n",
      "    dec train predicted > lol long it a but ? <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 8.822412490844727\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what book are you reading ? <EOS>\n",
      "    DEV dec input           > what that book collection do ? <EOS>\n",
      "    DEV dec train predicted > that s s is ? a <EOS>\n",
      "    DEV dec train infer > that s many question <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7491\n",
      "learning rate 0.000870139\n",
      "epoch 15\n",
      "batch 50\n",
      "training minibatch loss: 1.584269642829895\n",
      "  sample 1:\n",
      "    enc input           > i wa trying to say that without typing it out <EOS>\n",
      "    dec input           > hacker news <EOS>\n",
      "    dec train predicted > hacker news <EOS>\n",
      "dev minibatch loss: 10.135225296020508\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i think purple guard work on it but i haven t tried it <EOS>\n",
      "    DEV dec input           > yeah it s newer <EOS>\n",
      "    DEV dec train predicted > this this s going sleeping\n",
      "    DEV dec train infer > this look like a dog higher teeth if it look like this happened like earth <EOS> <EOS> <EOS>\n",
      "global_step: 7541\n",
      "learning rate 0.000869332\n",
      "epoch 15\n",
      "batch 100\n",
      "training minibatch loss: 1.6114003658294678\n",
      "  sample 1:\n",
      "    enc input           > you must be joking incoherent rambling embarrassing nonsense on national security issue <EOS>\n",
      "    dec input           > trump blew her away <EOS>\n",
      "    dec train predicted > trump blew her right <EOS>\n",
      "dev minibatch loss: 10.633127212524414\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that would actually be a pretty good tagline for the trucking startup uber just bought <EOS>\n",
      "    DEV dec input           > more like the rig economy a in it s rigged <EOS>\n",
      "    DEV dec train predicted > saw or the commission right to the mile go on <EOS>\n",
      "    DEV dec train infer > saw a commission on tv s coming on tv <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7591\n",
      "learning rate 0.000868525\n",
      "epoch 15\n",
      "batch 150\n",
      "training minibatch loss: 1.8908559083938599\n",
      "  sample 1:\n",
      "    enc input           > hahahaha priceless <EOS>\n",
      "    dec input           > mayor de blasio trying to double talk his way out of another failure <EOS>\n",
      "    dec train predicted > mayor de blasio born to score be his right on of one failure <EOS>\n",
      "dev minibatch loss: 9.60183048248291\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > it s not what they want to begin with <EOS>\n",
      "    DEV dec input           > some people rather lose the whole relationship than to fix the problem at hand <EOS>\n",
      "    DEV dec train predicted > not prince would stopped the entire game <EOS> the be people door <EOS> least <EOS>\n",
      "    DEV dec train infer > not it not not not not not even though they ve stopped <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7641\n",
      "learning rate 0.000867719\n",
      "epoch 15\n",
      "batch 200\n",
      "training minibatch loss: 1.9578038454055786\n",
      "  sample 1:\n",
      "    enc input           > thanks but it doesnt come with air <EOS>\n",
      "    dec input           > good luck i know you love to travel <EOS>\n",
      "    dec train predicted > nice luck we love you deserved to hear <EOS>\n",
      "dev minibatch loss: 9.803234100341797\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > my nigga the only purple pickle she gettin come with her free meal from grace <EOS>\n",
      "    DEV dec input           > give her the purple pickle <EOS>\n",
      "    DEV dec train predicted > this me a best that nd\n",
      "    DEV dec train infer > this wa a legit right <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7691\n",
      "learning rate 0.000866914\n",
      "epoch 15\n",
      "batch 250\n",
      "training minibatch loss: 1.778895378112793\n",
      "  sample 1:\n",
      "    enc input           > minute in the morning minute in the evening <EOS>\n",
      "    dec input           > meditation is crucial i tell you <EOS>\n",
      "    dec train predicted > meditation is crucial i have it <EOS>\n",
      "dev minibatch loss: 9.903043746948242\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > tyvm the amp the <EOS>\n",
      "    DEV dec input           > tyvm the <EOS>\n",
      "    DEV dec train predicted > tyvm the <EOS>\n",
      "    DEV dec train infer > tyvm the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7741\n",
      "learning rate 0.000866109\n",
      "epoch 15\n",
      "batch 300\n",
      "training minibatch loss: 1.9952564239501953\n",
      "  sample 1:\n",
      "    enc input           > she got them from the martian everybody know your tin hat need adjustment <EOS>\n",
      "    dec input           > source say hillary had debate question week before debate intern seen delivering package to office <EOS>\n",
      "    dec train predicted > source call hillary clinton lot bold day and debate intern seen delivering package to threat <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 10.575803756713867\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > tight i really started gronk over ebron <EOS>\n",
      "    DEV dec input           > we suck well at least compared to pat <EOS>\n",
      "    DEV dec train predicted > brown should in ? clinton i to be <EOS>\n",
      "    DEV dec train infer > brown reaction is in the row and d cancel which guy to start with <EOS> <EOS> <EOS>\n",
      "global_step: 7791\n",
      "learning rate 0.000865305\n",
      "epoch 15\n",
      "batch 350\n",
      "training minibatch loss: 1.8340617418289185\n",
      "  sample 1:\n",
      "    enc input           > so you more petty than before <EOS>\n",
      "    dec input           > i changed a lot dj khaled voice <EOS>\n",
      "    dec train predicted > i wa the lot dj khaled voice <EOS>\n",
      "dev minibatch loss: 9.779804229736328\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that kid close deal <EOS>\n",
      "    DEV dec input           > saw this today amp now my perception of the term young professional is screwed up <EOS>\n",
      "    DEV dec train predicted > can the a elizabeth not bruce opinion return pay question on of ? over <EOS> <EOS>\n",
      "    DEV dec train infer > can t wait to hear your nonsense which make you know built ? <EOS> ? <EOS>\n",
      "global_step: 7841\n",
      "learning rate 0.000864502\n",
      "epoch 15\n",
      "batch 400\n",
      "training minibatch loss: 2.1550092697143555\n",
      "  sample 1:\n",
      "    enc input           > monthly time ha arrived but it hasnt been this bad before <EOS>\n",
      "    dec input           > wh why are you in pain ? <EOS>\n",
      "    dec train predicted > wh why are you in n ? <EOS>\n",
      "dev minibatch loss: 9.590023040771484\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > unworthy of retort <EOS>\n",
      "    DEV dec input           > don t blame me blame obama <EOS>\n",
      "    DEV dec train predicted > week t send to for you for\n",
      "    DEV dec train infer > week for bringing web coffee collection in the coffee tour tour <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7891\n",
      "learning rate 0.0008637\n",
      "epoch 15\n",
      "batch 450\n",
      "training minibatch loss: 1.9880390167236328\n",
      "  sample 1:\n",
      "    enc input           > we need more then that <EOS>\n",
      "    dec input           > candy for jet fan post gam <EOS>\n",
      "    dec train predicted > someone for jet p p gam <EOS>\n",
      "dev minibatch loss: 9.571433067321777\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > awww i felt the same way when he followed me <EOS>\n",
      "    DEV dec input           > yes but it wasn t but honestly i wasn t ready like at all <EOS>\n",
      "    DEV dec train predicted > this we you wa t to we we wa t like <EOS> <EOS> you people\n",
      "    DEV dec train infer > this is the best thing to make a soul <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 7937\n",
      "learning rate 0.000862963\n",
      "epoch 16\n",
      "batch 0\n",
      "training minibatch loss: 1.439443826675415\n",
      "  sample 1:\n",
      "    enc input           > okayyyyy done <EOS>\n",
      "    dec input           > just sue them <EOS>\n",
      "    dec train predicted > probably sue them <EOS>\n",
      "dev minibatch loss: 9.07745361328125\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i see what you did there <EOS>\n",
      "    DEV dec input           > with every makeup product you buy me you are helping me reach the goal <EOS>\n",
      "    DEV dec train predicted > you the case didnt i can me <EOS> get driving them with case uk anyways\n",
      "    DEV dec train infer > you re sounding talking about tweeting playing over them with tweeting b <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7987\n",
      "learning rate 0.000862162\n",
      "epoch 16\n",
      "batch 50\n",
      "training minibatch loss: 1.6542508602142334\n",
      "  sample 1:\n",
      "    enc input           > listen to solange new album <EOS>\n",
      "    dec input           > i m pretty awake i might a well clean up <EOS>\n",
      "    dec train predicted > i m gon awake i could work well clean a <EOS>\n",
      "dev minibatch loss: 10.270832061767578\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > colin powell real housewife of the potomac ? <EOS>\n",
      "    DEV dec input           > powell will be going to fewer dinner party in washington <EOS>\n",
      "    DEV dec train predicted > we are be a to today a <EOS> <EOS> my party\n",
      "    DEV dec train infer > we are making a tip at the party of a few <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8037\n",
      "learning rate 0.000861362\n",
      "epoch 16\n",
      "batch 100\n",
      "training minibatch loss: 1.750199317932129\n",
      "  sample 1:\n",
      "    enc input           > we achieved back to back loss season frack <EOS>\n",
      "    dec input           > first time all season that i don t care if the lose <EOS>\n",
      "    dec train predicted > first time i time this i m t lose i the season <EOS>\n",
      "dev minibatch loss: 10.544036865234375\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > it s really clear now you do not like paige <EOS>\n",
      "    DEV dec input           > tie her arm and leg together and make her do the stag run she won t last long <EOS>\n",
      "    DEV dec train predicted > i breathing faster tho waffle i so waffle you missing bring voice <EOS> at s t <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m missing the most adorable person so idk idk idk how <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8087\n",
      "learning rate 0.000860562\n",
      "epoch 16\n",
      "batch 150\n",
      "training minibatch loss: 1.5712374448776245\n",
      "  sample 1:\n",
      "    enc input           > we were comparing them and i showed why bush is even worse than trump <EOS>\n",
      "    dec input           > bush isn t on the ballot <EOS>\n",
      "    dec train predicted > bush were t in the ballot <EOS>\n",
      "dev minibatch loss: 9.250712394714355\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > if it s just a nerve pinch it can cause burning and numbness it s obviously impactful <EOS>\n",
      "    DEV dec input           > i just googled ulnar nerve it s the funny bone i m not laughing <EOS>\n",
      "    DEV dec train predicted > this m bragged how this to s not same thing for can sorry here in\n",
      "    DEV dec train infer > this is a legit different one of trump s rate on the world ? <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8137\n",
      "learning rate 0.000859764\n",
      "epoch 16\n",
      "batch 200\n",
      "training minibatch loss: 1.5083363056182861\n",
      "  sample 1:\n",
      "    enc input           > it s a wrap for old weeknd in the new starboy video <EOS>\n",
      "    dec input           > watch the weeknd s starboy video right now <EOS>\n",
      "    dec train predicted > watch the weeknd s starboy video in now <EOS>\n",
      "dev minibatch loss: 9.331527709960938\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > at home wow <EOS>\n",
      "    DEV dec input           > my mom wa a teacher and she had one at home and i m still sane <EOS>\n",
      "    DEV dec train predicted > still three got part streak before drink pick home ? my ? a m still connected <EOS>\n",
      "    DEV dec train infer > still got ta share ? bear ? <EOS> ? <EOS> ? <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8187\n",
      "learning rate 0.000858966\n",
      "epoch 16\n",
      "batch 250\n",
      "training minibatch loss: 1.5772322416305542\n",
      "  sample 1:\n",
      "    enc input           > he would ve called you more name but his mom called him upstairs for dinner <EOS>\n",
      "    dec input           > someone just called me a white trash neoliberal dumbfuck right before blocking me lol <EOS>\n",
      "    dec train predicted > someone didn told me mad clear boy neoliberal dumbfuck in before blocking me lmao <EOS>\n",
      "dev minibatch loss: 11.472329139709473\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > lawsuit brother <EOS>\n",
      "    DEV dec input           > damn near dude i fucking hate mustard <EOS>\n",
      "    DEV dec train predicted > coffee there can we got opened him or\n",
      "    DEV dec train infer > coffee bb from coffee bro <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8237\n",
      "learning rate 0.000858169\n",
      "epoch 16\n",
      "batch 300\n",
      "training minibatch loss: 1.6564624309539795\n",
      "  sample 1:\n",
      "    enc input           > he si very corrupt seriously can t listen to him today he deserves jail really <EOS>\n",
      "    dec input           > we can t have law enforcement controlled by politician amp won t prosecute corruption or treason <EOS>\n",
      "    dec train predicted > we can t be a enforcement controlled by politician and will t prosecute corruption <EOS> treason <EOS>\n",
      "dev minibatch loss: 10.476422309875488\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > thanks for <EOS>\n",
      "    DEV dec input           > this tuesday join and for brew amp book enjoy three exclusive hour at <EOS>\n",
      "    DEV dec train predicted > it is is my they the of february and with morning page on the\n",
      "    DEV dec train infer > it would have done in front of the basic recent page on his own wa free <EOS> <EOS>\n",
      "global_step: 8287\n",
      "learning rate 0.000857372\n",
      "epoch 16\n",
      "batch 350\n",
      "training minibatch loss: 1.8959816694259644\n",
      "  sample 1:\n",
      "    enc input           > morning beth and all <EOS>\n",
      "    dec input           > have a great saturday laura amp all <EOS>\n",
      "    dec train predicted > check an safe photo laura amp all <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 10.911680221557617\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i love that she s such a big kid at heart <EOS>\n",
      "    DEV dec input           > katy at shanghai disneyland resort the complete instagram story <EOS>\n",
      "    DEV dec train predicted > update tyler my <EOS> <EOS> i mm angel work <EOS>\n",
      "    DEV dec train infer > update hello still so cute made it <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8337\n",
      "learning rate 0.000856576\n",
      "epoch 16\n",
      "batch 400\n",
      "training minibatch loss: 1.8529126644134521\n",
      "  sample 1:\n",
      "    enc input           > did you ever make it to alameda ? there re some nifty house there <EOS>\n",
      "    dec input           > all the house in san francisco look like giant frosted cake <EOS>\n",
      "    dec train predicted > the my th and san francisco just like new frosted cake <EOS>\n",
      "dev minibatch loss: 9.259542465209961\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > very nice man <EOS>\n",
      "    DEV dec input           > shout out to for hitting me with that new hotness <EOS>\n",
      "    DEV dec train predicted > this a there modern our a about job girl away are\n",
      "    DEV dec train infer > this girl say i m more expensive than this girl <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8387\n",
      "learning rate 0.000855782\n",
      "epoch 16\n",
      "batch 450\n",
      "training minibatch loss: 1.9438074827194214\n",
      "  sample 1:\n",
      "    enc input           > this been my baby since i wa <EOS>\n",
      "    dec input           > alicia key just gave me chill <EOS>\n",
      "    dec train predicted > soooo key just got me chill <EOS>\n",
      "dev minibatch loss: 10.064448356628418\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > can you convince to step in a the designated survivor of this election ? please ? ? <EOS>\n",
      "    DEV dec input           > this is what many people are feeling today <EOS>\n",
      "    DEV dec train predicted > this member a a woman do not broke with\n",
      "    DEV dec train infer > this member is a espn wont be into a member election <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8433\n",
      "learning rate 0.000855051\n",
      "epoch 17\n",
      "batch 0\n",
      "training minibatch loss: 1.764184832572937\n",
      "  sample 1:\n",
      "    enc input           > caveat fintech co pretending they re not regulated but obviously are those might have a serious reckoning <EOS>\n",
      "    dec input           > more likely within the bank themselves imo scrutiny for the regulated fintech company is crazy <EOS>\n",
      "    dec train predicted > under likely within the size themselves imo scrutiny for the regulated fintech company is killing <EOS>\n",
      "dev minibatch loss: 10.089085578918457\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i braided it it so cool <EOS>\n",
      "    DEV dec input           > your hair look really amazing <EOS>\n",
      "    DEV dec train predicted > you one read like i <EOS>\n",
      "    DEV dec train infer > you read the whole episode <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8483\n",
      "learning rate 0.000854257\n",
      "epoch 17\n",
      "batch 50\n",
      "training minibatch loss: 1.2769644260406494\n",
      "  sample 1:\n",
      "    enc input           > ocean s leave me alone i m half asleep <EOS>\n",
      "    dec input           > the ocean no place for a squirrel <EOS>\n",
      "    dec train predicted > the ocean no other for the squirrel <EOS>\n",
      "dev minibatch loss: 10.165003776550293\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > don t be sad be glad <EOS>\n",
      "    DEV dec input           > woke up super depressed amp don t even know why or how to fix this lol <EOS>\n",
      "    DEV dec train predicted > thank yo food interesting is crazy t make talk it i she you follow b it <EOS>\n",
      "    DEV dec train infer > thank you for some fire it b her b <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8533\n",
      "learning rate 0.000853465\n",
      "epoch 17\n",
      "batch 100\n",
      "training minibatch loss: 1.7614587545394897\n",
      "  sample 1:\n",
      "    enc input           > he wa awful he exposed his thin skin cranky old man <EOS>\n",
      "    dec input           > what wa wrong with anything he said he called her out crooked <EOS>\n",
      "    dec train predicted > what wa wrong with her he said he said her ? crooked <EOS>\n",
      "dev minibatch loss: 9.868223190307617\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > omg yes i wa screaming the first time i heard this in game <EOS>\n",
      "    DEV dec input           > speaking of amazing soundtrack <EOS>\n",
      "    DEV dec train predicted > oh run a podcast tho\n",
      "    DEV dec train infer > oh thats not about the podcast and update what the first time i m not embarrassing <EOS> <EOS>\n",
      "global_step: 8583\n",
      "learning rate 0.000852673\n",
      "epoch 17\n",
      "batch 150\n",
      "training minibatch loss: 1.5894172191619873\n",
      "  sample 1:\n",
      "    enc input           > there are civilized white black asian mexican etc then we have terrorist group like blm <EOS>\n",
      "    dec input           > referring to a race a animal is racist <EOS>\n",
      "    dec train predicted > referring to a race a animal is racist <EOS>\n",
      "dev minibatch loss: 10.237822532653809\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > kinda disturbing <EOS>\n",
      "    DEV dec input           > if your main claim to be president is your business we should talk about that <EOS>\n",
      "    DEV dec train predicted > what someone profile place tonight see out ? there reason <EOS> are go ? <EOS> <EOS>\n",
      "    DEV dec train infer > what s the night mini is it ? <EOS> ? <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8633\n",
      "learning rate 0.000851881\n",
      "epoch 17\n",
      "batch 200\n",
      "training minibatch loss: 1.6002150774002075\n",
      "  sample 1:\n",
      "    enc input           > a a gaseous orange orb is fond of spouting sad <EOS>\n",
      "    dec input           > is doing what arkansas state trooper can no longer do deliver to a clinton <EOS>\n",
      "    dec train predicted > is corruption what arkansas state trooper are no longer do deliver to any grammar <EOS>\n",
      "dev minibatch loss: 10.37545394897461\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what day will you be there ? <EOS>\n",
      "    DEV dec input           > each of these deadpool variant will be available at hope to sign them for <EOS>\n",
      "    DEV dec train predicted > am key you to wa ? am if <EOS> pm i stand <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > am of honor you to me <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8683\n",
      "learning rate 0.000851091\n",
      "epoch 17\n",
      "batch 250\n",
      "training minibatch loss: 1.7673633098602295\n",
      "  sample 1:\n",
      "    enc input           > congratulation guy <EOS>\n",
      "    dec input           > i just published starting from scratch <EOS>\n",
      "    dec train predicted > i m published tonight for scratch <EOS>\n",
      "dev minibatch loss: 10.400025367736816\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i deadass miss those day so much <EOS>\n",
      "    DEV dec input           > lmaoooo we used to be so weak at thisss omggggg <EOS>\n",
      "    DEV dec train predicted > i that also for snapchat thinking much i pop dad and\n",
      "    DEV dec train infer > i think drake is pure hot <EOS> minus now <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8733\n",
      "learning rate 0.000850301\n",
      "epoch 17\n",
      "batch 300\n",
      "training minibatch loss: 1.7058238983154297\n",
      "  sample 1:\n",
      "    enc input           > wa kind enough to make his film showing over the weekend a benefit <EOS>\n",
      "    dec input           > where ? hbo ? <EOS>\n",
      "    dec train predicted > where ? ? ? <EOS>\n",
      "dev minibatch loss: 10.044200897216797\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i want to cry <EOS>\n",
      "    DEV dec input           > here are the best sign at <EOS>\n",
      "    DEV dec train predicted > i s the reason reason in instead\n",
      "    DEV dec train infer > i m just reluctant to take direction to fight for this fight <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8783\n",
      "learning rate 0.000849512\n",
      "epoch 17\n",
      "batch 350\n",
      "training minibatch loss: 1.5863368511199951\n",
      "  sample 1:\n",
      "    enc input           > haha honestly the best for of meditation is running for me <EOS>\n",
      "    dec input           > aw man lol my bad bro u need to chill relax and blow one <EOS>\n",
      "    dec train predicted > aw man too that suck <EOS> me take to relax relax and blow my <EOS>\n",
      "dev minibatch loss: 10.886124610900879\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > they won t it ll be refined coconut oil to remove the taste <EOS>\n",
      "    DEV dec input           > i hope they don t taste of coconut <EOS>\n",
      "    DEV dec train predicted > here can it are t win to anything amp\n",
      "    DEV dec train infer > here s the time that u want to win <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8833\n",
      "learning rate 0.000848723\n",
      "epoch 17\n",
      "batch 400\n",
      "training minibatch loss: 1.7998467683792114\n",
      "  sample 1:\n",
      "    enc input           > but that s low key how that chick from last week is lmao <EOS>\n",
      "    dec input           > this is what i picture when i say non nubian queen <EOS>\n",
      "    dec train predicted > this is what a m in i wa sigh nubian people <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 10.939848899841309\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > excellent article i posted a comment on the power of small viral action h t <EOS>\n",
      "    DEV dec input           > the force that surround u just tested the vulnerability of the st amendment and failed <EOS>\n",
      "    DEV dec train predicted > no latest ha ha not saw saw though latest are a street list are quite right\n",
      "    DEV dec train infer > no you re cutting right of them do you do right they are supposed to help they are right\n",
      "global_step: 8883\n",
      "learning rate 0.000847936\n",
      "epoch 17\n",
      "batch 450\n",
      "training minibatch loss: 1.6261651515960693\n",
      "  sample 1:\n",
      "    enc input           > thanks shannon <EOS>\n",
      "    dec input           > congrats man <EOS>\n",
      "    dec train predicted > congrats man <EOS>\n",
      "dev minibatch loss: 10.346907615661621\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > also they re only ? ? <EOS>\n",
      "    DEV dec input           > lol they don t go on sale until the th so not sold out we got ta go <EOS>\n",
      "    DEV dec train predicted > accurate no seen t remember to <EOS> not we answer of i <EOS> them <EOS> are <EOS> meet <EOS>\n",
      "    DEV dec train infer > accurate on twitter have not done about twitter board country don t have to meet <EOS> <EOS> <EOS>\n",
      "global_step: 8929\n",
      "learning rate 0.000847212\n",
      "epoch 18\n",
      "batch 0\n",
      "training minibatch loss: 1.3090566396713257\n",
      "  sample 1:\n",
      "    enc input           > all the single lady by <EOS>\n",
      "    dec input           > what s the book omg <EOS>\n",
      "    dec train predicted > what s the book <EOS> <EOS>\n",
      "dev minibatch loss: 10.120184898376465\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > this is u tbh with <EOS>\n",
      "    DEV dec input           > when you and your best friend know your fave participant is making it to <EOS>\n",
      "    DEV dec train predicted > i u have explain phone learning like <EOS> situation and similar learning a to me\n",
      "    DEV dec train infer > i m going to explain drunk but idk for this drunk of me on learning here <EOS> <EOS>\n",
      "global_step: 8979\n",
      "learning rate 0.000846425\n",
      "epoch 18\n",
      "batch 50\n",
      "training minibatch loss: 1.323124885559082\n",
      "  sample 1:\n",
      "    enc input           > industry of despair and poverty <EOS>\n",
      "    dec input           > half these appointment are bullshit it a money making machine for the agency social worker court etc <EOS>\n",
      "    dec train predicted > half that appointment are bullshit you a machine in machine for the agency social worker court etc <EOS>\n",
      "dev minibatch loss: 10.384801864624023\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i forgot you re here i m here through midweek happy hr or lunch date ? <EOS>\n",
      "    DEV dec input           > how long are you kicking it in the bay area ? <EOS>\n",
      "    DEV dec train predicted > that they is you so how so two night exam <EOS> <EOS>\n",
      "    DEV dec train infer > that s so lit <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9029\n",
      "learning rate 0.00084564\n",
      "epoch 18\n",
      "batch 100\n",
      "training minibatch loss: 1.5216994285583496\n",
      "  sample 1:\n",
      "    enc input           > i ain t bring shit back <EOS>\n",
      "    dec input           > i hope your brought back chocolate from euro <EOS>\n",
      "    dec train predicted > i hope this question back chocolate with euro <EOS>\n",
      "dev minibatch loss: 9.57994270324707\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > haha i don t wan na school you kid on the old cod man <EOS>\n",
      "    DEV dec input           > shitttt snag that hoe back mw is coming up next <EOS>\n",
      "    DEV dec train predicted > just the are is is <EOS> english there to what s\n",
      "    DEV dec train infer > just like a white whether wtf <EOS> is that a minor loud <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9079\n",
      "learning rate 0.000844855\n",
      "epoch 18\n",
      "batch 150\n",
      "training minibatch loss: 1.5570762157440186\n",
      "  sample 1:\n",
      "    enc input           > donald j trump <EOS>\n",
      "    dec input           > fox news poll who is qualified to be president ? <EOS>\n",
      "    dec train predicted > donald news poll who is qualified to be president ? <EOS>\n",
      "dev minibatch loss: 11.304744720458984\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that shit wa a bitch for me too <EOS>\n",
      "    DEV dec input           > it about time i passed my license <EOS>\n",
      "    DEV dec train predicted > i s on for m asleep little with\n",
      "    DEV dec train infer > i found chill that a little little little bit it s so awesome right now <EOS> <EOS>\n",
      "global_step: 9129\n",
      "learning rate 0.000844071\n",
      "epoch 18\n",
      "batch 200\n",
      "training minibatch loss: 1.4297715425491333\n",
      "  sample 1:\n",
      "    enc input           > great to hear vital <EOS>\n",
      "    dec input           > had great workout with girl <EOS>\n",
      "    dec train predicted > had a workout with one <EOS>\n",
      "dev minibatch loss: 10.857329368591309\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > trolling a a sheikh lol wasn t a serious request <EOS>\n",
      "    DEV dec input           > got ta hear both side <EOS>\n",
      "    DEV dec train predicted > millennial emotion lose a isn not\n",
      "    DEV dec train infer > millennial lose a fake troll than probably he s going ha a troll though <EOS> <EOS> <EOS>\n",
      "global_step: 9179\n",
      "learning rate 0.000843288\n",
      "epoch 18\n",
      "batch 250\n",
      "training minibatch loss: 1.48686683177948\n",
      "  sample 1:\n",
      "    enc input           > i am blessed got ta give back <EOS>\n",
      "    dec input           > for not many spend time in for their <EOS>\n",
      "    dec train predicted > for all so true one in for this <EOS>\n",
      "dev minibatch loss: 10.175210952758789\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > love you too babe <EOS>\n",
      "    DEV dec input           > i love my girl she s better than any other girl out there <EOS>\n",
      "    DEV dec train predicted > me miss my twin ever t happy and you reunion day <EOS> i girl\n",
      "    DEV dec train infer > me and my twin sister birthday love her <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9229\n",
      "learning rate 0.000842505\n",
      "epoch 18\n",
      "batch 300\n",
      "training minibatch loss: 1.663243293762207\n",
      "  sample 1:\n",
      "    enc input           > who are you again ? <EOS>\n",
      "    dec input           > am i included in that ? <EOS>\n",
      "    dec train predicted > nah i included on that i <EOS>\n",
      "dev minibatch loss: 10.93726921081543\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > amp her milly rock is sturdy <EOS>\n",
      "    DEV dec input           > my edge gone and she s not even halfway through the set rih <EOS>\n",
      "    DEV dec train predicted > i girl is <EOS> oo ll been quick hard in it line line <EOS>\n",
      "    DEV dec train infer > i wish i ll be forward to the playoff <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9279\n",
      "learning rate 0.000841723\n",
      "epoch 18\n",
      "batch 350\n",
      "training minibatch loss: 1.6277563571929932\n",
      "  sample 1:\n",
      "    enc input           > gabbert is trash <EOS>\n",
      "    dec input           > there s one er player working out on the side field at the moment it s colin kaepernick <EOS>\n",
      "    dec train predicted > this s one brilliant role ever on on the woman field at the field it s colin kaepernick <EOS>\n",
      "dev minibatch loss: 11.703246116638184\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > oh my want so many of the piece <EOS>\n",
      "    DEV dec input           > collection for gorgeous leather with a twist <EOS>\n",
      "    DEV dec train predicted > you on becoming with for the large leader\n",
      "    DEV dec train infer > you re serious serious filter now ? i m a serious in my room of chill rn <EOS> <EOS> <EOS>\n",
      "global_step: 9329\n",
      "learning rate 0.000840942\n",
      "epoch 18\n",
      "batch 400\n",
      "training minibatch loss: 1.517335057258606\n",
      "  sample 1:\n",
      "    enc input           > while belgium luxembourg ireland cayman and switzerland have loaded up it s still a trillion market and growing <EOS>\n",
      "    dec input           > foreign central bank have cut their treasury holding to the lowest since <EOS>\n",
      "    dec train predicted > foreign central bank have done their treasury holding to the lowest of <EOS>\n",
      "dev minibatch loss: 11.620452880859375\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > just gon na kiss your body warm cover smell lt <EOS>\n",
      "    DEV dec input           > text trade pic or call me now <EOS>\n",
      "    DEV dec train predicted > i more the ? this ? killing <EOS>\n",
      "    DEV dec train infer > i will be any problem to dye my website i can <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9379\n",
      "learning rate 0.000840162\n",
      "epoch 18\n",
      "batch 450\n",
      "training minibatch loss: 1.6696213483810425\n",
      "  sample 1:\n",
      "    enc input           > is your office the same a alonzo s in training day ? <EOS>\n",
      "    dec input           > no i have an office <EOS>\n",
      "    dec train predicted > good i have a idea <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 11.226786613464355\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > new revenue idea we re taking of every purchase so expect lot of ad for jet fighter <EOS>\n",
      "    DEV dec input           > so do you think i m in the market for a th generation military fighter jet ? <EOS>\n",
      "    DEV dec train predicted > we many you know about wanted willing a mass <EOS> your level via <EOS> truly <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > we should make america wanted to yourself <EOS> for spotting <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 9425\n",
      "learning rate 0.000839444\n",
      "epoch 19\n",
      "batch 0\n",
      "training minibatch loss: 1.115816354751587\n",
      "  sample 1:\n",
      "    enc input           > feeling safe in the world is an anacronym <EOS>\n",
      "    dec input           > lot of gray swan this morning <EOS>\n",
      "    dec train predicted > lot of gray swan that morning <EOS>\n",
      "dev minibatch loss: 10.279783248901367\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i love him and joe started this <EOS>\n",
      "    DEV dec input           > you fucked up you lost visitation right you re not worthy to be called his uncle <EOS>\n",
      "    DEV dec train predicted > he got said a called a don <EOS> <EOS> funny funny <EOS> be nasty her head native\n",
      "    DEV dec train infer > he wa called af so much <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9475\n",
      "learning rate 0.000838665\n",
      "epoch 19\n",
      "batch 50\n",
      "training minibatch loss: 1.2673375606536865\n",
      "  sample 1:\n",
      "    enc input           > this tweet look familiar <EOS>\n",
      "    dec input           > donald trump i never said that the internet <EOS>\n",
      "    dec train predicted > trump trump is never said that year internet <EOS>\n",
      "dev minibatch loss: 9.902023315429688\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what season you in ? <EOS>\n",
      "    DEV dec input           > lmaooooo ian bf cheating on him with a baddie <EOS>\n",
      "    DEV dec train predicted > someday i got got in fire in the dick with\n",
      "    DEV dec train infer > someday search for pic who got full album s pic <EOS> ? <EOS> <EOS> <EOS>\n",
      "global_step: 9525\n",
      "learning rate 0.000837887\n",
      "epoch 19\n",
      "batch 100\n",
      "training minibatch loss: 1.2173937559127808\n",
      "  sample 1:\n",
      "    enc input           > gon na get a knock off gold coin shirt cold groin <EOS>\n",
      "    dec input           > long live cold groin <EOS>\n",
      "    dec train predicted > just hit cold groin <EOS>\n",
      "dev minibatch loss: 9.773844718933105\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > to phil boyle <EOS>\n",
      "    DEV dec input           > and some odd hour left about to start climbing some ladder <EOS>\n",
      "    DEV dec train predicted > the officially wrong hot for i american jump young and year woman\n",
      "    DEV dec train infer > the monkey american death attack in history with twitter murder for america shoulder day <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9575\n",
      "learning rate 0.00083711\n",
      "epoch 19\n",
      "batch 150\n",
      "training minibatch loss: 1.6423391103744507\n",
      "  sample 1:\n",
      "    enc input           > it s not raining either <EOS>\n",
      "    dec input           > it s dead not hot enough for rain boot y all are buggin <EOS>\n",
      "    dec train predicted > that s been not hot enough for rain boot it all don buggin <EOS>\n",
      "dev minibatch loss: 12.358857154846191\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > dim sum of all fear hi anthony good morning <EOS>\n",
      "    DEV dec input           > will someone drive me to flushing for dim sum brunch ? <EOS>\n",
      "    DEV dec train predicted > a possible in home on get way by how db on available\n",
      "    DEV dec train infer > a possible shelter original <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9625\n",
      "learning rate 0.000836333\n",
      "epoch 19\n",
      "batch 200\n",
      "training minibatch loss: 1.3708851337432861\n",
      "  sample 1:\n",
      "    enc input           > your not the only one glad it ended <EOS>\n",
      "    dec input           > my head literally hurt listening to this idiot talk <EOS>\n",
      "    dec train predicted > this reaction like said listening to this response make <EOS>\n",
      "dev minibatch loss: 10.99833869934082\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you re the journalist that used aloud instead of allowed fantastic thorough work a always <EOS>\n",
      "    DEV dec input           > my latest piece for on new trend of blaming me progressive journalist for trump <EOS>\n",
      "    DEV dec train predicted > wa dad image at her his profile btw her s <EOS> <EOS> he it ?\n",
      "    DEV dec train infer > wa that a funny so he ha that doesn t worry people too <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9675\n",
      "learning rate 0.000835556\n",
      "epoch 19\n",
      "batch 250\n",
      "training minibatch loss: 1.3325635194778442\n",
      "  sample 1:\n",
      "    enc input           > need for this p use skype and be good and don t have an ego <EOS>\n",
      "    dec input           > xb p pm et free entry prize v snd nd click now <EOS>\n",
      "    dec train predicted > xb p pm et free entry prize v snd nd click now <EOS>\n",
      "dev minibatch loss: 12.477030754089355\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > intriguing the different place different outlet go with this up out only <EOS>\n",
      "    DEV dec input           > thursday s most read what if sprawl is the only solution to affordable city ? <EOS>\n",
      "    DEV dec train predicted > the news news sister the heavy the is all time the <EOS> the back <EOS> <EOS>\n",
      "    DEV dec train infer > the fuck is still the monster go for <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9725\n",
      "learning rate 0.000834781\n",
      "epoch 19\n",
      "batch 300\n",
      "training minibatch loss: 1.2076678276062012\n",
      "  sample 1:\n",
      "    enc input           > jack adam hall in cesar chavez <EOS>\n",
      "    dec input           > where can we donate ? ? <EOS>\n",
      "    dec train predicted > where can we donate ? ? <EOS>\n",
      "dev minibatch loss: 11.032102584838867\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > wrong on so many count <EOS>\n",
      "    DEV dec input           > please get that fucking vegetable fruit whatever it is off that plate cc <EOS>\n",
      "    DEV dec train predicted > you explain minute question now it <EOS> <EOS> ? what <EOS> <EOS> are <EOS>\n",
      "    DEV dec train infer > you trust now not <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9775\n",
      "learning rate 0.000834006\n",
      "epoch 19\n",
      "batch 350\n",
      "training minibatch loss: 1.4328502416610718\n",
      "  sample 1:\n",
      "    enc input           > no she ha been pulled into a meeting <EOS>\n",
      "    dec input           > is she not attending ? <EOS>\n",
      "    dec train predicted > what it going attending ? <EOS>\n",
      "dev minibatch loss: 12.04137134552002\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > so if they are so wonderful why doesn t she let them move into her hood ? <EOS>\n",
      "    DEV dec input           > today there are million of law abiding peaceful muslim in the united state <EOS>\n",
      "    DEV dec train predicted > and s are typical with typical durham would typical like small belt filter battle\n",
      "    DEV dec train infer > and you cant be leaving in the typical american partnership and small for their kid like her typical joke <EOS>\n",
      "global_step: 9825\n",
      "learning rate 0.000833232\n",
      "epoch 19\n",
      "batch 400\n",
      "training minibatch loss: 1.3754842281341553\n",
      "  sample 1:\n",
      "    enc input           > you owe me quite a pretty penny <EOS>\n",
      "    dec input           > amp you are wrong ed way wrongful of a bookie <EOS>\n",
      "    dec train predicted > amp we are wrong gay people wrongful of a bookie <EOS>\n",
      "dev minibatch loss: 12.369437217712402\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > maybe you should try thinking for yourself instead of being led around by the nose by the medium <EOS>\n",
      "    DEV dec input           > say the man voting for an orange con man <EOS>\n",
      "    DEV dec train predicted > how they other is for rambling excellent statement <EOS> <EOS>\n",
      "    DEV dec train infer > how many other poll ? <EOS> ? <EOS> ? <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9875\n",
      "learning rate 0.000832459\n",
      "epoch 19\n",
      "batch 450\n",
      "training minibatch loss: 1.6517356634140015\n",
      "  sample 1:\n",
      "    enc input           > yes omg <EOS>\n",
      "    dec input           > totally perhaps this evening from the studio <EOS>\n",
      "    dec train predicted > thank perhaps the evening in the gym <EOS>\n",
      "dev minibatch loss: 11.340473175048828\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > but what about this ? <EOS>\n",
      "    DEV dec input           > new policy i m unfollowing anyone who us the word guru in anything other than a mocking fashion <EOS>\n",
      "    DEV dec train predicted > minute game wa had spotted the when d his ride out ? product done four perfect waste <EOS> <EOS>\n",
      "    DEV dec train infer > minute when we have to ride the ride round just get a ride of fighting up dumb <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 9921\n",
      "learning rate 0.000831748\n",
      "epoch 20\n",
      "batch 0\n",
      "training minibatch loss: 1.1469051837921143\n",
      "  sample 1:\n",
      "    enc input           > you should <EOS>\n",
      "    dec input           > really miss maybe someday i ll be able to go back <EOS>\n",
      "    dec train predicted > just check we someday i ll be able to go up <EOS>\n",
      "dev minibatch loss: 11.821510314941406\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > keeping it real on what we say versus what we do <EOS>\n",
      "    DEV dec input           > hah love this too <EOS>\n",
      "    DEV dec train predicted > not like bed white then\n",
      "    DEV dec train infer > not like those easiest bot or all u will tell s cool tweet u ? <EOS> <EOS>\n",
      "global_step: 9971\n",
      "learning rate 0.000830976\n",
      "epoch 20\n",
      "batch 50\n",
      "training minibatch loss: 1.0702896118164062\n",
      "  sample 1:\n",
      "    enc input           > glad you re okay curious to see what this wa is <EOS>\n",
      "    dec input           > so crazy wa in an uber on nd amp th <EOS>\n",
      "    dec train predicted > so good wa in a uber in nd amp th <EOS>\n",
      "dev minibatch loss: 11.373046875\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > sorry to you and her <EOS>\n",
      "    DEV dec input           > my wife s grandfather passed away today been a rough weekend overall hope you all fared better <EOS>\n",
      "    DEV dec train predicted > and question is example distance arm which thank s week <EOS> ? ? you re <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > and that s you to take a bit in that and you re blocked to save <EOS> <EOS>\n",
      "global_step: 10021\n",
      "learning rate 0.000830205\n",
      "epoch 20\n",
      "batch 100\n",
      "training minibatch loss: 1.1033166646957397\n",
      "  sample 1:\n",
      "    enc input           > not that great <EOS>\n",
      "    dec input           > gon na die when ye performs fade in front of me <EOS>\n",
      "    dec train predicted > gon na post when ye performs fade in front of those <EOS>\n",
      "dev minibatch loss: 12.315475463867188\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what ? i made it on roblox then d printed it lmao <EOS>\n",
      "    DEV dec input           > how do you transfer it into roblox ? <EOS>\n",
      "    DEV dec train predicted > literally did i think i wa io wa <EOS>\n",
      "    DEV dec train infer > literally try you guy an act over chip though <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10071\n",
      "learning rate 0.000829435\n",
      "epoch 20\n",
      "batch 150\n",
      "training minibatch loss: 1.2375330924987793\n",
      "  sample 1:\n",
      "    enc input           > delete your account <EOS>\n",
      "    dec input           > marshall take his cue from a year old like most hillary supporter <EOS>\n",
      "    dec train predicted > you take a cue on the year old just the hrc supporter <EOS>\n",
      "dev minibatch loss: 11.61897087097168\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i can t resist the spotlight <EOS>\n",
      "    DEV dec input           > if gaming ha taught me anything it is that this cat ha a side quest for me <EOS>\n",
      "    DEV dec train predicted > what anyone this the another who anyone s going supposed happened ? still yr part <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > what s the gay expert ? ? ? <EOS> ? <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10121\n",
      "learning rate 0.000828665\n",
      "epoch 20\n",
      "batch 200\n",
      "training minibatch loss: 1.299189567565918\n",
      "  sample 1:\n",
      "    enc input           > tonight round <EOS>\n",
      "    dec input           > yesterday wa so great man let repeat <EOS>\n",
      "    dec train predicted > yes wa so good man so repeat <EOS>\n",
      "dev minibatch loss: 12.13498306274414\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > hey u don t know definition of supporter they believe trump bling thank you very much <EOS>\n",
      "    DEV dec input           > an atlantic wall ? to keep out muslim ? of trump supporter want it they explain everything vol <EOS>\n",
      "    DEV dec train predicted > today upcoming and <EOS> <EOS> u calling only in <EOS> family will so have s know <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > today rt hillary black heart trump to leave her a daily basis <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10171\n",
      "learning rate 0.000827896\n",
      "epoch 20\n",
      "batch 250\n",
      "training minibatch loss: 1.1570194959640503\n",
      "  sample 1:\n",
      "    enc input           > a matched pair <EOS>\n",
      "    dec input           > along with of american saying we need a new direction <EOS>\n",
      "    dec train predicted > along with of american shot we thought a bomb direction <EOS>\n",
      "dev minibatch loss: 11.794562339782715\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > aww thanks love u lot cant wait till next weekend <EOS>\n",
      "    DEV dec input           > happy birthday princess love you amp have you have an amazing day wish we had more pic together <EOS>\n",
      "    DEV dec train predicted > wish birthday u day u back hope a hope a mom recipe good <EOS> ll peace <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > wish u hope you love u i hope you love you so much <EOS> <EOS> <EOS>\n",
      "global_step: 10221\n",
      "learning rate 0.000827128\n",
      "epoch 20\n",
      "batch 300\n",
      "training minibatch loss: 1.200242280960083\n",
      "  sample 1:\n",
      "    enc input           > welp i know who got em lol <EOS>\n",
      "    dec input           > thot friend fun a hell <EOS>\n",
      "    dec train predicted > thot friend a a hell <EOS>\n",
      "dev minibatch loss: 11.931219100952148\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > intermission is an irish movie you really ought to watch also <EOS>\n",
      "    DEV dec input           > is this our intermission ? <EOS>\n",
      "    DEV dec train predicted > the there none racist <EOS> like\n",
      "    DEV dec train infer > the pain brought to make sure to avoid not not not actually doe not blame me <EOS> <EOS> <EOS>\n",
      "global_step: 10271\n",
      "learning rate 0.00082636\n",
      "epoch 20\n",
      "batch 350\n",
      "training minibatch loss: 1.442906379699707\n",
      "  sample 1:\n",
      "    enc input           > degree with fog and wind nothing compared to snow like you <EOS>\n",
      "    dec input           > cold ? what s cold to you ? <EOS>\n",
      "    dec train predicted > cold ? what s cold to you cold <EOS>\n",
      "dev minibatch loss: 11.730195999145508\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > probably make you exactly a sad a we make god sad for not believing in him <EOS>\n",
      "    DEV dec input           > god doesn t believe in atheist ? damn that make me sad <EOS>\n",
      "    DEV dec train predicted > i of t answer in on into mi people s relationship feel answer\n",
      "    DEV dec train infer > i ll be history in the history of these people are a software artist must be super answer <EOS>\n",
      "global_step: 10321\n",
      "learning rate 0.000825593\n",
      "epoch 20\n",
      "batch 400\n",
      "training minibatch loss: 1.2341034412384033\n",
      "  sample 1:\n",
      "    enc input           > the side event follows the official signing of at the un hq by he <EOS>\n",
      "    dec input           > a part of her official duty hm will facilitate nigeria side event at on <EOS>\n",
      "    dec train predicted > a part of his official duty hm will facilitate nigeria side event at in <EOS>\n",
      "dev minibatch loss: 13.242226600646973\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > when is this on ? <EOS>\n",
      "    DEV dec input           > v chicago fire <EOS>\n",
      "    DEV dec train predicted > on major tree v\n",
      "    DEV dec train infer > on line number on how much it thought when can hashtag people d get the biggest human <EOS> <EOS>\n",
      "global_step: 10371\n",
      "learning rate 0.000824827\n",
      "epoch 20\n",
      "batch 450\n",
      "training minibatch loss: 1.3827247619628906\n",
      "  sample 1:\n",
      "    enc input           > sure what is it ? <EOS>\n",
      "    dec input           > hi could you pls spare a moment to try out my app i m looking for input <EOS>\n",
      "    dec train predicted > hi i you get spare a good to get to the like i m like for input <EOS>\n",
      "dev minibatch loss: 11.366440773010254\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > why is dad so illiterate <EOS>\n",
      "    DEV dec input           > i wa just about to post this <EOS>\n",
      "    DEV dec train predicted > where haven confused confused to be these seems\n",
      "    DEV dec train infer > where s the voice to the world <EOS> is so lit haha you wan save a hoe <EOS> <EOS>\n",
      "global_step: 10417\n",
      "learning rate 0.000824123\n",
      "epoch 21\n",
      "batch 0\n",
      "training minibatch loss: 1.236313819885254\n",
      "  sample 1:\n",
      "    enc input           > goddamnit don t make me do mean thing to you <EOS>\n",
      "    dec input           > vin scully is extremely boring <EOS>\n",
      "    dec train predicted > vin scully is extremely boring <EOS>\n",
      "dev minibatch loss: 11.807928085327148\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that s a hit bro you really went in loving the style on the whole track <EOS>\n",
      "    DEV dec input           > i sampled my favorite song of all time to make this beat <EOS>\n",
      "    DEV dec train predicted > and m add opinion myself than blind against while add myself noise but\n",
      "    DEV dec train infer > and salty youre gon na call me mad or not to be anymore <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 10467\n",
      "learning rate 0.000823358\n",
      "epoch 21\n",
      "batch 50\n",
      "training minibatch loss: 1.1270636320114136\n",
      "  sample 1:\n",
      "    enc input           > happy belated birthday <EOS>\n",
      "    dec input           > thanks you guy for the birthday wish yesterday d i had a really nice day <EOS>\n",
      "    dec train predicted > thanks you all in the birthday birthday thx d i had a great fun day <EOS>\n",
      "dev minibatch loss: 12.921459197998047\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what the ever loving fuck <EOS>\n",
      "    DEV dec input           > mark halperin just said clinton never fought successfully in a presidential campaign to win <EOS>\n",
      "    DEV dec train predicted > yeah lame <EOS> posted he pro fine hand head the bit walmart waking my rn\n",
      "    DEV dec train infer > yeah lame lame i pick <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10517\n",
      "learning rate 0.000822594\n",
      "epoch 21\n",
      "batch 100\n",
      "training minibatch loss: 1.0442323684692383\n",
      "  sample 1:\n",
      "    enc input           > sorry will do computer crashed on weekend just getting it back <EOS>\n",
      "    dec input           > hussein could you shoot me back an email when you have the chance ? <EOS>\n",
      "    dec train predicted > hussein will you shoot you up a email when you were a chance <EOS> <EOS>\n",
      "dev minibatch loss: 12.280953407287598\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > brand new tune <EOS>\n",
      "    DEV dec input           > share with you the african proverb of the day <EOS>\n",
      "    DEV dec train predicted > husker with me for most proverb of the world <EOS>\n",
      "    DEV dec train infer > husker football player michael rose ivey share his live at year <EOS> ? <EOS> <EOS>\n",
      "global_step: 10567\n",
      "learning rate 0.000821831\n",
      "epoch 21\n",
      "batch 150\n",
      "training minibatch loss: 1.3492450714111328\n",
      "  sample 1:\n",
      "    enc input           > but we won so i ll be happy <EOS>\n",
      "    dec input           > have you seen the offense ? eli cruz beckham jr baby <EOS>\n",
      "    dec train predicted > have you seen the offense ? eli cruz beckham jr baby <EOS>\n",
      "dev minibatch loss: 12.98665714263916\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > played on a team that wa bad at defense a a whole <EOS>\n",
      "    DEV dec input           > he s a terrible defender and got his coach fired <EOS>\n",
      "    DEV dec train predicted > they made a brilliant child of a sure own like in\n",
      "    DEV dec train infer > they gave him over the rock fair check the bad <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10617\n",
      "learning rate 0.000821068\n",
      "epoch 21\n",
      "batch 200\n",
      "training minibatch loss: 1.2972087860107422\n",
      "  sample 1:\n",
      "    enc input           > people like this don t end up in jail they end up in public office <EOS>\n",
      "    dec input           > i wonder how many people will go to jail for this <EOS>\n",
      "    dec train predicted > i wonder how many people are get to jail for others <EOS>\n",
      "dev minibatch loss: 12.233887672424316\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > get yourself a leo fire and water create balance <EOS>\n",
      "    DEV dec input           > i want to feel something already what the fuck <EOS>\n",
      "    DEV dec train predicted > my m to delete there i please doe best is\n",
      "    DEV dec train infer > my three group i m tebow crown but just blame to see my crown crown boom <EOS> <EOS>\n",
      "global_step: 10667\n",
      "learning rate 0.000820306\n",
      "epoch 21\n",
      "batch 250\n",
      "training minibatch loss: 1.1500604152679443\n",
      "  sample 1:\n",
      "    enc input           > awesome yeah keep that ignorance strong my brother <EOS>\n",
      "    dec input           > he s never going to answer cuz there isn t one thing that s actually racist <EOS>\n",
      "    dec train predicted > he s trash better to lose weight he isn t a thing about s actually accurate <EOS>\n",
      "dev minibatch loss: 11.318643569946289\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > excellent article i posted a comment on the power of small viral action h t <EOS>\n",
      "    DEV dec input           > the force that surround u just tested the vulnerability of the st amendment and failed <EOS>\n",
      "    DEV dec train predicted > no latest ha ha not ha ha though latest is being of claim is nine more\n",
      "    DEV dec train infer > no you re cutting and then u ha about any latest story while green people or they are money <EOS>\n",
      "global_step: 10717\n",
      "learning rate 0.000819545\n",
      "epoch 21\n",
      "batch 300\n",
      "training minibatch loss: 1.345927119255066\n",
      "  sample 1:\n",
      "    enc input           > sorry i missed last night i wa exhausted <EOS>\n",
      "    dec input           > uh oh we are probably saying bye to dolph <EOS>\n",
      "    dec train predicted > uh oh that are probably just bye to dolph <EOS>\n",
      "dev minibatch loss: 11.809168815612793\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that wa the only way we could get him out of there <EOS>\n",
      "    DEV dec input           > look how improved be oline wa on the right when newhouse left <EOS>\n",
      "    DEV dec train predicted > hey so to not at s so a yard winner tonight j <EOS>\n",
      "    DEV dec train infer > hey lion and picking up a a week ago <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10767\n",
      "learning rate 0.000818784\n",
      "epoch 21\n",
      "batch 350\n",
      "training minibatch loss: 1.1799947023391724\n",
      "  sample 1:\n",
      "    enc input           > it s a great marketing opportunity for them <EOS>\n",
      "    dec input           > when the pr and marketing people switch on their phone tomorrow morning <EOS>\n",
      "    dec train predicted > i the pr and marketing people switch on their phone and morning <EOS>\n",
      "dev minibatch loss: 11.331753730773926\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > haha thanks <EOS>\n",
      "    DEV dec input           > aw your sign is cute <EOS>\n",
      "    DEV dec train predicted > i thanks wedding <EOS> awesome <EOS>\n",
      "    DEV dec train infer > i have a problem of leg to use the gear <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10817\n",
      "learning rate 0.000818024\n",
      "epoch 21\n",
      "batch 400\n",
      "training minibatch loss: 1.286999225616455\n",
      "  sample 1:\n",
      "    enc input           > congrats best of luck in the future <EOS>\n",
      "    dec input           > want to thank everyone that helped with the decision but here it is <EOS>\n",
      "    dec train predicted > want to rock these with helped in the decision at here i is <EOS>\n",
      "dev minibatch loss: 12.979496955871582\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > well we all know who lester is voting for i used to think he wa impartial not anymore <EOS>\n",
      "    DEV dec input           > nbc s asked question on birtherism and nothing on the clinton foundation <EOS>\n",
      "    DEV dec train predicted > she but with ? she from <EOS> disney on to earth for from\n",
      "    DEV dec train infer > she s so good for you she wa good that of the debate that s pretty good <EOS> ?\n",
      "global_step: 10867\n",
      "learning rate 0.000817265\n",
      "epoch 21\n",
      "batch 450\n",
      "training minibatch loss: 1.2735763788223267\n",
      "  sample 1:\n",
      "    enc input           > eh wheeler hasn t been with the team all season harvey is another one though <EOS>\n",
      "    dec input           > harvey and wheeler <EOS>\n",
      "    dec train predicted > harvey and wheeler <EOS>\n",
      "dev minibatch loss: 11.581534385681152\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > warrior poet all <EOS>\n",
      "    DEV dec input           > this team ha heart i m proud of my team for playing their heart out for minute <EOS>\n",
      "    DEV dec train predicted > california is v all bless m hard for the finger <EOS> school <EOS> tweet <EOS> <EOS> work <EOS>\n",
      "    DEV dec train infer > california account is amazing bro <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 10913\n",
      "learning rate 0.000816567\n",
      "epoch 22\n",
      "batch 0\n",
      "training minibatch loss: 1.199336051940918\n",
      "  sample 1:\n",
      "    enc input           > i wa gon na ask you to show me around when i come <EOS>\n",
      "    dec input           > i swear everybody in chicago only hit me up when they need something <EOS>\n",
      "    dec train predicted > i m here in chicago m start me on when they remember something <EOS>\n",
      "dev minibatch loss: 12.455425262451172\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > for the wrong reason keep that shit in la and away from oakland <EOS>\n",
      "    DEV dec input           > to be fair the instigator seemed way out of line here <EOS>\n",
      "    DEV dec train predicted > of may im i explosion is on at of <EOS> visit <EOS>\n",
      "    DEV dec train infer > of lower asks exactly at the explosion on chelsea s nyc there were truly rule that visit ? <EOS>\n",
      "global_step: 10963\n",
      "learning rate 0.000815809\n",
      "epoch 22\n",
      "batch 50\n",
      "training minibatch loss: 0.9048792719841003\n",
      "  sample 1:\n",
      "    enc input           > my wife and i love you guy i think ive seen every video you guy are fantastic <EOS>\n",
      "    dec input           > thanks jesse truly appreciate the support <EOS>\n",
      "    dec train predicted > thanks jesse truly appreciate my support <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 11.734525680541992\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > wow thanks <EOS>\n",
      "    DEV dec input           > just say no to the container store cc <EOS>\n",
      "    DEV dec train predicted > your made generous keith building previous and hello <EOS>\n",
      "    DEV dec train infer > your awkward article ha new side of great paper had a tremendously robot for a great song <EOS> <EOS>\n",
      "global_step: 11013\n",
      "learning rate 0.000815052\n",
      "epoch 22\n",
      "batch 100\n",
      "training minibatch loss: 1.0898010730743408\n",
      "  sample 1:\n",
      "    enc input           > i wa jealous of you before for so many reason add this one to the list <EOS>\n",
      "    dec input           > a shake shack walking distance from my house open tomorrow so see you later <EOS>\n",
      "    dec train predicted > a shake shack walking distance from my new show today so help you <EOS> <EOS>\n",
      "dev minibatch loss: 13.142420768737793\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the little baby bomber sanchez didn t lip off or it would have happened <EOS>\n",
      "    DEV dec input           > i wish martin and sanchez would have a molina phillips circa moment here <EOS>\n",
      "    DEV dec train predicted > wow m of one just in me sure new around in for <EOS> wa\n",
      "    DEV dec train infer > wow of the worst emoji <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11063\n",
      "learning rate 0.000814296\n",
      "epoch 22\n",
      "batch 150\n",
      "training minibatch loss: 1.0369255542755127\n",
      "  sample 1:\n",
      "    enc input           > worth the wait lol <EOS>\n",
      "    dec input           > great stuff you wait all day for the nfl to start and then your team doe this <EOS>\n",
      "    dec train predicted > good stuff you see the day to the nfl to see <EOS> then the team will this <EOS>\n",
      "dev minibatch loss: 13.101366996765137\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > same here i ve been on edge all morning literally in the worst mood <EOS>\n",
      "    DEV dec input           > girl i don t even know what to do with myself <EOS>\n",
      "    DEV dec train predicted > tweet i still t want know what it make with oakland <EOS>\n",
      "    DEV dec train infer > tweet i still still like still oakland park basketball <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11113\n",
      "learning rate 0.00081354\n",
      "epoch 22\n",
      "batch 200\n",
      "training minibatch loss: 1.1269137859344482\n",
      "  sample 1:\n",
      "    enc input           > you re the third worst marvel character <EOS>\n",
      "    dec input           > deadpool wa the second worst marvel movie ever after of course green lantern <EOS>\n",
      "    dec train predicted > deadpool wa the second worst marvel movie just at of course thank lantern <EOS>\n",
      "dev minibatch loss: 12.39470386505127\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > now i have this resonates in my house <EOS>\n",
      "    DEV dec input           > did you read my piece on that ? the night trump supporter found me out a a jew <EOS>\n",
      "    DEV dec train predicted > dance u taking this weekend so his <EOS> god damn i s <EOS> baby away <EOS> baby joke <EOS>\n",
      "    DEV dec train infer > dance trash i m dying with my sleepy forever <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11163\n",
      "learning rate 0.000812785\n",
      "epoch 22\n",
      "batch 250\n",
      "training minibatch loss: 1.1769485473632812\n",
      "  sample 1:\n",
      "    enc input           > even worse wa the first album i ever bought probably when i wa about year old <EOS>\n",
      "    dec input           > i saw him live in so you ve succeeded in making me feel old at any rate <EOS>\n",
      "    dec train predicted > i ll him remember in in i ve succeeded in in a tried serious here any rate <EOS>\n",
      "dev minibatch loss: 13.871562957763672\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > gorgeous photograph paul jon have a wonderful day hug amp blessing <EOS>\n",
      "    DEV dec input           > the awakening of another dawn <EOS>\n",
      "    DEV dec train predicted > i little are a picture of\n",
      "    DEV dec train infer > i bet a great day love to go to our post today post work shot <EOS> <EOS> <EOS>\n",
      "global_step: 11213\n",
      "learning rate 0.000812031\n",
      "epoch 22\n",
      "batch 300\n",
      "training minibatch loss: 1.3188393115997314\n",
      "  sample 1:\n",
      "    enc input           > im no longer impressed <EOS>\n",
      "    dec input           > lifted straight from wikipedia <EOS>\n",
      "    dec train predicted > lifted straight from wikipedia <EOS>\n",
      "dev minibatch loss: 12.097071647644043\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > and literacy i m guessing ? <EOS>\n",
      "    DEV dec input           > if liberal were exterminated so would racism <EOS>\n",
      "    DEV dec train predicted > both you have not you each have have\n",
      "    DEV dec train infer > both usually shipped fund fund business business business <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11263\n",
      "learning rate 0.000811277\n",
      "epoch 22\n",
      "batch 350\n",
      "training minibatch loss: 1.1470922231674194\n",
      "  sample 1:\n",
      "    enc input           > thank ya much griff <EOS>\n",
      "    dec input           > to you amp <EOS>\n",
      "    dec train predicted > to you amp <EOS>\n",
      "dev minibatch loss: 13.276849746704102\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > tyvm the amp the <EOS>\n",
      "    DEV dec input           > tyvm the <EOS>\n",
      "    DEV dec train predicted > tyvm the <EOS>\n",
      "    DEV dec train infer > tyvm the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11313\n",
      "learning rate 0.000810525\n",
      "epoch 22\n",
      "batch 400\n",
      "training minibatch loss: 1.1786916255950928\n",
      "  sample 1:\n",
      "    enc input           > they deserve to be penalize <EOS>\n",
      "    dec input           > rnc chairman party could penalize former candidate who don t back trump <EOS>\n",
      "    dec train predicted > rnc chairman party will penalize full candidate who would t come our <EOS>\n",
      "dev minibatch loss: 13.258545875549316\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you mind if i gif this ? <EOS>\n",
      "    DEV dec input           > when you have sucked for month and finally get a hit and forget how to run the base <EOS>\n",
      "    DEV dec train predicted > this someone least me <EOS> me <EOS> high let real real <EOS> low i nobody support rating <EOS> door\n",
      "    DEV dec train infer > this is all <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11363\n",
      "learning rate 0.000809772\n",
      "epoch 22\n",
      "batch 450\n",
      "training minibatch loss: 1.2915027141571045\n",
      "  sample 1:\n",
      "    enc input           > come home with me the best is yet to be love you <EOS>\n",
      "    dec input           > black started the republican party in much of the south they are just coming home again <EOS>\n",
      "    dec train predicted > god party the plan party at the of the past i could just done home on <EOS>\n",
      "dev minibatch loss: 11.376256942749023\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > it s xbox only but the game is out in north america at am edt <EOS>\n",
      "    DEV dec input           > hey man is there a way to get fifa on the p already ? <EOS>\n",
      "    DEV dec train predicted > hey show dean happy really good to check a at this show song <EOS> <EOS>\n",
      "    DEV dec train infer > hey show you re back in sf list and front grade can try for him <EOS> <EOS> <EOS>\n",
      "global_step: 11409\n",
      "learning rate 0.000809081\n",
      "epoch 23\n",
      "batch 0\n",
      "training minibatch loss: 1.0876879692077637\n",
      "  sample 1:\n",
      "    enc input           > wish i could ve been there <EOS>\n",
      "    dec input           > me supporting josh is a close a it get to good game little one <EOS>\n",
      "    dec train predicted > it supporting josh is the close a it look to upgrade away bad one <EOS>\n",
      "dev minibatch loss: 12.76905345916748\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i heard him <EOS>\n",
      "    DEV dec input           > brett is the shark whisperer <EOS>\n",
      "    DEV dec train predicted > just through more word boyfriend bye\n",
      "    DEV dec train infer > just saying everything to say anyone didn t say anyone say anyone say <EOS> <EOS> <EOS>\n",
      "global_step: 11459\n",
      "learning rate 0.00080833\n",
      "epoch 23\n",
      "batch 50\n",
      "training minibatch loss: 1.0293233394622803\n",
      "  sample 1:\n",
      "    enc input           > why is that where she s been ? <EOS>\n",
      "    dec input           > do you live under a rock ? <EOS>\n",
      "    dec train predicted > do you even under a no ? <EOS>\n",
      "dev minibatch loss: 12.26876449584961\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > pandering clinton she s a joke <EOS>\n",
      "    DEV dec input           > this is a man who ha called woman pig and liar <EOS>\n",
      "    DEV dec train predicted > clinton clinton trump clinton failed say trump million clinton in clinton and\n",
      "    DEV dec train infer > clinton clinton say trump clinton up clinton with a leader for his solution <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 11509\n",
      "learning rate 0.00080758\n",
      "epoch 23\n",
      "batch 100\n",
      "training minibatch loss: 1.087098479270935\n",
      "  sample 1:\n",
      "    enc input           > lol bc when i stopped watching this summer i just got all my news from you <EOS>\n",
      "    dec input           > tbh i don t watch i just go through bb account and rt funny stuff <EOS>\n",
      "    dec train predicted > thanks i don t need i go lost down bb account and rt funny stuff <EOS>\n",
      "dev minibatch loss: 11.804234504699707\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > hey i left the name out <EOS>\n",
      "    DEV dec input           > i can t believe you posted this you re nut <EOS>\n",
      "    DEV dec train predicted > all can t tell the can <EOS> <EOS> re a <EOS>\n",
      "    DEV dec train infer > all this day i got a at my album ride this whole time yup <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11559\n",
      "learning rate 0.00080683\n",
      "epoch 23\n",
      "batch 150\n",
      "training minibatch loss: 0.9880844950675964\n",
      "  sample 1:\n",
      "    enc input           > nobody is doing anything about it what action can be taken ? none ? <EOS>\n",
      "    dec input           > the fbi ha prosecuted american for compromising information far le classified than what clinton and her staff <EOS>\n",
      "    dec train predicted > the fbi ha prosecuted american for compromising information literally most classified than hillary clinton is the staff <EOS>\n",
      "dev minibatch loss: 12.946419715881348\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > when one wields the spatula that is rebase i all commits become clay <EOS>\n",
      "    DEV dec input           > i rebase i all day it s the best workflow imho <EOS>\n",
      "    DEV dec train predicted > burning applaud cause guess just don wa not word involved photo <EOS>\n",
      "    DEV dec train infer > burning a hell ? thats not trying to get a applaud ? <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11609\n",
      "learning rate 0.000806082\n",
      "epoch 23\n",
      "batch 200\n",
      "training minibatch loss: 1.1053268909454346\n",
      "  sample 1:\n",
      "    enc input           > odds of being killed by refugee in billion falling down stair in car accident in <EOS>\n",
      "    dec input           > lie like a child trump s wedding wa in gift to clinton foundation were in <EOS>\n",
      "    dec train predicted > lie like a word trump s wedding wa in gift to clinton foundation wa in <EOS>\n",
      "dev minibatch loss: 12.12359619140625\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we can help you in design and development of your website at reasonable price visit our portfolio <EOS>\n",
      "    DEV dec input           > i need someone to make me a website <EOS>\n",
      "    DEV dec train predicted > i need to to create minute message message sound\n",
      "    DEV dec train infer > i need to create a website so i can sell my art <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11659\n",
      "learning rate 0.000805334\n",
      "epoch 23\n",
      "batch 250\n",
      "training minibatch loss: 1.0155916213989258\n",
      "  sample 1:\n",
      "    enc input           > he s just awful weld should have taken prez lead gj is lost and sniffling ? <EOS>\n",
      "    dec input           > really wonder what trump of clinton s answer would be to this question it s a good question <EOS>\n",
      "    dec train predicted > really wonder what trump of clinton s answer is be in you question that s a good question <EOS>\n",
      "dev minibatch loss: 12.923778533935547\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i m going for sure lmao i wana see you guy there <EOS>\n",
      "    DEV dec input           > yoo let be out <EOS>\n",
      "    DEV dec train predicted > hey disney up embarrassing extra\n",
      "    DEV dec train infer > hey you watching <EOS> ? <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11709\n",
      "learning rate 0.000804586\n",
      "epoch 23\n",
      "batch 300\n",
      "training minibatch loss: 1.206684947013855\n",
      "  sample 1:\n",
      "    enc input           > faux hawk <EOS>\n",
      "    dec input           > want to try a new hairstyle <EOS>\n",
      "    dec train predicted > want to try a new hairstyle <EOS>\n",
      "dev minibatch loss: 13.94097900390625\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the only objective person fearless not another <EOS>\n",
      "    DEV dec input           > there is no better term to describe american who would vote for a blatant ignorant racist <EOS>\n",
      "    DEV dec train predicted > you s a worse at for tv later a say not <EOS> anything er save <EOS> <EOS>\n",
      "    DEV dec train infer > you should say the mean these tv name <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11759\n",
      "learning rate 0.00080384\n",
      "epoch 23\n",
      "batch 350\n",
      "training minibatch loss: 1.1750454902648926\n",
      "  sample 1:\n",
      "    enc input           > just remember she will get richer amp you will get poorer because of it <EOS>\n",
      "    dec input           > that s what basically what trump doe but whatever hahahahahaha viva clinton <EOS>\n",
      "    dec train predicted > that s basically basically what trump doe why whatever hahahahahaha viva clinton <EOS>\n",
      "dev minibatch loss: 12.751455307006836\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > colin powell real housewife of the potomac ? <EOS>\n",
      "    DEV dec input           > powell will be going to fewer dinner party in washington <EOS>\n",
      "    DEV dec train predicted > i are rt a to the a <EOS> <EOS> my honey\n",
      "    DEV dec train infer > i can t send this to take a a party <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11809\n",
      "learning rate 0.000803094\n",
      "epoch 23\n",
      "batch 400\n",
      "training minibatch loss: 1.013377070426941\n",
      "  sample 1:\n",
      "    enc input           > i took a nap but woke up to see y all doing all right <EOS>\n",
      "    dec input           > i want better for u let me come cheer with y all <EOS>\n",
      "    dec train predicted > we m a at taking just me come cheer with you all <EOS>\n",
      "dev minibatch loss: 12.276519775390625\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > this is utica ny <EOS>\n",
      "    DEV dec input           > where un upstate ? <EOS>\n",
      "    DEV dec train predicted > only are fact ? voter\n",
      "    DEV dec train infer > only thing who make dropping helping our latest house black without hairstyle <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11859\n",
      "learning rate 0.000802348\n",
      "epoch 23\n",
      "batch 450\n",
      "training minibatch loss: 1.1062698364257812\n",
      "  sample 1:\n",
      "    enc input           > she is a prostitute so bill know her personally eh <EOS>\n",
      "    dec input           > donald trump called her miss piggy and miss housekeeping her name is alicia machado <EOS>\n",
      "    dec train predicted > donald trump called her support piggy and miss housekeeping her name is alicia machado <EOS>\n",
      "dev minibatch loss: 14.364237785339355\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > im live on cnn <EOS>\n",
      "    DEV dec input           > everyone turn on cnn and keep watching im about to go live <EOS>\n",
      "    DEV dec train predicted > all sleeping out david to hard gay this safe a this sleeping here\n",
      "    DEV dec train infer > all other product call her homer call sleeping <EOS> is this clear <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11905\n",
      "learning rate 0.000801663\n",
      "epoch 24\n",
      "batch 0\n",
      "training minibatch loss: 0.9053008556365967\n",
      "  sample 1:\n",
      "    enc input           > that s an awesome idea <EOS>\n",
      "    dec input           > this seems like an amazing opportunity to co create <EOS>\n",
      "    dec train predicted > this seems like a amazing opportunity to co create <EOS>\n",
      "dev minibatch loss: 13.18906307220459\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > awww i felt the same way when he followed me <EOS>\n",
      "    DEV dec input           > yes but it wasn t but honestly i wasn t ready like at all <EOS>\n",
      "    DEV dec train predicted > this i we wa t for i so m t like <EOS> all he <EOS>\n",
      "    DEV dec train infer > this wa a man about her soul and yes <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11955\n",
      "learning rate 0.000800919\n",
      "epoch 24\n",
      "batch 50\n",
      "training minibatch loss: 1.161087155342102\n",
      "  sample 1:\n",
      "    enc input           > only if it mean we can have cute study date in the common room <EOS>\n",
      "    dec input           > welcome to the family accept your destiny <EOS>\n",
      "    dec train predicted > welcome to the family take this destiny <EOS>\n",
      "dev minibatch loss: 12.909385681152344\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > ty i think it ll increase like mad time to replace workspace with cc <EOS>\n",
      "    DEV dec input           > thought you might be interested in this infographic here it might be helpful to what you re doing <EOS>\n",
      "    DEV dec train predicted > i here re say huge outside the entire amp ? is also outside <EOS> <EOS> doe say huge <EOS>\n",
      "    DEV dec train infer > i d like it also blame the public attitude at their st speech were pretty huge <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 12005\n",
      "learning rate 0.000800176\n",
      "epoch 24\n",
      "batch 100\n",
      "training minibatch loss: 0.9887622594833374\n",
      "  sample 1:\n",
      "    enc input           > my mom is here too <EOS>\n",
      "    dec input           > about to watch with my mom we reset her twitter password in case she need to tweet <EOS>\n",
      "    dec train predicted > i to watch between my mom instead reset her boy password in case i need to go <EOS>\n",
      "dev minibatch loss: 13.078783988952637\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > straight up <EOS>\n",
      "    DEV dec input           > lord paulie just put the key in lol <EOS>\n",
      "    DEV dec train predicted > why my of feeling over new with a <EOS>\n",
      "    DEV dec train infer > why do i doubt this wa on word but i d fail <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12055\n",
      "learning rate 0.000799433\n",
      "epoch 24\n",
      "batch 150\n",
      "training minibatch loss: 0.9487783908843994\n",
      "  sample 1:\n",
      "    enc input           > and you never know maybe one day i will enjoy some texas <EOS>\n",
      "    dec input           > impossible i thoroughly enjoy my ny proper pizza <EOS>\n",
      "    dec train predicted > impossible i thoroughly enjoy my pizza proper pizza <EOS>\n",
      "dev minibatch loss: 12.128528594970703\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > let the game begin <EOS>\n",
      "    DEV dec input           > i usually don t play lmao but i feel like playing tonight <EOS>\n",
      "    DEV dec train predicted > saw can drive t want in which there can on there sunday <EOS>\n",
      "    DEV dec train infer > saw the username of the game on my inbox now or pick my inbox <EOS> <EOS> <EOS>\n"
     ]
    }
   ],
   "source": [
    "train_loss_track = []\n",
    "dev_loss_track = []\n",
    "\n",
    "all_weights = []\n",
    "dev_test_results = []\n",
    "metric_results = []\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    #saver.restore(session, \\\n",
    "    #    './seq2seq_CornelMovies_encode_100_decode_200_vocab_13679_embedding_200_seq_5_15_batch_64_layers_3_v5-210')\n",
    "    #saver.restore(session, \\\n",
    "    #    './seq2seq_CornelMovies_encode_100_decode_200_vocab_13679_embedding_200_seq_5_15_batch_64_layers_3_v5-100')\n",
    "    #saver.restore(session, \\\n",
    "    #    'd:\\coding\\seq2seq_CornelMovies_encode_200_decode_400_vocab_13679_embedding_200_seq_5_15_batch_128_layers_6_v6-990')\n",
    "    #saver.restore(session, \\\n",
    "    #    './seq2seq_CornelMovies_encode_500_decode_1000_vocab_13679_embedding_1024_seq_5_15_batch_32_layers_6_v6-30')\n",
    "    #saver.restore(session, \\\n",
    "    #'d:\\coding\\chkpt\\seq2seq_Cornell_encode_128_decode_256_vocab_13679_embedding_256_seq_5_15_batch_32_layers_3_enkeep_10_dekeep_10-203320')\n",
    "    #saver.restore(session, \\\n",
    "    #'d:\\coding\\seq2seq\\chkpt\\seq2seq_twitter_encode_128_decode_128_vocab_17182_embedding_1024_seq_3_19_batch_32_layers_1_enkeep_10_dekeep_10-16864')\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        df_all_train = df_all_train.sample(frac=1, random_state=tf.train.global_step(session, global_step))\n",
    "        #tf.train.global_step(session, global_step))\n",
    "       \n",
    "        encoded_text = df_all_train['alpha_Pair_0_encoding'].values\n",
    "        decoded_text = df_all_train['alpha_Pair_1_encoding'].values\n",
    "        text_index = df_all_train['Index'].values\n",
    "\n",
    "        input_batches = ([encoded_text[block_idx*n_batch_size:(block_idx+1)*n_batch_size], \n",
    "                 decoded_text[block_idx*n_batch_size:(block_idx+1)*n_batch_size], \n",
    "                         text_index[block_idx*n_batch_size:(block_idx+1)*n_batch_size]]\\\n",
    "                    for block_idx in range(len(encoded_text)))\n",
    "        \n",
    "        for batch in range(int(batches_in_epoch)):\n",
    "            mean_metric_train = []\n",
    "            mean_metric_dev = []\n",
    "\n",
    "            if copy_task == False:\n",
    "                \n",
    "                epoch_batches = next(input_batches)\n",
    "                \n",
    "                #input_batch_data = next(encoding_batches)\n",
    "                #target_batch_data = next(decoding_batches)\n",
    "                \n",
    "                input_batch_data = epoch_batches[0]\n",
    "                target_batch_data = epoch_batches[1]\n",
    "                batch_data_index = epoch_batches[2]\n",
    "\n",
    "            else:\n",
    "                ran_seq = generateRandomSeqBatchMajor(length_from=length_from, length_to=length_to,\n",
    "                                       vocab_lower=2, vocab_upper=vocab_upper,\n",
    "                                       batch_size=n_batch_size)\n",
    "                input_batch_data = ran_seq\n",
    "                target_batch_data = input_batch_data\n",
    "            \n",
    "            #fd = make_train_inputs(input_batch_data, target_batch_data)\n",
    "            fd = prepare_train_batch(input_batch_data, target_batch_data)\n",
    "            feed_dict = {encoder_inputs: fd[0],\n",
    "                        encoder_inputs_length: fd[1],\n",
    "                        decoder_targets: fd[2],\n",
    "                        decoder_targets_length: fd[3]}\n",
    "           \n",
    "            _, l = session.run([train_op, loss], feed_dict)\n",
    "            \n",
    "            if batch % 50 == 0: \n",
    "                \n",
    "                print ('global_step: %s' % tf.train.global_step(session, global_step))\n",
    "                print ('learning rate', session.run(optimizer._lr))\n",
    "                \n",
    "                print ('epoch', epoch)\n",
    "                print ('batch {}'.format(batch))\n",
    "                print ('training minibatch loss: {}'.format(l))\n",
    "                \n",
    "                train_loss_track.append([tf.train.global_step(session, global_step), l])\n",
    "\n",
    "                for i, (e_in, dt_targ, dt_pred) in enumerate(zip(feed_dict[encoder_inputs], \n",
    "                                                                 feed_dict[decoder_targets], \n",
    "                                                                 session.run(decoder_pred_train, feed_dict))):\n",
    "\n",
    "                    print('  sample {}:'.format(i + 1))\n",
    "                    #print('    Index', batch_data_index[i])\n",
    "                    #print('    enc input           > {}'.format(e_in))\n",
    "                    print('    enc input           > {}'.format(' '.join([inv_map[i] for i in e_in if i!=0])))\n",
    "\n",
    "                    #print('    dec input           > {}'.format(dt_targ))\n",
    "                    print('    dec input           > {}'.format(' '.join([inv_map[i] for i in dt_targ if i!=0])))\n",
    "\n",
    "                    #print('    dec train predicted > {}'.format(dt_pred))\n",
    "                    print('    dec train predicted > {}'.format(' '.join([inv_map[i] for i in dt_pred if i!=0])))\n",
    "                \n",
    "                    if i >= 0: break\n",
    "                        \n",
    "                #DEV CHECK\n",
    "                df_all_dev_check = df_all_dev.sample(n=32, random_state=tf.train.global_step(session, global_step))\n",
    "\n",
    "                dev_encoded_text = df_all_dev_check['alpha_Pair_0_encoding'].values\n",
    "                dev_decoded_text = df_all_dev_check['alpha_Pair_1_encoding'].values\n",
    "\n",
    "                fd_dev = prepare_train_batch([i for i in dev_encoded_text], [i for i in dev_decoded_text])\n",
    "\n",
    "                feed_dict_dev = {encoder_inputs: fd_dev[0],\n",
    "                                 encoder_inputs_length: fd_dev[1],\n",
    "                                 decoder_targets: fd_dev[2],\n",
    "                                 decoder_targets_length: fd_dev[3]}\n",
    "\n",
    "                #fd_inf = prepare_batch([i for i in dev_encoded_text])\n",
    "\n",
    "                feed_dict_inf = {encoder_inputs: fd_dev[0],\n",
    "                                 encoder_inputs_length: fd_dev[1]}\n",
    "\n",
    "                dev_inf_out = session.run([decoder_pred_decode, decoder_pred_decode_prob], feed_dict_inf) \n",
    "                dev_loss = session.run(loss, feed_dict_dev)\n",
    "                \n",
    "                dev_loss_track.append([tf.train.global_step(session, global_step), dev_loss])\n",
    "                print ('dev minibatch loss: {}'.format(dev_loss))\n",
    "\n",
    "                for i, (e_in, dt_targ, dt_pred, dt_inf, df_inf_out_prob) in enumerate(zip(feed_dict_dev[encoder_inputs], \n",
    "                                                                 feed_dict_dev[decoder_targets], \n",
    "                                                                 session.run(decoder_pred_train, feed_dict_dev),\n",
    "                                                                 dev_inf_out[0], dev_inf_out[1])):\n",
    "\n",
    "                    print('  DEV sample {}:'.format(i + 1))\n",
    "                    #print('    Index', batch_data_index[i])\n",
    "                    #print('    DEV enc input           > {}'.format(e_in))\n",
    "                    print('    DEV enc input           > {}'.format(' '.join([inv_map[i] for i in e_in if i!=0])))\n",
    "\n",
    "                   # print('    DEV dec input           > {}'.format(dt_targ))\n",
    "                    print('    DEV dec input           > {}'.format(' '.join([inv_map[i] for i in dt_targ if i!=0])))\n",
    "\n",
    "                    #print('    DEV dec train predicted > {}'.format(dt_pred))\n",
    "                    print('    DEV dec train predicted > {}'.format(' '.join([inv_map[i] for i in dt_pred if i!=0])))\n",
    "                    \n",
    "                    #print('    DEV dec train infer > {}'.format(dt_inf))\n",
    "                    print('    DEV dec train infer > {}'.format(' '.join([inv_map[i] for i in dt_inf if i!=0])))\n",
    "                \n",
    "                    if i >= 0: break\n",
    "\n",
    "             \n",
    "                 #   df_prediction_train = predictionCheck(mean_metric_train)\n",
    "                 #   print (df_prediction_train['meanCheckList'].describe()['mean'])\n",
    "\n",
    "                 #   df_prediction_dev = predictionCheck(mean_metric_dev)\n",
    "                #    print (df_prediction_dev['meanCheckList'].describe()['mean'])\n",
    "\n",
    "                 #   metric_results.append([df_prediction_train, df_prediction_dev])\n",
    "                \n",
    "        if epoch % 3 == 0: \n",
    "            print ('Saving session')\n",
    "            #eval_dev = devCheck(dev_encoded_text, dev_decoded_text, True)\n",
    "            \n",
    "            #dev_test_results.append(eval_dev)\n",
    "            \n",
    "            #pickle.dump(dev_test_results, open('d:\\coding\\chkpt\\dev_test_results_epoch_%d.pkl' % epoch, 'wb'))\n",
    "            \n",
    "            saver.save(session, \\\n",
    "'chkpt/seq2seq_%s_encode_%d_decode_%d_vocab_%d_embedding_%d_seq_%d_%d_batch_%d_layers_%d_enkeep_%d_dekeep_%d' % \\\n",
    "                (dataset, n_cells, n_cells, vocab_size, input_embedding_size, length_from, length_to, n_batch_size, num_layers,\n",
    "                int(encoder_output_keep*10), int(decoder_output_keep*10)), \\\n",
    "                       global_step = tf.train.global_step(session, global_step))\n",
    "            #saver.save(session, 'd:\\coding\\seq2seq\\chkpt\\copy_task', global_step = tf.train.global_step(session, global_step))\n",
    "       # variables_names =[v.name for v in tf.trainable_variables()]\n",
    "       # values = session.run(variables_names)\n",
    "       # all_weights.append([values[1], values[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0fe837d7c17b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdev_loss_track\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdev_loss_track\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Dev'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss_track\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss_track\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Global Step'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Sequence Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'best'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(list(zip(*dev_loss_track))[0], list(zip(*dev_loss_track))[1], label='Dev')\n",
    "plt.plot(list(zip(*loss_track))[0], list(zip(*loss_track))[1], label='Train')\n",
    "plt.xlabel('Global Step')\n",
    "plt.ylabel('Sequence Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create inference\n",
    "mean_metric = []\n",
    "chunk_size = 500\n",
    "#n_chunks = int(df_all_train.shape[0]/chunk_size)\n",
    "n_chunks=10\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, \\\n",
    "'d:\\coding\\seq2seq\\chkpt\\seq2seq_enron_encode_24_decode_48_vocab_10_embedding_32_seq_3_10_batch_32_layers_1_enkeep_10_dekeep_10-9639')\n",
    "  #  saver.restore(session, 'd:\\coding\\seq2seq\\chkpt\\copy_task-19100')\n",
    "    \n",
    "    for chunk in range(n_chunks):\n",
    "        if chunk>0: break\n",
    "        ran_seq = generateRandomSeqBatchMajor(length_from=length_from, length_to=length_to,\n",
    "                       vocab_lower=2, vocab_upper=vocab_upper,\n",
    "                       batch_size=n_batch_size)\n",
    "        \n",
    "        #input_batch_data = ran_seq\n",
    "        input_batch_data = [[3,4,5,6,7,8, 9]]\n",
    "        #input_batch_data = df_all_dev['alpha_Pair_0_encoding'].values[:32]\n",
    "        \n",
    "        fd_inf = prepare_batch(input_batch_data)\n",
    "        feed_dict_inf = {encoder_inputs: fd_inf[0],\n",
    "                    encoder_inputs_length: fd_inf[1]}\n",
    "        inf_out = session.run([decoder_pred_decode, decoder_pred_decode_prob], feed_dict_inf)\n",
    "\n",
    "        #print (df_all_train.values[0][1], df_all_train.values[1][1])\n",
    "        #print (feed_dict_inf)\n",
    "        for i, (e_in, dt_inf) in enumerate(zip(feed_dict_inf[encoder_inputs], inf_out[0])):\n",
    "            #mean_metric.append([df_all_train.values[i][0], df_all_train.values[i][1], dt_inf])\n",
    "            print('    sample {}:'.format(i + 1))\n",
    "            print('    enc input                > {}'.format([inv_map[k] for k in e_in]))\n",
    "            print('    dec input                > {}'.format([inv_map[k] for k in df_all_dev['alpha_Pair_1_encoding'].values[i]]))\n",
    "            print('    dec train inference      > {}'.format([inv_map[k] for k in dt_inf]))\n",
    "            #print('    dec train inference prob > {}'.format([inf_out[1][j][i].max() for j in range((len(inf_out[1])))]))\n",
    "            \n",
    "            #if i>0: break\n",
    "        \n",
    "       # print ('Save Model')\n",
    "       # builder = tf.saved_model.builder.SavedModelBuilder('d:\\coding\\seq2seq\\model')\n",
    "       # builder.add_meta_graph_and_variables(session, ['serve'])\n",
    "\n",
    "        #builder.save()\n",
    "    ops = session.graph.get_operations()\n",
    "\n",
    "    feed_ops = [op for op in ops if op.type=='Placeholder']\n",
    "\n",
    "    print(feed_ops)\n",
    "        #if n_chunks >0: \n",
    "         #   break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
