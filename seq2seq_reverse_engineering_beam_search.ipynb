{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import create_model\n",
    "import pickle\n",
    "import time \n",
    "import copy\n",
    "import random\n",
    "import data_formatting\n",
    "\n",
    "\n",
    "def validate(train):\n",
    "    \n",
    "    if train == True:\n",
    "        \n",
    "        model = train_model\n",
    "        mode = 'TRAIN'\n",
    "        \n",
    "    else:\n",
    "        mode = 'DEV'\n",
    "        model = dev_model\n",
    "\n",
    "    encoder, decoder, predicted = session.run([model.encoder_inputs, model.decoder_targets, model.decoder_pred_train])            \n",
    "\n",
    "    print ('Current mode:%s' % mode)\n",
    "    for i, (e_in, dt_targ, dt_pred) in enumerate(zip( encoder, decoder, predicted)):\n",
    "\n",
    "        print('  sample {}:'.format(i + 1))\n",
    "        #print('    enc input           > {}'.format(e_in))\n",
    "        print('    enc input           > {}'.format(data_formatting.decodeSent(e_in, inv_map)))\n",
    "\n",
    "        #print('    dec input           > {}'.format(dt_targ))\n",
    "        print('    dec input           > {}'.format(data_formatting.decodeSent(dt_targ, inv_map)))\n",
    "\n",
    "        #print('    dec train predicted > {}'.format(dt_pred))\n",
    "        print('    dec train predicted > {}'.format(data_formatting.decodeSent(dt_pred, inv_map)))\n",
    "\n",
    "        if i >= 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = 'enron'\n",
    "\n",
    "#vocab_dict = pickle.load(open('../processed_data/word_dict_v02_enron_py35_seq_length_3_49_sample_4256_limited_vocab.pkl', 'rb'))\n",
    "#df_all = pd.read_pickle('../processed_data/processed_data_v02_enron_py35_seq_length_3_49_sample_4256_limited_vocab.pkl')\n",
    "\n",
    "dataset = 'twitter'\n",
    "\n",
    "#vocab_dict = pickle.load(open('../processed_data/word_dict_v02_twitter_py35_seq_length_3_25_sample_1764604_questions.pkl', 'rb'))\n",
    "#df_all = pd.read_pickle('../processed_data/processed_data_v02_twitter_py35_seq_length_3_25_sample_1764604_questions.pkl')\n",
    "\n",
    "#vocab_dict = pickle.load(open('../processed_data/word_dict_v02_twitter_py35_seq_length_4_15_sample_134241_full.pkl', 'rb'))\n",
    "#df_all = pd.read_pickle('../processed_data/processed_data_v02_twitter_py35_seq_length_4_15_sample_134241_full.pkl')\n",
    "\n",
    "vocab_dict = pickle.load(open('../processed_data/word_dict_v02_twitter_py35_seq_length_3_19_sample_21946_lem.pkl', 'rb'))\n",
    "df_all = pd.read_pickle('../processed_data/processed_data_v02_twitter_py35_seq_length_3_19_sample_21946_lem.pkl')\n",
    "\n",
    "#vocab_dict = pickle.load(open('../processed_data/word_dict_v02_twitter_py35_seq_length_3_25_sample_1901567_full.pkl', 'rb'))\n",
    "#df_all = pd.read_pickle('../processed_data/processed_data_v02_twitter_py35_seq_length_3_25_sample_1901567_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = data_formatting.createInvMap(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['alpha_Pair_0_encoding'] = df_all['alpha_Pair_0_tokens'].apply(lambda x: data_formatting.encodeSent(x, vocab_dict))\n",
    "df_all['alpha_Pair_1_encoding'] = df_all['alpha_Pair_1_tokens'].apply(lambda x: data_formatting.encodeSent(x, vocab_dict))\n",
    "df_all['Index'] = df_all.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_train = df_all.sample(frac=0.97, random_state=1)\n",
    "\n",
    "df_all_dev = df_all[df_all['Index'].isin(df_all_train['Index'].values) == False]\n",
    "\n",
    "df_all_test = df_all_dev.sample(frac=0.10, random_state=1)\n",
    "\n",
    "df_all_dev = df_all_dev[df_all_dev['Index'].isin(df_all_test['Index'].values) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17690 17159 478 53 17322\n"
     ]
    }
   ],
   "source": [
    "print (df_all.shape[0], df_all_train.shape[0],  df_all_dev.shape[0], df_all_test.shape[0], len(vocab_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_formatting.prepare_train_batch(df_all_train['alpha_Pair_0_encoding'].values, \n",
    "                                                    df_all_train['alpha_Pair_1_encoding'].values)\n",
    "\n",
    "dev_data = data_formatting.prepare_train_batch(df_all_dev['alpha_Pair_0_encoding'].values, \n",
    "                                                    df_all_dev['alpha_Pair_1_encoding'].values)\n",
    "\n",
    "test_data = data_formatting.prepare_train_batch(df_all_test['alpha_Pair_0_encoding'].values, \n",
    "                                                    df_all_test['alpha_Pair_1_encoding'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_params = {'n_cells':256, 'num_layers':2, 'embedding_size':512, \n",
    "          'vocab_size':len(vocab_dict) + 1, 'minibatch_size':64, 'n_threads':128,\n",
    "          'beam_width':10, 'limit_decode_steps': None,\n",
    "          'encoder_input_keep':1, 'decoder_input_keep':1,\n",
    "          'encoder_output_keep':1, 'decoder_output_keep':1,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_model_params = train_model_params\n",
    "dev_model_params['encoder_input_keep'] = 1\n",
    "dev_model_params['encoder_output_keep'] = 1\n",
    "dev_model_params['decoder_input_keep'] = 1\n",
    "dev_model_params['decoder_output_keep'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = { 'vocab_lower':3, 'vocab_upper':train_model_params['vocab_size']-1, \n",
    "                    'n_epochs':500000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enc input           > ['i', 'll', 'be', 'there', 'tomorrow', '<EOS>']\n",
    "#dec input           > ['last', 'game', 'of', 'the', 'regular', 'season', '<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import tensor_array_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "inf_model_1_params = {'n_cells':256, 'num_layers':2, 'embedding_size':512, \n",
    "          'vocab_size':len(vocab_dict) + 1, 'minibatch_size':64, 'n_threads':128,\n",
    "          'beam_width':1, 'limit_decode_steps': None,\n",
    "          'encoder_input_keep':1, 'decoder_input_keep':1,\n",
    "          'encoder_output_keep':1, 'decoder_output_keep':1,\n",
    "         }\n",
    "\n",
    "#inf_model_2_params = {'n_cells':2, 'num_layers':1, 'embedding_size':5, \n",
    "#            'vocab_size':7,\n",
    "#          'minibatch_size':64, 'n_threads':128,\n",
    "#          'beam_width':3, 'limit_decode_steps': None,\n",
    "#          'encoder_input_keep':1, 'decoder_input_keep':1,\n",
    "#          'encoder_output_keep':1, 'decoder_output_keep':1,\n",
    "#         }\n",
    "\n",
    "\n",
    "inf_model_2_params = {'n_cells':256, 'num_layers':2, 'embedding_size':512, \n",
    "          'vocab_size':len(vocab_dict) + 1,\n",
    "          'minibatch_size':64, 'n_threads':128,\n",
    "          'beam_width':7, 'limit_decode_steps': None,\n",
    "          'encoder_input_keep':1, 'decoder_input_keep':1,\n",
    "          'encoder_output_keep':1, 'decoder_output_keep':1,\n",
    "         }\n",
    "\n",
    "#with tf.variable_scope('training_model'):\n",
    "\n",
    "#    inf_model_1 = create_model.Model(inf_model_1_params, 'infer', None)\n",
    "    \n",
    "#with tf.variable_scope('training_model'):\n",
    "\n",
    "#    inf_model_2 = create_model.Model(inf_model_2_params, 'infer', None)   \n",
    "\n",
    "with tf.variable_scope('training_model'):\n",
    "\n",
    "    inf_train = create_model.Model(inf_model_2_params, 'debug', None)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#names_to_vars_inf = [[v.op.name, v] for v in tf.all_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-41-97ea65a51c80>:1: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-41-97ea65a51c80>:1: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    }
   ],
   "source": [
    "names_to_vars = [[v.op.name, v] for v in tf.all_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[i[0] for i in names_to_vars_inf if 'forget' in i[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training_model/rnn/multi_rnn_cell/cell_0/layer_norm_basic_lstm_cell/forget/gamma',\n",
       " 'training_model/rnn/multi_rnn_cell/cell_0/layer_norm_basic_lstm_cell/forget/beta',\n",
       " 'training_model/rnn/multi_rnn_cell/cell_1/layer_norm_basic_lstm_cell/forget/gamma',\n",
       " 'training_model/rnn/multi_rnn_cell/cell_1/layer_norm_basic_lstm_cell/forget/beta',\n",
       " 'training_model/decoder/multi_rnn_cell/cell_0/layer_norm_basic_lstm_cell/forget/gamma',\n",
       " 'training_model/decoder/multi_rnn_cell/cell_0/layer_norm_basic_lstm_cell/forget/beta',\n",
       " 'training_model/decoder/multi_rnn_cell/cell_1/Attention_Wrapper/layer_norm_basic_lstm_cell/forget/gamma',\n",
       " 'training_model/decoder/multi_rnn_cell/cell_1/Attention_Wrapper/layer_norm_basic_lstm_cell/forget/beta']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i[0] for i in names_to_vars if 'forget' in i[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ list(['and', 'it', 'be', 'the', 'one', 'going', 'for', 'one', 'stop', '<EOS>']),\n",
       "       list(['and', 'the', 'bus', 'omg', '<EOS>']),\n",
       "       list([8378, 13635, 14926, 7441, 6200, 9187, 434, 6200, 3425, 1]),\n",
       "       list([8378, 7441, 11483, 11526, 1]), 353842], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_dev.values[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['thank', 'you'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run(\n",
    "        inf_train.final_outputs, \n",
    "        #inf_model_2.decoder_pred_decode,\n",
    "        feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[['jodi', 'now', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>',\n",
       "         '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>',\n",
       "         '<EOS>'],\n",
       "        ['-0.6924710869789124', '-2.4161858558654785',\n",
       "         '-2.4267899990081787', '-2.4267899990081787',\n",
       "         '-2.4267899990081787', '-2.4267899990081787',\n",
       "         '-2.4267899990081787', '-2.4267899990081787',\n",
       "         '-2.4267899990081787', '-2.4267899990081787',\n",
       "         '-2.4267899990081787', '-2.4267899990081787',\n",
       "         '-2.4267899990081787', '-2.4267899990081787',\n",
       "         '-2.4267899990081787']],\n",
       "\n",
       "       [['congratulation', 'derek', '<EOS>', '<EOS>', '<EOS>', '<EOS>',\n",
       "         '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>',\n",
       "         '<EOS>', '<EOS>'],\n",
       "        ['-1.7698712348937988', '-2.4260313510894775',\n",
       "         '-2.969587802886963', '-2.969587802886963', '-2.969587802886963',\n",
       "         '-2.969587802886963', '-2.969587802886963', '-2.969587802886963',\n",
       "         '-2.969587802886963', '-2.969587802886963', '-2.969587802886963',\n",
       "         '-2.969587802886963', '-2.969587802886963', '-2.969587802886963',\n",
       "         '-2.969587802886963']],\n",
       "\n",
       "       [['homie', 'w', 'my', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>',\n",
       "         '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>'],\n",
       "        ['-3.1095023155212402', '-2.609581470489502', '-3.1298828125',\n",
       "         '-3.4522345066070557', '-3.4522345066070557',\n",
       "         '-3.4522345066070557', '-3.4522345066070557',\n",
       "         '-3.4522345066070557', '-3.4522345066070557',\n",
       "         '-3.4522345066070557', '-3.4522345066070557',\n",
       "         '-3.4522345066070557', '-3.4522345066070557',\n",
       "         '-3.4522345066070557', '-3.4522345066070557']],\n",
       "\n",
       "       [['thank', 'derek', '<EOS>', 'you', 'too', '<EOS>', '<EOS>',\n",
       "         '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>',\n",
       "         '<EOS>'],\n",
       "        ['-3.3625073432922363', '-2.6448006629943848',\n",
       "         '-3.4522345066070557', '-3.856332302093506',\n",
       "         '-3.9115090370178223', '-4.019842147827148', '-4.019842147827148',\n",
       "         '-4.019842147827148', '-4.019842147827148', '-4.019842147827148',\n",
       "         '-4.019842147827148', '-4.019842147827148', '-4.019842147827148',\n",
       "         '-4.019842147827148', '-4.019842147827148']],\n",
       "\n",
       "       [['during', 'from', 'your', 'now', 'i', '<EOS>', '<EOS>', '<EOS>',\n",
       "         '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>'],\n",
       "        ['-3.888383388519287', '-2.9021668434143066',\n",
       "         '-3.5440897941589355', '-4.248806953430176', '-4.273626804351807',\n",
       "         '-4.883602142333984', '-4.883602142333984', '-4.883602142333984',\n",
       "         '-4.883602142333984', '-4.883602142333984', '-4.883602142333984',\n",
       "         '-4.883602142333984', '-4.883602142333984', '-4.883602142333984',\n",
       "         '-4.883602142333984']],\n",
       "\n",
       "       [['thanks', 'to', 'leave', 'friend', 'too', 'you', 'proud', '<EOS>',\n",
       "         '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>'],\n",
       "        ['-3.9709057807922363', '-3.2598118782043457',\n",
       "         '-3.6619648933410645', '-4.348054885864258', '-4.883602142333984',\n",
       "         '-4.91726016998291', '-4.941133975982666', '-5.229187965393066',\n",
       "         '-5.229187965393066', '-5.229187965393066', '-5.229187965393066',\n",
       "         '-5.229187965393066', '-5.229187965393066', '-5.229187965393066',\n",
       "         '-5.229187965393066']],\n",
       "\n",
       "       [['wow', 'you', 'thank', 'amazing', 'hope', 'm', 'have', 'been',\n",
       "         'prince', 'great', 'video', 'you', 're', 'proud', '<EOS>'],\n",
       "        ['-3.9937825202941895', '-3.371202230453491',\n",
       "         '-3.8560545444488525', '-4.586474418640137', '-4.89382266998291',\n",
       "         '-4.930269241333008', '-4.995573043823242', '-5.884693145751953',\n",
       "         '-7.95020866394043', '-8.124147415161133', '-9.444478988647461',\n",
       "         '-10.273761749267578', '-10.336865425109863',\n",
       "         '-11.396442413330078', '-12.286056518554688']]],\n",
       "      dtype='<U32')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[data_formatting.decodeSent(q_0.predicted_ids.T[i][0], inv_map), \n",
    "          q_0.scores.T[i][0]]for i in range(inf_model_2_params['beam_width'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_formatting.decodeSent(q_0.beam_search_decoder_output.predicted_ids.T[0][0], inv_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = inf_model_2.inference_decoder._batch_size\n",
    "beam_width = inf_model_2.inference_decoder._beam_width\n",
    "end_token = inf_model_2.inference_decoder._end_token\n",
    "length_penalty_weight = inf_model_2.inference_decoder._length_penalty_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "name= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished, first_inputs, initial_state = inf_model_2.inference_decoder.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_model_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'training_model/embedding_lookup_1:0' shape=(?, 3, 512) dtype=float32>,\n",
       " BeamSearchDecoderState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'training_model/Reshape:0' shape=(?, 3, 256) dtype=float32>, h=<tf.Tensor 'training_model/Reshape_1:0' shape=(?, 3, 256) dtype=float32>), AttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'training_model/Reshape_2:0' shape=(?, 3, 256) dtype=float32>, h=<tf.Tensor 'training_model/Reshape_3:0' shape=(?, 3, 256) dtype=float32>), attention=<tf.Tensor 'training_model/Reshape_4:0' shape=(?, 3, 256) dtype=float32>, time=<tf.Tensor 'training_model/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'training_model/Reshape_5:0' shape=(?, 3, ?) dtype=float32>, alignment_history=())), log_probs=<tf.Tensor 'zeros_2:0' shape=(?, 3) dtype=float32>, finished=<tf.Tensor 'training_model/zeros:0' shape=(?, 3) dtype=bool>, lengths=<tf.Tensor 'zeros_3:0' shape=(?, 3) dtype=int32>))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_inputs, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_inputs\n",
    "#These are the word embeddings of the initial token (which for initialize should be from the encoder?)\n",
    "#Shape is [batch_size, embedding_size]\n",
    "#initial_state\n",
    "#This is the initial state of the decoder, a tuple consisting of \n",
    "#1. the LSTM cell and hiddens states\n",
    "#2. The attention states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When we pass time !=0 to inference decoder, even with initial step results we get non-zero parent ids\n",
    "\n",
    "#beam_search_output, beam_search_state, next_inputs, finished = \\\n",
    "#                                                inf_model_2.inference_decoder.step(time, first_inputs, initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run([first_inputs, initial_state, batch_size], \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 512)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with ops.name_scope(name, \"BeamSearchDecoderStep\", (time, first_inputs, initial_state)):\n",
    "    \n",
    "cell_state = initial_state.cell_state\n",
    "\n",
    "first_inputs = nest.map_structure(lambda inp: inf_model_2.inference_decoder._merge_batch_beams(inp, s=inp.shape[2:]), \n",
    "                                  first_inputs)\n",
    "\n",
    "cell_state = nest.map_structure(inf_model_2.inference_decoder._maybe_merge_batch_beams, cell_state, \n",
    "                                inf_model_2.inference_decoder._cell.state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run([first_inputs, initial_state], \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 512)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inf_model_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-ca19c07668c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#what does alignment history and attention states really mean? need to internalize...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcell_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_cell_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minf_model_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'inf_model_2' is not defined"
     ]
    }
   ],
   "source": [
    "##This goes through _cell first\n",
    "#The output of cell_outputs is also present in the next_cell_state, the cell_outputs is the attention\n",
    "\n",
    "#The cell_state includes, the cell and hidden states of the LSTM, the attention states, the alignment history, and timestep, \n",
    "#what does alignment history and attention states really mean? need to internalize...\n",
    "\n",
    "cell_outputs, next_cell_state = inf_model_2.inference_decoder._cell(first_inputs, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cell_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-4aee581a99ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcell_outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cell_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "cell_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run([cell_outputs, next_cell_state], \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user, input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user), len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 256)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions convert the shape of the cell outputs and states from [batch_size, beam_width, embedding_size]\n",
    "#to [batch_size*beam_width, embedding_size], essentially, reducing a dimensionality by flattening the nested structure\n",
    "#the map structure fn is used because cell state is a complex tuple structure, \n",
    "#outputs is a simpe structure (should be able to just call flatten?)\n",
    "cell_outputs = nest.map_structure(lambda out: inf_model_2.inference_decoder._split_batch_beams(out, out.shape[1:]), \n",
    "                                  cell_outputs)\n",
    "\n",
    "next_cell_state = nest.map_structure(inf_model_2.inference_decoder._maybe_split_batch_beams, next_cell_state, \n",
    "                                     inf_model_2.inference_decoder._cell.state_size)\n",
    "\n",
    "if inf_model_2.inference_decoder._output_layer is not None:\n",
    "    cell_outputs = inf_model_2.inference_decoder._output_layer(cell_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run([step_log_probs, cell_outputs, next_cell_state], \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 17323)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell_outputs_rnn_call, next_cell_state_rnn_call = inf_model_2.inference_decoder._cell.call(first_inputs, cell_state)\n",
    "\n",
    "#cell_outputs_rnn_call_call, next_cell_state_rnn_call_call = inf_model_2.inference_decoder._cell.call(cell_outputs_rnn_call, \n",
    "#                                                                                                     next_cell_state_rnn_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_batch_size = tensor_util.constant_value(batch_size)\n",
    "\n",
    "prediction_lengths = initial_state.lengths\n",
    "previously_finished = initial_state.finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token = data_formatting.EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_maybe(t):\n",
    "  if isinstance(t, tensor_array_ops.TensorArray):\n",
    "    raise TypeError(\n",
    "        \"TensorArray state is not supported by BeamSearchDecoder: %s\" % t.name)\n",
    "  if t.shape.ndims is None:\n",
    "    raise ValueError(\n",
    "        \"Expected tensor (%s) to have known rank, but ndims == None.\" % t)\n",
    "\n",
    "def _maybe_tensor_gather_helper(gather_indices, gather_from, batch_size,\n",
    "                                range_size, gather_shape):\n",
    "  \"\"\"Maybe applies _tensor_gather_helper.\n",
    "  This applies _tensor_gather_helper when the gather_from dims is at least as\n",
    "  big as the length of gather_shape. This is used in conjunction with nest so\n",
    "  that we don't apply _tensor_gather_helper to inapplicable values like scalars.\n",
    "  Args:\n",
    "    gather_indices: The tensor indices that we use to gather.\n",
    "    gather_from: The tensor that we are gathering from.\n",
    "    batch_size: The batch size.\n",
    "    range_size: The number of values in each range. Likely equal to beam_width.\n",
    "    gather_shape: What we should reshape gather_from to in order to preserve the\n",
    "      correct values. An example is when gather_from is the attention from an\n",
    "      AttentionWrapperState with shape [batch_size, beam_width, attention_size].\n",
    "      There, we want to preserve the attention_size elements, so gather_shape is\n",
    "      [batch_size * beam_width, -1]. Then, upon reshape, we still have the\n",
    "      attention_size as desired.\n",
    "  Returns:\n",
    "    output: Gathered tensor of shape tf.shape(gather_from)[:1+len(gather_shape)]\n",
    "      or the original tensor if its dimensions are too small.\n",
    "  \"\"\"\n",
    "  _check_maybe(gather_from)\n",
    "  if gather_from.shape.ndims >= len(gather_shape):\n",
    "    return _tensor_gather_helper(\n",
    "        gather_indices=gather_indices,\n",
    "        gather_from=gather_from,\n",
    "        batch_size=batch_size,\n",
    "        range_size=range_size,\n",
    "        gather_shape=gather_shape)\n",
    "  else:\n",
    "    return gather_from\n",
    "\n",
    "def _tensor_gather_helper(gather_indices, gather_from, batch_size,\n",
    "                          range_size, gather_shape):\n",
    "  \"\"\"Helper for gathering the right indices from the tensor.\n",
    "  This works by reshaping gather_from to gather_shape (e.g. [-1]) and then\n",
    "  gathering from that according to the gather_indices, which are offset by\n",
    "  the right amounts in order to preserve the batch order.\n",
    "  Args:\n",
    "    gather_indices: The tensor indices that we use to gather.\n",
    "    gather_from: The tensor that we are gathering from.\n",
    "    batch_size: The input batch size.\n",
    "    range_size: The number of values in each range. Likely equal to beam_width.\n",
    "    gather_shape: What we should reshape gather_from to in order to preserve the\n",
    "      correct values. An example is when gather_from is the attention from an\n",
    "      AttentionWrapperState with shape [batch_size, beam_width, attention_size].\n",
    "      There, we want to preserve the attention_size elements, so gather_shape is\n",
    "      [batch_size * beam_width, -1]. Then, upon reshape, we still have the\n",
    "      attention_size as desired.\n",
    "  Returns:\n",
    "    output: Gathered tensor of shape tf.shape(gather_from)[:1+len(gather_shape)]\n",
    "  \"\"\"\n",
    "  range_ = array_ops.expand_dims(math_ops.range(batch_size) * range_size, 1)\n",
    "  gather_indices = array_ops.reshape(gather_indices + range_, [-1])\n",
    "  output = array_ops.gather(\n",
    "      array_ops.reshape(gather_from, gather_shape), gather_indices)\n",
    "  final_shape = array_ops.shape(gather_from)[:1 + len(gather_shape)]\n",
    "  static_batch_size = tensor_util.constant_value(batch_size)\n",
    "  final_static_shape = (tensor_shape.TensorShape([static_batch_size])\n",
    "                        .concatenate(\n",
    "                            gather_from.shape[1:1 + len(gather_shape)]))\n",
    "  output = array_ops.reshape(output, final_shape)\n",
    "  output.set_shape(final_static_shape)\n",
    "  return output\n",
    "#Have to understand how this works!\n",
    "def _mask_probs(probs, eos_token, finished):\n",
    "  \"\"\"Masks log probabilities.\n",
    "  The result is that finished beams allocate all probability mass to eos and\n",
    "  unfinished beams remain unchanged.\n",
    "  Args:\n",
    "    probs: Log probabiltiies of shape `[batch_size, beam_width, vocab_size]`\n",
    "    eos_token: An int32 id corresponding to the EOS token to allocate\n",
    "      probability to.\n",
    "    finished: A boolean tensor of shape `[batch_size, beam_width]` that\n",
    "      specifies which\n",
    "      elements in the beam are finished already.\n",
    "  Returns:\n",
    "    A tensor of shape `[batch_size, beam_width, vocab_size]`, where unfinished\n",
    "    beams stay unchanged and finished beams are replaced with a tensor with all\n",
    "    probability on the EOS token.\n",
    "  \"\"\"\n",
    "  vocab_size = array_ops.shape(probs)[2]\n",
    "  finished_mask = array_ops.expand_dims(\n",
    "      math_ops.to_float(1. - math_ops.to_float(finished)), 2)\n",
    "  # These examples are not finished and we leave them\n",
    "  non_finished_examples = finished_mask * probs\n",
    "  # All finished examples are replaced with a vector that has all\n",
    "  # probability on EOS\n",
    "  finished_row = array_ops.one_hot(\n",
    "      eos_token,\n",
    "      vocab_size,\n",
    "      dtype=probs.dtype,\n",
    "      on_value=0.,\n",
    "      off_value=probs.dtype.min)\n",
    "  finished_examples = (1. - finished_mask) * finished_row\n",
    "  return finished_examples + non_finished_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = cell_outputs\n",
    "step_log_probs = nn_ops.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run([step_log_probs, logits, cell_outputs], \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3, 17323), (1, 3, 17323), (1, 3, 17323))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[0].shape, q_0[1].shape, q_0[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_22:0' shape=(?, 3, 17323) dtype=float32>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_23:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(step_log_probs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_log_probs = _mask_probs(step_log_probs, end_token, previously_finished)\n",
    "total_probs = array_ops.expand_dims(initial_state.log_probs, 2) + step_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = logits.shape[-1].value or array_ops.shape(logits)[-1]\n",
    "\n",
    "lengths_to_add = array_ops.one_hot(\n",
    "      indices=array_ops.tile(\n",
    "          array_ops.reshape(end_token, [1, 1]), [batch_size, beam_width]),\n",
    "      depth=vocab_size,\n",
    "      on_value=0,\n",
    "      off_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run([total_probs, array_ops.expand_dims(initial_state.log_probs, 2), step_log_probs], \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-33.68927383, -27.15158844, -27.81147385, ..., -16.75343132,\n",
       "       -25.89280701, -28.28565216], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[2][0][2] + q_0[1][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-33.68927383, -27.15158844, -27.81147385, ..., -16.75342751,\n",
       "        -25.8928051 , -28.28565216],\n",
       "       [-33.68927002, -27.15158844, -27.81147385, ..., -16.75342941,\n",
       "        -25.89280701, -28.28565216],\n",
       "       [-33.68927383, -27.15158844, -27.81147385, ..., -16.75343132,\n",
       "        -25.89280701, -28.28565216]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_mask = (1 - math_ops.to_int32(previously_finished))\n",
    "\n",
    "lengths_to_add = array_ops.expand_dims(add_mask, 2) * lengths_to_add\n",
    "\n",
    "new_prediction_lengths = (lengths_to_add + array_ops.expand_dims(prediction_lengths, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = total_probs\n",
    "\n",
    "scores_shape = array_ops.shape(scores)\n",
    "\n",
    "time = ops.convert_to_tensor(time, name=\"time\")\n",
    "\n",
    "scores_flat = control_flow_ops.cond(time > 0,\n",
    "      lambda: array_ops.reshape(scores, [batch_size, -1]), lambda: scores[:, 0])\n",
    "\n",
    "num_available_beam = control_flow_ops.cond(\n",
    "      time > 0, lambda: math_ops.reduce_prod(scores_shape[1:]),\n",
    "      lambda: math_ops.reduce_prod(scores_shape[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the next beams according to the specified successors function\n",
    "next_beam_size = math_ops.minimum(\n",
    "  ops.convert_to_tensor(beam_width, dtype=dtypes.int32, name=\"beam_width\"),\n",
    "  num_available_beam)\n",
    "\n",
    "next_beam_scores, word_indices = nn_ops.top_k(scores_flat, k=next_beam_size)\n",
    "next_beam_scores.set_shape([static_batch_size, beam_width])\n",
    "word_indices.set_shape([static_batch_size, beam_width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_beam_probs = _tensor_gather_helper(\n",
    "      gather_indices=word_indices,\n",
    "      gather_from=scores,\n",
    "      batch_size=batch_size,\n",
    "      range_size=beam_width * vocab_size,\n",
    "      gather_shape=[-1])\n",
    "\n",
    "#Here, word indices represents the positions of the flattened list of beams, which goes from 1 to beam_width*vocab_size\n",
    "#thus the word indices obtained are not the word indices of the 1 to N vocab but the flattened out list of logprobs from 1 to \n",
    "#beam_width*vocab_size\n",
    "next_word_ids = math_ops.to_int32(word_indices % vocab_size)\n",
    "#The beam_ids represent which beams these word indices (and thereby log prob values belong to), as\n",
    "#we have word indices extracted out of 1 to beam_width*vocab_size\n",
    "next_beam_ids = math_ops.to_int32(word_indices / vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run([next_word_ids, next_beam_ids, scores, scores_flat, next_beam_scores, next_beam_probs,\n",
    "                    word_indices, next_beam_size], \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-33.68927002, -27.15158653, -27.81147194, ..., -16.75342941,\n",
       "        -25.89280319, -28.28565025],\n",
       "       [-33.68927002, -27.15158653, -27.81147194, ..., -16.75342941,\n",
       "        -25.8928051 , -28.28565025],\n",
       "       [-33.68927002, -27.15158844, -27.81147385, ..., -16.75342941,\n",
       "        -25.89280701, -28.28565216]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.60707283, -2.0549202 , -0.85561192], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[3][0][np.argsort(q_0[3][0])[-3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-33.68927002, -27.15158653, -27.81147194, ..., -16.75342941,\n",
       "        -25.89280319, -28.28565025], dtype=float32),\n",
       " array([], dtype=float32),\n",
       " array([], dtype=float32))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[3][0][:vocab_size], q_0[3][0][vocab_size:vocab_size*2], q_0[3][0][vocab_size*2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.85561192, -2.0549202 , -2.60707283]], dtype=float32),\n",
       " array([[-0.85561192, -2.0549202 , -2.60707283]], dtype=float32))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[4], q_0[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 4400, 10359, 11678]]), array([[0, 0, 0]]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[0], q_0[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_finished = _tensor_gather_helper(\n",
    "      gather_indices=next_beam_ids,\n",
    "      gather_from=previously_finished,\n",
    "      batch_size=batch_size,\n",
    "      range_size=beam_width,\n",
    "      gather_shape=[-1])\n",
    "\n",
    "next_finished = math_ops.logical_or(previously_finished, math_ops.equal(next_word_ids, end_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_to_add = math_ops.to_int32(math_ops.not_equal(next_word_ids, end_token))\n",
    "lengths_to_add = (1 - math_ops.to_int32(next_finished)) * lengths_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "input_user = data_formatting.encodeSent(['i', 'll', 'be', 'there', 'tomorrow', '<EOS>'], vocab_dict)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #tf.set_random_seed(1)\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_0 = sess.run([previously_finished, next_finished, lengths_to_add], \n",
    "                 feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[False, False, False]], dtype=bool),\n",
       " array([[False, False, False]], dtype=bool),\n",
       " array([[1, 1, 1]])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_prediction_len = _tensor_gather_helper(\n",
    "      gather_indices=next_beam_ids,\n",
    "      gather_from=initial_state.lengths,\n",
    "      batch_size=batch_size,\n",
    "      range_size=beam_width,\n",
    "      gather_shape=[-1])\n",
    "\n",
    "next_prediction_len += lengths_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_1 = sess.run([next_prediction_len],\n",
    "                     feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1, 1, 1]])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_cell_state = nest.map_structure(\n",
    "      lambda gather_from: _maybe_tensor_gather_helper(\n",
    "          gather_indices=next_beam_ids,\n",
    "          gather_from=gather_from,\n",
    "          batch_size=batch_size,\n",
    "          range_size=beam_width,\n",
    "          gather_shape=[batch_size * beam_width, -1]),\n",
    "      next_cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_1 = sess.run([next_cell_state],\n",
    "                     feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the returns, next_state and output to the step function, but how exactly do we transpose this back and push it\n",
    "#into cell? Moreover what is teh mechanics that determines when the beams become different? I.E. are no longer identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state = tf.contrib.seq2seq.BeamSearchDecoderState(\n",
    "      cell_state=next_cell_state,\n",
    "      log_probs=next_beam_probs,\n",
    "      lengths=next_prediction_len,\n",
    "      finished=next_finished)\n",
    "\n",
    "output = tf.contrib.seq2seq.BeamSearchDecoderOutput(\n",
    "      scores=next_beam_scores,\n",
    "      predicted_ids=next_word_ids,\n",
    "      parent_ids=next_beam_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "   \n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_1 = sess.run([next_state, output, next_beam_scores],\n",
    "                     feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_state = next_state\n",
    "beam_search_output = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished = beam_search_state.finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = beam_search_output.predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_inputs = control_flow_ops.cond(math_ops.reduce_all(finished), \n",
    "                                    lambda: inf_model_2.inference_decoder._start_inputs,\n",
    "                                    lambda: inf_model_2.inference_decoder._embedding_fn(sample_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../chkpt/seq2seq_twitter_testing-5501\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver.restore(sess, '../chkpt/seq2seq_twitter_testing-5501')\n",
    "    q_1 = sess.run([next_inputs, beam_search_state],\n",
    "                     feed_dict={'training_model/encoder_inputs:0':[input_user], \n",
    "                         'training_model/encoder_inputs_length:0':[len(input_user)]}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3), (3, 512))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[0].shape, q_1[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 512)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3), (1, 3))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[1][3].shape, q_1[1][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3), (1, 3))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[1][2].shape, q_1[1][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3), (1, 3))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_0[1][1].shape, q_1[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q_0[1][0]), len(q_1[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next_inputs\n",
    "#These are the chosen word ids which we have performed the embedding look up for\n",
    "first_inputs = next_inputs[0]#The [0] is for batch size, should not need it\n",
    "\n",
    "#next_states\n",
    "#these are the next states of the decoder network which pass back through the process\n",
    "initial_state = next_state\n",
    "\n",
    "finished = finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "time =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
