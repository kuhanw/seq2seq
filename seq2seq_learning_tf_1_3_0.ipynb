{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kuhan Wang 17-09-15\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import helpers\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "def make_train_inputs(input_seq, target_seq):\n",
    "    inputs_, inputs_length_ = helpers.batch(input_seq)\n",
    "    targets_, targets_length_ = helpers.batch(target_seq)\n",
    "    \n",
    "    return {\n",
    "        encoder_inputs: inputs_,\n",
    "        encoder_inputs_length: inputs_length_,\n",
    "        decoder_targets: targets_,\n",
    "        decoder_targets_length: targets_length_,\n",
    "    }\n",
    "\n",
    "def make_inference_inputs(input_seq):\n",
    "    inputs_, inputs_length_ = helpers.batch(input_seq)\n",
    "\n",
    "    return {\n",
    "        encoder_inputs: inputs_,\n",
    "        encoder_inputs_length: inputs_length_,\n",
    "    }\n",
    "\n",
    "def encodeSent(sent):\n",
    "\n",
    "    if type(sent) == str: sent = sent.split(' ')\n",
    "    \n",
    "    return [vocab_dict[word] if word in vocab_dict else 2 for word in sent]\n",
    "\n",
    "def decodeSent(sent):\n",
    "    return [inv_map[i] for i in sent]\n",
    "\n",
    "def prepare_batch(seqs_x, maxlen=None):\n",
    "    # seqs_x: a list of sentences\n",
    "    lengths_x = [len(s) for s in seqs_x]\n",
    "    if maxlen is not None:\n",
    "        new_seqs_x = []\n",
    "        new_lengths_x = []\n",
    "        for l_x, s_x in zip(lengths_x, seqs_x):\n",
    "            if l_x <= maxlen:\n",
    "                new_seqs_x.append(s_x)\n",
    "                new_lengths_x.append(l_x)\n",
    "        lengths_x = new_lengths_x\n",
    "        seqs_x = new_seqs_x\n",
    "        \n",
    "        if len(lengths_x) < 1:\n",
    "            return None, None\n",
    "\n",
    "    batch_size = len(seqs_x)\n",
    "    x_lengths = np.array(lengths_x)\n",
    "    maxlen_x = np.max(x_lengths)\n",
    "    x = np.ones((batch_size, maxlen_x)).astype('int32') * PAD\n",
    "    for idx, s_x in enumerate(seqs_x):\n",
    "        x[idx, :lengths_x[idx]] = s_x\n",
    "    return x, x_lengths\n",
    "\n",
    "def prepare_train_batch(seqs_x, seqs_y, maxlen=None):\n",
    "    # seqs_x, seqs_y: a list of sentences\n",
    "    lengths_x = [len(s) for s in seqs_x]\n",
    "    lengths_y = [len(s) for s in seqs_y]\n",
    "\n",
    "    if maxlen is not None:\n",
    "        new_seqs_x = []\n",
    "        new_seqs_y = []\n",
    "        new_lengths_x = []\n",
    "        new_lengths_y = []\n",
    "        for l_x, s_x, l_y, s_y in zip(lengths_x, seqs_x, lengths_y, seqs_y):\n",
    "            if l_x <= maxlen and l_y <= maxlen:\n",
    "                new_seqs_x.append(s_x)\n",
    "                new_lengths_x.append(l_x)\n",
    "                new_seqs_y.append(s_y)\n",
    "                new_lengths_y.append(l_y)\n",
    "        lengths_x = new_lengths_x\n",
    "        seqs_x = new_seqs_x\n",
    "        lengths_y = new_lengths_y\n",
    "        seqs_y = new_seqs_y\n",
    "\n",
    "        if len(lengths_x) < 1 or len(lengths_y) < 1:\n",
    "            return None, None, None, None\n",
    "\n",
    "    batch_size = len(seqs_x)\n",
    "    \n",
    "    x_lengths = np.array(lengths_x)\n",
    "    y_lengths = np.array(lengths_y)\n",
    "\n",
    "    maxlen_x = np.max(x_lengths)\n",
    "    maxlen_y = np.max(y_lengths)\n",
    "\n",
    "    x = np.ones((batch_size, maxlen_x)).astype('int32') * PAD\n",
    "    y = np.ones((batch_size, maxlen_y)).astype('int32') * PAD\n",
    "    \n",
    "    for idx, [s_x, s_y] in enumerate(zip(seqs_x, seqs_y)):\n",
    "        x[idx, :lengths_x[idx]] = s_x\n",
    "        y[idx, :lengths_y[idx]] = s_y\n",
    "    return x, x_lengths, y, y_lengths\n",
    "\n",
    "def generateRandomSeqBatchMajor(length_from, length_to, vocab_lower, vocab_upper, batch_size):\n",
    "    return [\n",
    "            [random.randint(vocab_lower, vocab_upper-2) for digit in range(random.randint(length_from, length_to))] + [1]\n",
    "                for batch in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra vocabulary symbols\n",
    "_GO = '_GO'\n",
    "EOS = '_EOS' # also function as PAD\n",
    "UNK = '_UNK'\n",
    "\n",
    "extra_tokens = [_GO, EOS, UNK]\n",
    "\n",
    "start_token = extra_tokens.index(_GO)\t# start_token = 0\n",
    "end_token = extra_tokens.index(EOS)\t# end_token = 1\n",
    "unk_token = extra_tokens.index(UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all = pd.read_pickle('processed_data_v01_EN-DE_py35_seq_length_5_15_sample_540659_limited_vocab.pkl')\n",
    "#vocab_dict = pickle.load(open('word_dict_v01_EN-DE_py35_seq_length_5_15_sample_540659_limited_vocab.pkl', 'rb'))\n",
    "\n",
    "#df_all = pd.read_pickle('../processed_data/processed_data_v02_enron_py35_seq_length_3_49_sample_4256_limited_vocab.pkl')\n",
    "#vocab_dict = pickle.load(open('../processed_data/word_dict_v02_enron_py35_seq_length_3_49_sample_4256_limited_vocab.pkl', 'rb'))\n",
    "\n",
    "dataset = 'twitter'\n",
    "\n",
    "df_all = pd.read_pickle('../processed_data/processed_data_v02_twitter_py35_seq_length_3_19_sample_22028_lem.pkl')\n",
    "vocab_dict = pickle.load(open('../processed_data/word_dict_v02_twitter_py35_seq_length_3_19_sample_22028_lem.pkl', 'rb'))\n",
    "\n",
    "#Encode sequences\n",
    "#df_all['alpha_Pair_1_encoding'] =  df_all['alpha_Pair_1_tokens'].apply(numericEncode)\n",
    "#df_all['alpha_Pair_0_encoding'] = df_all['alpha_Pair_0_tokens'].apply(numericEncode)\n",
    "\n",
    "df_all['alpha_Pair_1_encoding'] =  df_all['alpha_Pair_1_tokens'].apply(encodeSent)\n",
    "df_all['alpha_Pair_0_encoding'] = df_all['alpha_Pair_0_tokens'].apply(encodeSent)\n",
    "\n",
    "df_all['Index'] = df_all.index.values\n",
    "\n",
    "df_all_train = df_all.sample(frac=0.90, random_state=0)\n",
    "\n",
    "df_all_dev = df_all[df_all['Index'].isin(df_all_train['Index'].values) == False]\n",
    "\n",
    "df_all_test = df_all_dev.sample(frac=0.10, random_state=0)\n",
    "\n",
    "df_all_dev = df_all_dev[df_all_dev['Index'].isin(df_all_test['Index'].values) == False]\n",
    "\n",
    "inv_map = {v: k for k, v in vocab_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_encoded_text = df_all_dev['alpha_Pair_0_encoding'].values\n",
    "dev_decoded_text = df_all_dev['alpha_Pair_1_encoding'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded_text = df_all_test['alpha_Pair_0_encoding'].values\n",
    "test_decoded_text = df_all_test['alpha_Pair_1_encoding'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15898, 5), (1590, 5), (177, 5), 17181)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_train.shape, df_all_dev.shape, df_all_test.shape, len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496.8125"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "PAD = 0\n",
    "EOS = 1\n",
    "UNK = 2\n",
    "vocab_size = 210 + 1\n",
    "#vocab_size = 30\n",
    "input_embedding_size = 512\n",
    "\n",
    "length_from = 3\n",
    "length_to = 19\n",
    "vocab_lower = 0\n",
    "vocab_upper = 20\n",
    "n_batch_size = 32\n",
    "\n",
    "batches_in_epoch = df_all_train.shape[0]/n_batch_size\n",
    "#batches_in_epoch=100\n",
    "n_cells = 64\n",
    "num_layers = 2\n",
    "\n",
    "n_epochs = 1000\n",
    "n_beam_width = 5\n",
    "\n",
    "encoder_output_keep = 1\n",
    "decoder_output_keep = 1\n",
    "\n",
    "batches_in_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_batches = helpers.random_sequences(length_from=length_from, length_to=length_to,\n",
    "                                       vocab_lower=vocab_lower, vocab_upper=vocab_size,\n",
    "                                       batch_size=n_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create handles for encoder and decoders\n",
    "encoder_inputs = tf.placeholder(\n",
    "            shape=(None, None),\n",
    "            dtype=tf.int32,\n",
    "            name='encoder_inputs',\n",
    "        )\n",
    "\n",
    "encoder_inputs_length = tf.placeholder(\n",
    "            shape=(None,),\n",
    "            dtype=tf.int32,\n",
    "            name='encoder_inputs_length',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required for training, not required for testing\n",
    "decoder_targets = tf.placeholder(\n",
    "            shape=(None, None),\n",
    "            dtype=tf.int32,\n",
    "            name='decoder_targets'\n",
    "        )\n",
    "\n",
    "decoder_targets_length = tf.placeholder(\n",
    "            shape=(None,),\n",
    "            dtype=tf.int32,\n",
    "            name='decoder_targets_length',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = tf.shape(encoder_inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Make EOS and PAD matrices to concatenate with targets\n",
    "EOS_SLICE = tf.ones([batch_size, 1], dtype=tf.int32) * EOS\n",
    "PAD_SLICE = tf.ones([batch_size, 1], dtype=tf.int32) * PAD\n",
    "\n",
    "#Adding EOS to the beginning of the decoder targets\n",
    "decoder_train_inputs = tf.concat([EOS_SLICE, decoder_targets], axis=1, name='decoder_train_inputs_concat')\n",
    "#[1,10], [10, 16]\n",
    "decoder_train_length = decoder_targets_length + 1\n",
    "\n",
    "decoder_train_targets = tf.concat([decoder_targets, PAD_SLICE], axis=1, name='decoder_train_targets')\n",
    "\n",
    "max_decoder_length = tf.reduce_max(decoder_train_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create word embeddings\n",
    "sqrt3 = math.sqrt(3)\n",
    "initializer = tf.random_uniform_initializer(-sqrt3, sqrt3)\n",
    "\n",
    "#Randomly initialize a embedding vector for each term in the vocabulary\n",
    "embedding_matrix = tf.get_variable(name=\"embedding_matrix\", shape=[vocab_size, input_embedding_size],\n",
    "                                   initializer=initializer, \n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "#Map each input unit to a column in the embedding matrix\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embedding_matrix, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_train_inputs_embedded = tf.nn.embedding_lookup(embedding_matrix, decoder_train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_layer = Dense(n_cells, name='input_projection')\n",
    "\n",
    "# Output projection layer to convert cell_outputs to logits\n",
    "output_layer = Dense(vocab_size, name='output_projection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a bi-directional encoder, encoding the forward and backward states\n",
    "#The core abstraction is in tf.nn.bidirectional_dynamic_rnn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_cell_fw = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(n_cells), input_keep_prob=1, \n",
    "#                                             output_keep_prob=encoder_output_keep)\n",
    "\n",
    "encoder_cell_list = []\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(n_cells, state_is_tuple=True), input_keep_prob=1, \n",
    "                                            output_keep_prob=encoder_output_keep)\n",
    "    encoder_cell_list.append(cell)\n",
    "\n",
    "encoder_cell =  tf.contrib.rnn.MultiRNNCell(encoder_cell_list)\n",
    "#encoder_cell_fw = tf.contrib.rnn.MultiRNNCell([encoder_cell_fw for _ in range(num_layers)])\n",
    "\n",
    "#encoder_cell_bw = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(n_cells), input_keep_prob=1, \n",
    "#                                             output_keep_prob=encoder_output_keep)\n",
    "\n",
    "#encoder_cell = tf.contrib.rnn.LSTMCell(n_cells)\n",
    "#encoder_cell_bw = tf.contrib.rnn.MultiRNNCell([encoder_cell_bw for _ in range(num_layers)])\n",
    "\n",
    "#(\n",
    "# (encoder_fw_outputs, encoder_bw_outputs),\n",
    "# (encoder_fw_state, encoder_bw_state)\n",
    "#) = (\n",
    "#     tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell, \n",
    "#                                     cell_bw=encoder_cell,\n",
    "#                                     inputs=encoder_inputs_embedded,\n",
    "#                                     sequence_length=encoder_inputs_length,\n",
    "#                                     time_major=True, dtype=tf.float32)\n",
    "#    )\n",
    "\n",
    "#concatenate backward and forward outputs\n",
    "#encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "#encoder_state_c = tf.concat((encoder_fw_state.c, encoder_bw_state.c), 1, name='bidirectional_concat_c')\n",
    "#encoder_state_h = tf.concat((encoder_fw_state.h, encoder_bw_state.h), 1, name='bidirectional_concat_h')\n",
    "\n",
    "#encoder_state = tf.contrib.rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "#encoder_state = []\n",
    "\n",
    "#for i in range(num_layers):\n",
    "    \n",
    "#    encoder_state_c = tf.concat((encoder_fw_state[i].c, encoder_bw_state[i].c), 1, name='bidirectional_concat_c')\n",
    "#    encoder_state_h = tf.concat((encoder_fw_state[i].h, encoder_bw_state[i].h), 1, name='bidirectional_concat_h')\n",
    "    \n",
    "#    current_encoder_state = tf.contrib.rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "#    encoder_state.append(current_encoder_state)\n",
    "\n",
    "#encoder_state = tuple(encoder_state)\n",
    "\n",
    "encoder_outputs, encoder_last_state = tf.nn.dynamic_rnn(\n",
    "        cell=encoder_cell, inputs=encoder_inputs_embedded,\n",
    "        sequence_length=encoder_inputs_length, dtype=tf.float32,\n",
    "        time_major=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width= n_beam_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_last_state = tf.contrib.seq2seq.tile_batch(encoder_last_state, beam_width)\n",
    "\n",
    "encoder_outputs = tf.contrib.seq2seq.tile_batch(encoder_outputs, multiplier=beam_width)\n",
    "\n",
    "encoder_inputs_length = tf.contrib.seq2seq.tile_batch(encoder_inputs_length, multiplier=beam_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                        num_units=n_cells, \n",
    "                        memory=encoder_outputs, \n",
    "                        memory_sequence_length=encoder_inputs_length) \n",
    "\n",
    "decoder_cell_list = []\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(n_cells, state_is_tuple=True), input_keep_prob=1, \n",
    "                                                output_keep_prob=decoder_output_keep)\n",
    "decoder_cell_list.append(cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last layer of decoders is wrapped in attention\n",
    "decoder_cell_list[-1] = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                             cell=decoder_cell_list[-1],\n",
    "                             attention_mechanism=attention_mechanism,\n",
    "                             attention_layer_size=n_cells,\n",
    "                             #  cell_input_fn=attn_decoder_input_fn,\n",
    "                             initial_cell_state=encoder_last_state[-1],                   \n",
    "                             alignment_history=False,\n",
    "                             name='Attention_Wrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = [state for state in encoder_last_state]\n",
    "\n",
    "initial_state[-1] = decoder_cell_list[-1].zero_state(batch_size=batch_size*beam_width, dtype=tf.float32)\n",
    "\n",
    "decoder_initial_state = tuple(initial_state)\n",
    "\n",
    "decoder_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list)\n",
    "\n",
    "start_tokens = tf.ones([batch_size*beam_width,], tf.int32) * EOS\n",
    "\n",
    "end_token = EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The two structures don't have the same number of elements.\n\nFirst structure (7 elements): (LSTMStateTuple(c=<tf.Tensor 'tile_batch/Reshape:0' shape=(?, 64) dtype=float32>, h=<tf.Tensor 'tile_batch/Reshape_1:0' shape=(?, 64) dtype=float32>), AttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'AttentionWrapperZeroState_2/checked_cell_state:0' shape=(?, 64) dtype=float32>, h=<tf.Tensor 'AttentionWrapperZeroState_2/checked_cell_state_1:0' shape=(?, 64) dtype=float32>), attention=<tf.Tensor 'AttentionWrapperZeroState_2/zeros_1:0' shape=(?, 64) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState_2/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState_2/zeros_2:0' shape=(?, ?) dtype=float32>, alignment_history=()))\n\nSecond structure (5 elements): (AttentionWrapperState(cell_state=LSTMStateTuple(c=64, h=64), attention=64, time=TensorShape([]), alignments=<tf.Tensor 'BahdanauAttention/strided_slice_4:0' shape=() dtype=int32>, alignment_history=()),)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-620417a628dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                                \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_initial_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                                                \u001b[0mbeam_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                                                                output_layer=output_layer)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\beam_search_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, cell, embedding, start_tokens, end_token, initial_state, beam_width, output_layer, length_penalty_weight)\u001b[0m\n\u001b[0;32m    191\u001b[0m     self._initial_cell_state = nest.map_structure(\n\u001b[0;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_split_batch_beams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         initial_state, self._cell.state_size)\n\u001b[0m\u001b[0;32m    194\u001b[0m     self._start_tokens = array_ops.tile(\n\u001b[0;32m    195\u001b[0m         array_ops.expand_dims(self._start_tokens, 1), [1, self._beam_width])\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **check_types_dict)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m     \u001b[0massert_same_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m   \u001b[0mflat_structure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[1;34m(nest1, nest2, check_types)\u001b[0m\n\u001b[0;32m    197\u001b[0m                      \u001b[1;34m\"elements.\\n\\nFirst structure (%i elements): %s\\n\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                      \u001b[1;34m\"Second structure (%i elements): %s\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                      % (len_nest1, nest1, len_nest2, nest2))\n\u001b[0m\u001b[0;32m    200\u001b[0m   \u001b[0m_recursive_assert_same_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnest2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The two structures don't have the same number of elements.\n\nFirst structure (7 elements): (LSTMStateTuple(c=<tf.Tensor 'tile_batch/Reshape:0' shape=(?, 64) dtype=float32>, h=<tf.Tensor 'tile_batch/Reshape_1:0' shape=(?, 64) dtype=float32>), AttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'AttentionWrapperZeroState_2/checked_cell_state:0' shape=(?, 64) dtype=float32>, h=<tf.Tensor 'AttentionWrapperZeroState_2/checked_cell_state_1:0' shape=(?, 64) dtype=float32>), attention=<tf.Tensor 'AttentionWrapperZeroState_2/zeros_1:0' shape=(?, 64) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState_2/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState_2/zeros_2:0' shape=(?, ?) dtype=float32>, alignment_history=()))\n\nSecond structure (5 elements): (AttentionWrapperState(cell_state=LSTMStateTuple(c=64, h=64), attention=64, time=TensorShape([]), alignments=<tf.Tensor 'BahdanauAttention/strided_slice_4:0' shape=() dtype=int32>, alignment_history=()),)"
     ]
    }
   ],
   "source": [
    "inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell=decoder_cell,\n",
    "                                                               embedding=embedding_matrix,\n",
    "                                                               start_tokens=start_tokens,\n",
    "                                                               end_token=end_token,\n",
    "                                                               initial_state=decoder_initial_state,\n",
    "                                                               beam_width=beam_width,\n",
    "                                                               output_layer=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_decode_step = tf.reduce_max(self.encoder_inputs_length) + 3\n",
    "\n",
    "(self.decoder_outputs_decode, self.decoder_last_state_decode,\n",
    "     self.decoder_outputs_length_decode) = (tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder=inference_decoder,\n",
    "        output_time_major=False,\n",
    "        #impute_finished=True,\t# error occurs --why?\n",
    "        maximum_iterations=max_decode_step))\n",
    "\n",
    "else:\n",
    "\n",
    "self.decoder_pred_decode = self.decoder_outputs_decode.predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_last_state_beam = tf.contrib.framework.nest.map_structure(\n",
    "#        lambda s: tf.contrib.seq2seq.tile_batch(s, n_beam_width), encoder_last_state)\n",
    "\n",
    "encoder_last_state = tf.contrib.seq2seq.tile_batch(encoder_last_state, n_beam_width)\n",
    "\n",
    "encoder_outputs = tf.contrib.seq2seq.tile_batch(encoder_outputs, multiplier=n_beam_width)\n",
    "\n",
    "encoder_inputs_length = tf.contrib.seq2seq.tile_batch(encoder_inputs_length, multiplier=n_beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_input_feeding = True\n",
    "\n",
    "def attn_decoder_input_fn(inputs, attention):\n",
    "    if not attn_input_feeding:\n",
    "        return inputs\n",
    "\n",
    "    # Essential when use_residual=True\n",
    "    _input_layer = Dense(n_cells, dtype=tf.float32,\n",
    "                         name='attn_input_feeding')\n",
    "    return _input_layer(array_ops.concat([inputs, attention], -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "    num_units=n_cells, \n",
    "    memory=encoder_outputs, \n",
    "    memory_sequence_length=encoder_inputs_length) \n",
    "\n",
    "decoder_cell_list = []\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(n_cells, state_is_tuple=True), input_keep_prob=1, \n",
    "                                            output_keep_prob=decoder_output_keep)\n",
    "    decoder_cell_list.append(cell)\n",
    "\n",
    "#decoder_initial_state = encoder_last_state\n",
    "\n",
    "decoder_cell_list[-1] = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            cell=decoder_cell_list[-1],\n",
    "            attention_mechanism=attention_mechanism,\n",
    "            attention_layer_size=n_cells,\n",
    "          #  cell_input_fn=attn_decoder_input_fn,\n",
    "            #initial_cell_state=encoder_last_state[-1],\n",
    "            initial_cell_state=encoder_last_state[-1],                   \n",
    "            alignment_history=False,\n",
    "            name='Attention_Wrapper')\n",
    "\n",
    "initial_state = [state for state in encoder_last_state]\n",
    "\n",
    "initial_state[-1] = decoder_cell_list[-1].zero_state(batch_size=batch_size*n_beam_width, dtype=tf.float32)\n",
    "\n",
    "decoder_initial_state = tuple(initial_state)\n",
    "\n",
    "decoder_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More efficient to do the projection on the batch-time-concatenated tensor\n",
    "# logits_train: [batch_size, max_time_step + 1, num_decoder_symbols]\n",
    "# self.decoder_logits_train = output_layer(self.decoder_outputs_train.rnn_output)\n",
    "decoder_logits_train = tf.identity(decoder_outputs_train.rnn_output) \n",
    "\n",
    "# Use argmax to extract decoder symbols to emit\n",
    "decoder_pred_train = tf.argmax(decoder_logits_train, axis=-1, name='decoder_pred_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.ones([batch_size*n_beam_width,], tf.int32) * EOS\n",
    "end_token = EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Helper to feed inputs for greedy decoding: uses the argmax of the output\n",
    "decoding_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(start_tokens=start_tokens,\n",
    "                                                end_token=end_token,\n",
    "                                                embedding=embedding_matrix)\n",
    "\n",
    "# Basic decoder performs greedy decoding at each time step\n",
    "inference_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell,\n",
    "                                         helper=decoding_helper,\n",
    "                                         initial_state=decoder_initial_state,\n",
    "                                         output_layer=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_decode_step = tf.reduce_max(encoder_inputs_length) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell=decoder_cell,\n",
    "                                                               embedding=embedding_matrix,\n",
    "                                                               start_tokens=start_tokens,\n",
    "                                                               end_token=end_token,\n",
    "                                                               initial_state=decoder_initial_state,\n",
    "                                                               beam_width=n_beam_width,\n",
    "                                                               output_layer=output_layer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(decoder_outputs_decode, decoder_last_state_decode,\n",
    "         decoder_outputs_length_decode) = (tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder=inference_decoder,\n",
    "            output_time_major=False,\n",
    "            #impute_finished=True,\t# error occurs --why?\n",
    "            maximum_iterations=max_decode_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_pred_decode = tf.argmax(decoder_outputs_decode.rnn_output, axis=-1, name='decoder_pred_decode')\n",
    "\n",
    "decoder_pred_decode_prob = tf.nn.softmax(decoder_outputs_decode.rnn_output, name='decoder_pred_decode_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder_pred_decode_prob = tf.nn.softmax(decoder_outputs_decode.rnn_output, name='decoder_pred_decode_prob')\n",
    "#decoder_pred_decode_prob = decoder_outputs_decode.beam_search_decoder_output.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks: masking for valid and padded time steps, [batch_size, max_time_step + 1]\n",
    "masks = tf.sequence_mask(lengths=decoder_train_length, \n",
    "                         maxlen=max_decoder_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "# Computes per word average cross-entropy over a batch\n",
    "# Internally calls 'nn_ops.sparse_softmax_cross_entropy_with_logits' by default\n",
    "loss = tf.contrib.seq2seq.sequence_loss(logits=decoder_logits_train, \n",
    "                                  targets=decoder_train_targets,\n",
    "                                  weights=masks,\n",
    "                                  average_across_timesteps=True,\n",
    "                                  average_across_batch=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=decoder_train_targets,\n",
    "    logits=decoder_logits_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = df_all_train['alpha_Pair_0_encoding'].values\n",
    "decoded_text = df_all_train['alpha_Pair_1_encoding'].values\n",
    "text_index = df_all_train['Index'].values\n",
    "\n",
    "input_batches = ([encoded_text[block_idx*n_batch_size:(block_idx+1)*n_batch_size], \n",
    "         decoded_text[block_idx*n_batch_size:(block_idx+1)*n_batch_size], \n",
    "                 text_index[block_idx*n_batch_size:(block_idx+1)*n_batch_size]]\\\n",
    "            for block_idx in range(len(encoded_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_seq = generateRandomSeqBatchMajor(length_from=length_from, length_to=length_to,\n",
    "                                       vocab_lower=2, vocab_upper=vocab_upper,\n",
    "                                       batch_size=n_batch_size)\n",
    "input_batch_data = ran_seq\n",
    "target_batch_data = input_batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17182"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fd = make_train_inputs(input_batch_data, target_batch_data)\n",
    "fd = prepare_train_batch([[4, 5, 7, 2, 5, 1], [4, 5, 7, 4, 5, 1]], [[4, 5, 7, 2, 30, 1],[4, 5, 7, 9, 5, 1]])\n",
    "feed_dict = {encoder_inputs: fd[0],\n",
    "        encoder_inputs_length: fd[1],\n",
    "        decoder_targets: fd[2],\n",
    "        decoder_targets_length: fd[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    for i in range(1):\n",
    "        print (i)\n",
    "        #epoch_batches = next(input_batches)\n",
    "\n",
    "        #input_batch_data = epoch_batches[0]\n",
    "        #target_batch_data = epoch_batches[1]\n",
    "        #batch_data_index = epoch_batches[2]\n",
    "        #print ([inv_map[i] for i in input_batch_data[0]])\n",
    "       #fd = prepare_train_batch(input_batch_data, target_batch_data)\n",
    "        \n",
    "        #feed_dict = {encoder_inputs: fd[0],\n",
    "        #encoder_inputs_length: fd[1],\n",
    "        #decoder_targets: fd[2],\n",
    "        #decoder_targets_length: fd[3]}\n",
    "        \n",
    "        t = session.run([loss], feed_dict)\n",
    "        #y  = session.run([encoder_outputs_original, encoder_outputs], feed_dict)\n",
    "        #if t[1] !=t[1]: \n",
    "        #    print (loss)\n",
    "         #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.001\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, \\\n",
    "                                           n_epochs*int(batches_in_epoch), 0.0001, staircase=False)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_task = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 1\n",
      "learning rate 0.000999982\n",
      "epoch 0\n",
      "batch 0\n",
      "training minibatch loss: 9.750490188598633\n",
      "  sample 1:\n",
      "    enc input           > i think they gon na go straight to the it s the th anniversary of the iphone <EOS>\n",
      "    dec input           > i m not sure if i want the or to wait for s <EOS>\n",
      "    dec train predicted > shiite oauth maintained cooperation the aah stanley melt executed heroism cauliflower mad cauliflower <EOS> <EOS>\n",
      "dev minibatch loss: 9.745782852172852\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > just think in month he will be off the stage thank you lord <EOS>\n",
      "    DEV dec input           > on stage on the importance of investing in africa amp inspiring entrepreneur in africa u <EOS>\n",
      "    DEV dec train predicted > flicker motion timely timely mitochondrion distressing distressing <EOS> <EOS> <EOS> editorial bandwidth motion reputable sophomore canvas fuzzy\n",
      "    DEV dec train infer > nasa snitching montana dominion flip shilled <EOS> mad self comedy sparkling abuse saying abuse abuse abuse thinking clubhouse thinking retrospective until butter\n",
      "global_step: 51\n",
      "learning rate 0.000999053\n",
      "epoch 0\n",
      "batch 50\n",
      "training minibatch loss: 6.615805149078369\n",
      "  sample 1:\n",
      "    enc input           > lol i m just messing with you <EOS>\n",
      "    dec input           > that s my gov name though <EOS>\n",
      "    dec train predicted > i <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.588064193725586\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > gold plated fixture installers <EOS>\n",
      "    DEV dec input           > i think marble installers is the nail in the coffin tonight <EOS>\n",
      "    DEV dec train predicted > <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 101\n",
      "learning rate 0.000998126\n",
      "epoch 0\n",
      "batch 100\n",
      "training minibatch loss: 6.238506317138672\n",
      "  sample 1:\n",
      "    enc input           > sorry that wa me <EOS>\n",
      "    dec input           > why are these scene kid making whale noise a they walk down main street ? ? ? ? <EOS>\n",
      "    dec train predicted > i i <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.1225738525390625\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you were supposed to be on electronic blackout tonight what happened ? <EOS>\n",
      "    DEV dec input           > turn out he actually didnt prepare pity these thing arent thirty minute longer <EOS>\n",
      "    DEV dec train predicted > i i i <EOS> i <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i i i i i the <EOS>\n",
      "global_step: 151\n",
      "learning rate 0.0009972\n",
      "epoch 0\n",
      "batch 150\n",
      "training minibatch loss: 6.157619476318359\n",
      "  sample 1:\n",
      "    enc input           > even though i am much younger than she is <EOS>\n",
      "    dec input           > is the kris jenner of wine critic ? <EOS>\n",
      "    dec train predicted > i i i <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.346717834472656\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > but that s how he roll being the dumbass <EOS>\n",
      "    DEV dec input           > damn right we all know it except him <EOS>\n",
      "    DEV dec train predicted > i i i <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i i <EOS> <EOS>\n",
      "global_step: 201\n",
      "learning rate 0.000996275\n",
      "epoch 0\n",
      "batch 200\n",
      "training minibatch loss: 6.24723482131958\n",
      "  sample 1:\n",
      "    enc input           > ta da another retweet amp true coolness abounds thanks so much grady visit u for some goody <EOS>\n",
      "    dec input           > grazie gracias danka merci spoceba amp thanks for the retweet grady we have music book <EOS>\n",
      "    dec train predicted > i i the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.99099588394165\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you should close your account because they got busted for fraud <EOS>\n",
      "    DEV dec input           > because i bank with them haha <EOS>\n",
      "    DEV dec train predicted > i i the <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i i the <EOS>\n",
      "global_step: 251\n",
      "learning rate 0.00099535\n",
      "epoch 0\n",
      "batch 250\n",
      "training minibatch loss: 5.919284343719482\n",
      "  sample 1:\n",
      "    enc input           > which mean absolutely nothing <EOS>\n",
      "    dec input           > better start for team usa in the second <EOS>\n",
      "    dec train predicted > i the the the <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.344244480133057\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > ah i see kind of reminds me of those people mover at the airport fun to ride <EOS>\n",
      "    DEV dec input           > yi they are guided with guide wheel and the bus just flow along like going through rail <EOS>\n",
      "    DEV dec train predicted > i you the the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i you the the <EOS>\n",
      "global_step: 301\n",
      "learning rate 0.000994426\n",
      "epoch 0\n",
      "batch 300\n",
      "training minibatch loss: 5.950214385986328\n",
      "  sample 1:\n",
      "    enc input           > you were robbed it s ok denis we all know how fantastic is let s dance <EOS>\n",
      "    dec input           > what i do when the forget to invite me to the party <EOS>\n",
      "    dec train predicted > i s the to to the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.109096527099609\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > pandering clinton she s a joke <EOS>\n",
      "    DEV dec input           > this is a man who ha called woman pig and liar <EOS>\n",
      "    DEV dec train predicted > i i the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i s the <EOS> <EOS>\n",
      "global_step: 351\n",
      "learning rate 0.000993503\n",
      "epoch 0\n",
      "batch 350\n",
      "training minibatch loss: 5.964794635772705\n",
      "  sample 1:\n",
      "    enc input           > not that hard <EOS>\n",
      "    dec input           > how would you suggest i try to verify that claim <EOS>\n",
      "    dec train predicted > i t the the <EOS> i <EOS> the <EOS> i <EOS>\n",
      "dev minibatch loss: 5.782113552093506\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the power of the moon is real very no denying it <EOS>\n",
      "    DEV dec input           > when strange behavior become worse and you look at the calendar and this in two day <EOS>\n",
      "    DEV dec train predicted > i you the the i <EOS> the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i i the the the <EOS>\n",
      "global_step: 401\n",
      "learning rate 0.000992581\n",
      "epoch 0\n",
      "batch 400\n",
      "training minibatch loss: 6.278687953948975\n",
      "  sample 1:\n",
      "    enc input           > so many weird sunburn forthcoming <EOS>\n",
      "    dec input           > it is glorious day sunscreen those bare butt my folsom bound friend <EOS>\n",
      "    dec train predicted > i t the the <EOS> <EOS> <EOS> <EOS> <EOS> the <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.791227340698242\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > this is one for black jesus <EOS>\n",
      "    DEV dec input           > yep and somehow that wa made okay come on jesus <EOS>\n",
      "    DEV dec train predicted > i i the the i the <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m the the the <EOS> <EOS> <EOS>\n",
      "global_step: 451\n",
      "learning rate 0.00099166\n",
      "epoch 0\n",
      "batch 450\n",
      "training minibatch loss: 5.612447738647461\n",
      "  sample 1:\n",
      "    enc input           > i came in with legging before honestly but idk fam lp is starting to be more extra <EOS>\n",
      "    dec input           > they re getting worse with the checkup though <EOS>\n",
      "    dec train predicted > i m the the <EOS> the the <EOS> <EOS>\n",
      "dev minibatch loss: 5.9131550788879395\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i just buy myself stuff from the guy section in nike <EOS>\n",
      "    DEV dec input           > i just want a boyfriend so i can wear his sweat shirt and sweat pant <EOS>\n",
      "    DEV dec train predicted > i m the s the <EOS> the s s the <EOS> <EOS> <EOS> the <EOS> <EOS>\n",
      "    DEV dec train infer > i s the the the the the the <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 497\n",
      "learning rate 0.000990814\n",
      "epoch 1\n",
      "batch 0\n",
      "training minibatch loss: 5.468383312225342\n",
      "  sample 1:\n",
      "    enc input           > wow u r right on brother <EOS>\n",
      "    dec input           > the church and homosexuality <EOS>\n",
      "    dec train predicted > i the the the <EOS>\n",
      "dev minibatch loss: 6.1278395652771\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you just said it <EOS>\n",
      "    DEV dec input           > so i guess half of american are idiot then ? or a hillary like to call them deplorables <EOS>\n",
      "    DEV dec train predicted > i you m i <EOS> the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m the the the the <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 547\n",
      "learning rate 0.000989894\n",
      "epoch 1\n",
      "batch 50\n",
      "training minibatch loss: 5.771731376647949\n",
      "  sample 1:\n",
      "    enc input           > thank you <EOS>\n",
      "    dec input           > happy birthday johnny <EOS>\n",
      "    dec train predicted > i the to <EOS>\n",
      "dev minibatch loss: 5.8202128410339355\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > no fool you are not that lit <EOS>\n",
      "    DEV dec input           > my birthday is the day project x come out coincidence ? ? <EOS>\n",
      "    DEV dec train predicted > i the to the a <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m the a the a <EOS> <EOS> <EOS>\n",
      "global_step: 597\n",
      "learning rate 0.000988975\n",
      "epoch 1\n",
      "batch 100\n",
      "training minibatch loss: 6.056326866149902\n",
      "  sample 1:\n",
      "    enc input           > we are doomed then ugh goodnight <EOS>\n",
      "    dec input           > total resistance came from the people the offered no resistance and the court went along <EOS>\n",
      "    dec train predicted > i the <EOS> <EOS> the the s the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.7560505867004395\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > thanks for the rt <EOS>\n",
      "    DEV dec input           > rt reach for the sky <EOS>\n",
      "    DEV dec train predicted > i is the the the <EOS>\n",
      "    DEV dec train infer > i m the the the the <EOS> <EOS> <EOS>\n",
      "global_step: 647\n",
      "learning rate 0.000988058\n",
      "epoch 1\n",
      "batch 150\n",
      "training minibatch loss: 5.4761810302734375\n",
      "  sample 1:\n",
      "    enc input           > worse playoff race ever <EOS>\n",
      "    dec input           > and we re already in the giant bullpen <EOS>\n",
      "    dec train predicted > i is m the <EOS> the <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.90289831161499\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > but then navas and san i mean whatever pep s got this <EOS>\n",
      "    DEV dec input           > lm probably <EOS>\n",
      "    DEV dec train predicted > i is is\n",
      "    DEV dec train infer > i m the a a a a the the <EOS>\n",
      "global_step: 697\n",
      "learning rate 0.000987141\n",
      "epoch 1\n",
      "batch 200\n",
      "training minibatch loss: 5.578559875488281\n",
      "  sample 1:\n",
      "    enc input           > i feared that people would think san fran but it wa too good a name to pas up <EOS>\n",
      "    dec input           > i ll allow it <EOS>\n",
      "    dec train predicted > i m be a t\n",
      "dev minibatch loss: 5.864553928375244\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > wake county and nc can t afford not investing in transit infrastructure <EOS>\n",
      "    DEV dec input           > more use sale tax amp fee that ask more from folk of many who can not afford <EOS>\n",
      "    DEV dec train predicted > i a a to <EOS> a <EOS> t <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> t <EOS> <EOS>\n",
      "    DEV dec train infer > i m a a a a <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 747\n",
      "learning rate 0.000986225\n",
      "epoch 1\n",
      "batch 250\n",
      "training minibatch loss: 5.439533710479736\n",
      "  sample 1:\n",
      "    enc input           > good luck baby <EOS>\n",
      "    dec input           > im literally at the dmv by myself amp i have to uber to school <EOS>\n",
      "    dec train predicted > i the the the time <EOS> the <EOS> <EOS> have the be <EOS> the <EOS>\n",
      "dev minibatch loss: 5.8386335372924805\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > is it like deleting confidential email ? <EOS>\n",
      "    DEV dec input           > yet they turn video off or destroy tape to no consequence ? <EOS>\n",
      "    DEV dec train predicted > the is s to <EOS> <EOS> <EOS> <EOS> <EOS> the <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i s a the time <EOS> <EOS>\n",
      "global_step: 797\n",
      "learning rate 0.000985309\n",
      "epoch 1\n",
      "batch 300\n",
      "training minibatch loss: 5.3469767570495605\n",
      "  sample 1:\n",
      "    enc input           > better get terry s sword <EOS>\n",
      "    dec input           > did i hear zombie <EOS>\n",
      "    dec train predicted > i you m you <EOS>\n",
      "dev minibatch loss: 5.6470513343811035\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > haha i don t wan na school you kid on the old cod man <EOS>\n",
      "    DEV dec input           > shitttt snag that hoe back mw is coming up next <EOS>\n",
      "    DEV dec train predicted > i is a s <EOS> <EOS> <EOS> the <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a new new <EOS> <EOS> <EOS>\n",
      "global_step: 847\n",
      "learning rate 0.000984395\n",
      "epoch 1\n",
      "batch 350\n",
      "training minibatch loss: 5.457339286804199\n",
      "  sample 1:\n",
      "    enc input           > no way he doesn t watch porn he rape kid <EOS>\n",
      "    dec input           > trump s recommended video section on pornhub is definitely hentai <EOS>\n",
      "    dec train predicted > i s the <EOS> <EOS> <EOS> the <EOS> the <EOS> <EOS>\n",
      "dev minibatch loss: 6.035749912261963\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i lied i read back to and then read <EOS>\n",
      "    DEV dec input           > you never fail to make me laugh i love that xx <EOS>\n",
      "    DEV dec train predicted > i s have a be a <EOS> <EOS> m a <EOS> <EOS>\n",
      "    DEV dec train infer > i m a birthday <EOS> <EOS> <EOS>\n",
      "global_step: 897\n",
      "learning rate 0.000983481\n",
      "epoch 1\n",
      "batch 400\n",
      "training minibatch loss: 5.5836358070373535\n",
      "  sample 1:\n",
      "    enc input           > no the form is if i don t file them someone else will which is a different idea <EOS>\n",
      "    dec input           > the people want frivolous lawsuit so i have to file them or someone else will <EOS>\n",
      "    dec train predicted > i birthday is to <EOS> <EOS> the m a be a <EOS> <EOS> <EOS> <EOS> be\n",
      "dev minibatch loss: 6.076010704040527\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > club banger <EOS>\n",
      "    DEV dec input           > how about love love love ? lol <EOS>\n",
      "    DEV dec train predicted > i s the <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a best <EOS> <EOS> <EOS>\n",
      "global_step: 947\n",
      "learning rate 0.000982569\n",
      "epoch 1\n",
      "batch 450\n",
      "training minibatch loss: 5.460690498352051\n",
      "  sample 1:\n",
      "    enc input           > nyc nyc happy to be home for a sec come over p <EOS>\n",
      "    dec input           > i miss you too where in the world are you now ? <EOS>\n",
      "    dec train predicted > i have the a <EOS> t <EOS> new <EOS> the <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.048986911773682\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > ugh it s taking forever to find one <EOS>\n",
      "    DEV dec input           > lmfao omg so me but we all have one i just haven t found mine either <EOS>\n",
      "    DEV dec train predicted > i you to i to i m a the <EOS> m be t be the <EOS> <EOS>\n",
      "    DEV dec train infer > i m a birthday <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 993\n",
      "learning rate 0.00098173\n",
      "epoch 2\n",
      "batch 0\n",
      "training minibatch loss: 5.441450595855713\n",
      "  sample 1:\n",
      "    enc input           > dont give any seed to kiefer <EOS>\n",
      "    dec input           > you can say it i don t mind <EOS>\n",
      "    dec train predicted > i s t to <EOS> have t have the\n",
      "dev minibatch loss: 5.629530429840088\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i applaud them <EOS>\n",
      "    DEV dec input           > i personally know hillary supporter who said they may no longer vote for her <EOS>\n",
      "    DEV dec train predicted > i m to you s <EOS> have you have be a <EOS> <EOS> the <EOS>\n",
      "    DEV dec train infer > i m a birthday <EOS> <EOS> <EOS>\n",
      "global_step: 1043\n",
      "learning rate 0.000980819\n",
      "epoch 2\n",
      "batch 50\n",
      "training minibatch loss: 5.1731486320495605\n",
      "  sample 1:\n",
      "    enc input           > and manson is responsible for fewer death <EOS>\n",
      "    dec input           > so would i he s sane compared to her <EOS>\n",
      "    dec train predicted > i i have have t a to <EOS> be <EOS>\n",
      "dev minibatch loss: 5.6197099685668945\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we should go to this <EOS>\n",
      "    DEV dec input           > come party with amp paper this thursday in bk rsvp here <EOS>\n",
      "    DEV dec train predicted > i is to the <EOS> <EOS> <EOS> <EOS> the <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m have a best <EOS> <EOS> <EOS>\n",
      "global_step: 1093\n",
      "learning rate 0.000979909\n",
      "epoch 2\n",
      "batch 100\n",
      "training minibatch loss: 5.525425910949707\n",
      "  sample 1:\n",
      "    enc input           > it s a pho craving morning here too <EOS>\n",
      "    dec input           > mama need pho and she need it now <EOS>\n",
      "    dec train predicted > i is i <EOS> the s the <EOS> <EOS>\n",
      "dev minibatch loss: 5.574739933013916\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > in i want a tank on governor island and horse too <EOS>\n",
      "    DEV dec input           > sat sept <EOS>\n",
      "    DEV dec train predicted > i is the\n",
      "    DEV dec train infer > i don t be a best of the best <EOS>\n",
      "global_step: 1143\n",
      "learning rate 0.000978999\n",
      "epoch 2\n",
      "batch 150\n",
      "training minibatch loss: 5.488512992858887\n",
      "  sample 1:\n",
      "    enc input           > thank you carly for endorsing trump <EOS>\n",
      "    dec input           > after more than a year of bashing trump ted cruz supporter carly fiorina is finally aboard the <EOS>\n",
      "    dec train predicted > i you is the good <EOS> the <EOS> <EOS> the <EOS> <EOS> <EOS> <EOS> the <EOS> <EOS> day\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 5.579395771026611\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i don t need surgery lol <EOS>\n",
      "    DEV dec input           > hope your surgery go well dude lt <EOS>\n",
      "    DEV dec train predicted > i you good is <EOS> <EOS> i <EOS>\n",
      "    DEV dec train infer > i m a na na be a good <EOS> <EOS>\n",
      "global_step: 1193\n",
      "learning rate 0.000978091\n",
      "epoch 2\n",
      "batch 200\n",
      "training minibatch loss: 5.258182525634766\n",
      "  sample 1:\n",
      "    enc input           > not when ur taking a dump lol <EOS>\n",
      "    dec input           > someone s always watching everything you do <EOS>\n",
      "    dec train predicted > i just a a the <EOS> <EOS> you\n",
      "dev minibatch loss: 5.776278018951416\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i would like to know where my country is going still thank you <EOS>\n",
      "    DEV dec input           > why do nothing but bitch about the candidate when you can t even vote ? <EOS>\n",
      "    DEV dec train predicted > i i you i i <EOS> the time <EOS> you re t be be <EOS> <EOS>\n",
      "    DEV dec train infer > i m been a good <EOS> <EOS> <EOS>\n",
      "global_step: 1243\n",
      "learning rate 0.000977183\n",
      "epoch 2\n",
      "batch 250\n",
      "training minibatch loss: 5.472010135650635\n",
      "  sample 1:\n",
      "    enc input           > how do you create that illusion ? lol <EOS>\n",
      "    dec input           > if you don t know how to twerk you have to create the illusion that you do know <EOS>\n",
      "    dec train predicted > i you re t be the <EOS> the <EOS> s the be the great <EOS> <EOS> s <EOS> <EOS>\n",
      "dev minibatch loss: 6.03585958480835\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > confirmed train is halted at th and oakland <EOS>\n",
      "    DEV dec input           > stuck on at th st possible person on track nothing from so far <EOS>\n",
      "    DEV dec train predicted > i is the the <EOS> <EOS> <EOS> <EOS> the <EOS> <EOS> the <EOS> <EOS>\n",
      "    DEV dec train infer > i m a na na na be a new day <EOS> <EOS>\n",
      "global_step: 1293\n",
      "learning rate 0.000976276\n",
      "epoch 2\n",
      "batch 300\n",
      "training minibatch loss: 5.501029014587402\n",
      "  sample 1:\n",
      "    enc input           > why is it when i see a mushroom i see rupert murdoch s face ? <EOS>\n",
      "    dec input           > one can make rotten an entire package one can poison a harvest of mushroom and ag <EOS>\n",
      "    dec train predicted > i of t the a best thing <EOS> <EOS> i <EOS> a <EOS> the <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.451013565063477\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the towel is in the bidet <EOS>\n",
      "    DEV dec input           > no that s the sink that european use to clean their as w every private bathroom ha one <EOS>\n",
      "    DEV dec train predicted > the the s a new <EOS> s <EOS> <EOS> be a other <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a a new new thing i m a a a new <EOS> <EOS>\n",
      "global_step: 1343\n",
      "learning rate 0.00097537\n",
      "epoch 2\n",
      "batch 350\n",
      "training minibatch loss: 5.224712371826172\n",
      "  sample 1:\n",
      "    enc input           > carrying the gospel with me because <EOS>\n",
      "    dec input           > i m on my way to the representing at the digital medium lounge <EOS>\n",
      "    dec train predicted > i m a the time <EOS> the time <EOS> the time <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.602107048034668\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > yeah that s hilarious you should go on the road with that act <EOS>\n",
      "    DEV dec input           > she s probably taking a nap don t expect you ll hear back <EOS>\n",
      "    DEV dec train predicted > i s a a a good <EOS> t be to have be to <EOS>\n",
      "    DEV dec train infer > i m a na have a na have a birthday <EOS> <EOS>\n",
      "global_step: 1393\n",
      "learning rate 0.000974465\n",
      "epoch 2\n",
      "batch 400\n",
      "training minibatch loss: 5.654566764831543\n",
      "  sample 1:\n",
      "    enc input           > actually a v good idea <EOS>\n",
      "    dec input           > throw a bunch of your other stuff away <EOS>\n",
      "    dec train predicted > i the the of the new <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.688155174255371\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > to the rioter in charlotte north carolina each to vote for <EOS>\n",
      "    DEV dec input           > charlotte police shooting victim wa armed with a gun not a book chief say <EOS>\n",
      "    DEV dec train predicted > i is is in of the <EOS> the the <EOS> the new <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > the new debate of the new new york <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1443\n",
      "learning rate 0.00097356\n",
      "epoch 2\n",
      "batch 450\n",
      "training minibatch loss: 5.466518878936768\n",
      "  sample 1:\n",
      "    enc input           > great to see you happy my friend be safe and take care your friend dan <EOS>\n",
      "    dec input           > congrats on the new location well deserved <EOS>\n",
      "    dec train predicted > i is the day day <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.634668827056885\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > have a nice night <EOS>\n",
      "    DEV dec input           > lol where wa all this outrage the last year ? <EOS>\n",
      "    DEV dec train predicted > i the you a <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a na be a a new day <EOS> <EOS>\n",
      "global_step: 1489\n",
      "learning rate 0.000972729\n",
      "epoch 3\n",
      "batch 0\n",
      "training minibatch loss: 5.209282875061035\n",
      "  sample 1:\n",
      "    enc input           > oh yes you will i m back next week and again the week after <EOS>\n",
      "    dec input           > i m never going to actually meet you in person am i ? <EOS>\n",
      "    dec train predicted > i m a be to be be a <EOS> the <EOS> <EOS> m <EOS>\n",
      "dev minibatch loss: 5.695618629455566\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > yes i m confident quietly confident bush endorsement will be interesting <EOS>\n",
      "    DEV dec input           > hey still confident ? can t imagine how you guy would feel losing to trump <EOS>\n",
      "    DEV dec train predicted > i i the i <EOS> t be to i can <EOS> be a <EOS> be <EOS>\n",
      "    DEV dec train infer > i m a a new york <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1539\n",
      "learning rate 0.000971826\n",
      "epoch 3\n",
      "batch 50\n",
      "training minibatch loss: 5.297121524810791\n",
      "  sample 1:\n",
      "    enc input           > if not what in the world is he referencing ? <EOS>\n",
      "    dec input           > did trump just say drug are a very very big factor in the protest in charlotte ? <EOS>\n",
      "    dec train predicted > trump you is not to is the debate debate debate <EOS> <EOS> the medium <EOS> the <EOS> <EOS>\n",
      "dev minibatch loss: 5.69624662399292\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > like obama did to thousand of coal miner <EOS>\n",
      "    DEV dec input           > don t forget how many small business he s bankrupted by contracting them then refusing to pay <EOS>\n",
      "    DEV dec train predicted > trump t think the trump in in <EOS> s a to the <EOS> <EOS> <EOS> <EOS> be <EOS>\n",
      "    DEV dec train infer > is the new york of the york <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1589\n",
      "learning rate 0.000970925\n",
      "epoch 3\n",
      "batch 100\n",
      "training minibatch loss: 5.298087120056152\n",
      "  sample 1:\n",
      "    enc input           > can we start w s immigrant ancestor please ? <EOS>\n",
      "    dec input           > well there it is <EOS>\n",
      "    dec train predicted > i you s s the\n",
      "dev minibatch loss: 5.812027454376221\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > nope it s your dad and we ve seen proof more than once <EOS>\n",
      "    DEV dec input           > eric trump bill clinton is maybe the worst sexist that ever lived <EOS>\n",
      "    DEV dec train predicted > i the is is <EOS> a you time <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a na have a na a na be a lot <EOS> <EOS> <EOS>\n",
      "global_step: 1639\n",
      "learning rate 0.000970024\n",
      "epoch 3\n",
      "batch 150\n",
      "training minibatch loss: 5.156021595001221\n",
      "  sample 1:\n",
      "    enc input           > martin please calm down <EOS>\n",
      "    dec input           > young thug there are no gender progressive a fuck pac probably thought there were only gender sexist pig <EOS>\n",
      "    dec train predicted > i s the s a new a <EOS> new <EOS> <EOS> s you s a <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.5035552978515625\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > did just call these post game interview ? <EOS>\n",
      "    DEV dec input           > the first u presidential debate is now underway watch live on <EOS>\n",
      "    DEV dec train predicted > i debate time <EOS> <EOS> <EOS> a the <EOS> <EOS> <EOS> the\n",
      "    DEV dec train infer > i just just just just a a new york <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1689\n",
      "learning rate 0.000969123\n",
      "epoch 3\n",
      "batch 200\n",
      "training minibatch loss: 5.22153377532959\n",
      "  sample 1:\n",
      "    enc input           > of course dude a glorified real estate agent beating out career politician <EOS>\n",
      "    dec input           > honestly think trump whole presidential race will lead to a new tv series for him win or lose <EOS>\n",
      "    dec train predicted > the the the s new new <EOS> be to be the <EOS> <EOS> <EOS> the <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 6.048330783843994\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > how adorable have a wonderful day <EOS>\n",
      "    DEV dec input           > seeing mommy properly for the first time beautiful <EOS>\n",
      "    DEV dec train predicted > i you a <EOS> the new day <EOS> <EOS>\n",
      "    DEV dec train infer > i m a a new day <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1739\n",
      "learning rate 0.000968224\n",
      "epoch 3\n",
      "batch 250\n",
      "training minibatch loss: 5.481926918029785\n",
      "  sample 1:\n",
      "    enc input           > they also got divorced when i wa but you can t prove those thing are directly linked <EOS>\n",
      "    dec input           > who even are you guy ? <EOS>\n",
      "    dec train predicted > i is is a <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.710312366485596\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > why would you talk trash on your gfs school lol bye <EOS>\n",
      "    DEV dec input           > supporting my girl on jv dawgggg screw the varsity <EOS>\n",
      "    DEV dec train predicted > i the time <EOS> the <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a na have a good <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1789\n",
      "learning rate 0.000967325\n",
      "epoch 3\n",
      "batch 300\n",
      "training minibatch loss: 5.387485027313232\n",
      "  sample 1:\n",
      "    enc input           > bet this come up in the next debate <EOS>\n",
      "    dec input           > and finger includes petitioning to make sure homeless veteran couldn t sit near trump tower <EOS>\n",
      "    dec train predicted > i i is the <EOS> be the <EOS> <EOS> <EOS> t be <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.616493225097656\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > whaaaat i thought you were the one who gave out weeb certification <EOS>\n",
      "    DEV dec input           > me studying to become a certified weeb <EOS>\n",
      "    DEV dec train predicted > i is <EOS> be a same <EOS> <EOS>\n",
      "    DEV dec train infer > i m a good thing <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1839\n",
      "learning rate 0.000966428\n",
      "epoch 3\n",
      "batch 350\n",
      "training minibatch loss: 5.421796798706055\n",
      "  sample 1:\n",
      "    enc input           > started my day with a laugh and smile thank you ricky <EOS>\n",
      "    dec input           > old tree going for a walk <EOS>\n",
      "    dec train predicted > i the is to the <EOS> <EOS>\n",
      "dev minibatch loss: 5.76628303527832\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > well done you will have to teach your niece <EOS>\n",
      "    DEV dec input           > since this afternoon <EOS>\n",
      "    DEV dec train predicted > i the is <EOS>\n",
      "    DEV dec train infer > i m not a good <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1889\n",
      "learning rate 0.000965531\n",
      "epoch 3\n",
      "batch 400\n",
      "training minibatch loss: 5.306218147277832\n",
      "  sample 1:\n",
      "    enc input           > ok lol thanks <EOS>\n",
      "    dec input           > happy birthday craig <EOS>\n",
      "    dec train predicted > happy birthday of <EOS>\n",
      "dev minibatch loss: 5.794609546661377\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > your so pretty <EOS>\n",
      "    DEV dec input           > trip on love but never fall <EOS>\n",
      "    DEV dec train predicted > i to the <EOS> i have to\n",
      "    DEV dec train infer > i m a birthday <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 1939\n",
      "learning rate 0.000964635\n",
      "epoch 3\n",
      "batch 450\n",
      "training minibatch loss: 5.288959503173828\n",
      "  sample 1:\n",
      "    enc input           > i think she s cute <EOS>\n",
      "    dec input           > this is so wrong why do y all do her like this <EOS>\n",
      "    dec train predicted > i s a a <EOS> i you you be <EOS> <EOS> i is\n",
      "dev minibatch loss: 5.6535325050354\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > so true lmfao <EOS>\n",
      "    DEV dec input           > the sign a joanne the scammer gifs aquarius <EOS>\n",
      "    DEV dec train predicted > i birthday of the <EOS> day <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a good day <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 1985\n",
      "learning rate 0.000963811\n",
      "epoch 4\n",
      "batch 0\n",
      "training minibatch loss: 5.123816967010498\n",
      "  sample 1:\n",
      "    enc input           > long stream tomorrow <EOS>\n",
      "    dec input           > alright bro see you tomorrow on twitch <EOS>\n",
      "    dec train predicted > i is s you <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.820743083953857\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > maybe backroom shady shit <EOS>\n",
      "    DEV dec input           > i just read something that say no public appearance til the debate wtf ? <EOS>\n",
      "    DEV dec train predicted > i m just the <EOS> s i <EOS> <EOS> <EOS> <EOS> time <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m a na be a good thing <EOS> <EOS> <EOS>\n",
      "global_step: 2035\n",
      "learning rate 0.000962917\n",
      "epoch 4\n",
      "batch 50\n",
      "training minibatch loss: 4.936639308929443\n",
      "  sample 1:\n",
      "    enc input           > now i m gon na drive myself off a bridge <EOS>\n",
      "    dec input           > i refuse to let a stanford fan hit me with their car <EOS>\n",
      "    dec train predicted > i m to be you good time <EOS> my <EOS> my same <EOS>\n",
      "dev minibatch loss: 6.218629360198975\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > for a cooperative solution i mean <EOS>\n",
      "    DEV dec input           > i think the subsidy are in place and many city have a long history of cooperative housing scenario <EOS>\n",
      "    DEV dec train predicted > i have you new is a the <EOS> i <EOS> <EOS> to new <EOS> <EOS> the <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m been a new york <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2085\n",
      "learning rate 0.000962023\n",
      "epoch 4\n",
      "batch 100\n",
      "training minibatch loss: 5.26114559173584\n",
      "  sample 1:\n",
      "    enc input           > if trump is not making any profit out of it then why don t u release your tax <EOS>\n",
      "    dec input           > the clinton have perfected the politics of profit <EOS>\n",
      "    dec train predicted > trump clinton is a the debate of the <EOS>\n",
      "dev minibatch loss: 5.916671276092529\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > how much longer i got ta be a side piece for ? <EOS>\n",
      "    DEV dec input           > also not my husband <EOS>\n",
      "    DEV dec train predicted > i i a time <EOS>\n",
      "    DEV dec train infer > i m just na be a a na have a na be a na be a best <EOS>\n",
      "global_step: 2135\n",
      "learning rate 0.00096113\n",
      "epoch 4\n",
      "batch 150\n",
      "training minibatch loss: 5.158213138580322\n",
      "  sample 1:\n",
      "    enc input           > sad state of affair no respect taught child <EOS>\n",
      "    dec input           > public school in america latino teacher fighting with african american student people wonder about white flight ? <EOS>\n",
      "    dec train predicted > trump is is trump clinton a a a trump debate clinton trump are he the <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.803086757659912\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > it s a brain axe <EOS>\n",
      "    DEV dec input           > you have my axe <EOS>\n",
      "    DEV dec train predicted > i re a good <EOS>\n",
      "    DEV dec train infer > i m not a new of the new york <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2185\n",
      "learning rate 0.000960238\n",
      "epoch 4\n",
      "batch 200\n",
      "training minibatch loss: 5.102809429168701\n",
      "  sample 1:\n",
      "    enc input           > it okay thanks for being so quick to respond is this dan or tom ? <EOS>\n",
      "    dec input           > ok great sorry for the trouble <EOS>\n",
      "    dec train predicted > i i time <EOS> the day <EOS>\n",
      "dev minibatch loss: 5.846218585968018\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we need a new theology of money <EOS>\n",
      "    DEV dec input           > thought on this thread ? <EOS>\n",
      "    DEV dec train predicted > i i the s <EOS> <EOS>\n",
      "    DEV dec train infer > i m a new thing i m a new day <EOS> <EOS> <EOS>\n",
      "global_step: 2235\n",
      "learning rate 0.000959347\n",
      "epoch 4\n",
      "batch 250\n",
      "training minibatch loss: 5.281574726104736\n",
      "  sample 1:\n",
      "    enc input           > white men being personally offended gt gt gt woman being actually physically tortured and brutalized <EOS>\n",
      "    dec input           > it s not like he s quietly sitting during a song like a monster <EOS>\n",
      "    dec train predicted > i s a a the s a a <EOS> the best <EOS> the a <EOS>\n",
      "dev minibatch loss: 5.8749098777771\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > my president <EOS>\n",
      "    DEV dec input           > most people who are bully grow out of it donald trump hasn t <EOS>\n",
      "    DEV dec train predicted > i just are are a <EOS> <EOS> <EOS> the <EOS> the <EOS> <EOS>\n",
      "    DEV dec train infer > i m a new new york <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2285\n",
      "learning rate 0.000958457\n",
      "epoch 4\n",
      "batch 300\n",
      "training minibatch loss: 5.334665775299072\n",
      "  sample 1:\n",
      "    enc input           > yes it is haha thank you <EOS>\n",
      "    dec input           > you are so beautiful is this you ? xd <EOS>\n",
      "    dec train predicted > i re a a <EOS> you <EOS> re <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 5.601955890655518\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > like every day <EOS>\n",
      "    DEV dec input           > how often are you in manhattan ? <EOS>\n",
      "    DEV dec train predicted > i i i you <EOS> the <EOS> <EOS>\n",
      "    DEV dec train infer > i m a a best <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2335\n",
      "learning rate 0.000957567\n",
      "epoch 4\n",
      "batch 350\n",
      "training minibatch loss: 5.252264022827148\n",
      "  sample 1:\n",
      "    enc input           > i originally read that a bug catching <EOS>\n",
      "    dec input           > friday night is for bug writing <EOS>\n",
      "    dec train predicted > i the is a a to to\n",
      "dev minibatch loss: 5.622704982757568\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > sure common respect you ll never hear a coach say anything negative about a rival player or team <EOS>\n",
      "    DEV dec input           > there is a reason d coordinator say he is one of the best <EOS>\n",
      "    DEV dec train predicted > i s a na of the you you s a <EOS> the debate <EOS>\n",
      "    DEV dec train infer > the debate is a the best of the debate <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2385\n",
      "learning rate 0.000956679\n",
      "epoch 4\n",
      "batch 400\n",
      "training minibatch loss: 5.223208427429199\n",
      "  sample 1:\n",
      "    enc input           > this is from march <EOS>\n",
      "    dec input           > message continues to resonate with american people <EOS>\n",
      "    dec train predicted > i is to the <EOS> the <EOS> <EOS>\n",
      "dev minibatch loss: 5.826286792755127\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > xx dj al emo xx on the s and s <EOS>\n",
      "    DEV dec input           > lmao just walk around cry into the microphone all night <EOS>\n",
      "    DEV dec train predicted > i i i to <EOS> <EOS> the <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m been a good day <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2435\n",
      "learning rate 0.000955791\n",
      "epoch 4\n",
      "batch 450\n",
      "training minibatch loss: 5.1509175300598145\n",
      "  sample 1:\n",
      "    enc input           > or let the woman she fight use steroid <EOS>\n",
      "    dec input           > please let her fight men sick of seeing her bully beat down these woman <EOS>\n",
      "    dec train predicted > i you you know <EOS> <EOS> <EOS> the the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.610608100891113\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > u fishing for straight people <EOS>\n",
      "    DEV dec input           > i want a boo thang too let go fishing <EOS>\n",
      "    DEV dec train predicted > i m to birthday <EOS> <EOS> <EOS> s <EOS> <EOS>\n",
      "    DEV dec train infer > i m to be a good of the day <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2481\n",
      "learning rate 0.000954975\n",
      "epoch 5\n",
      "batch 0\n",
      "training minibatch loss: 5.001895427703857\n",
      "  sample 1:\n",
      "    enc input           > your racist also you should stop lying for trump you re trapped in his bull <EOS>\n",
      "    dec input           > bloody weekend in nyc put rising murder rate in spotlight via <EOS>\n",
      "    dec train predicted > i is of the <EOS> a a to to the <EOS> <EOS>\n",
      "dev minibatch loss: 5.423099040985107\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i knew it ditto also love freaky friday w jodie amp barbara d <EOS>\n",
      "    DEV dec input           > love and of course <EOS>\n",
      "    DEV dec train predicted > i it i the i\n",
      "    DEV dec train infer > i m a na be a na be a na be a own day <EOS>\n",
      "global_step: 2531\n",
      "learning rate 0.000954089\n",
      "epoch 5\n",
      "batch 50\n",
      "training minibatch loss: 4.726776123046875\n",
      "  sample 1:\n",
      "    enc input           > let s put roast beef on our belly button <EOS>\n",
      "    dec input           > you re the dog of this city <EOS>\n",
      "    dec train predicted > i re a new <EOS> the <EOS> <EOS>\n",
      "dev minibatch loss: 5.864627838134766\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i ll try it next time <EOS>\n",
      "    DEV dec input           > the pork spicy taco across from tasty pot is pretty fuckin fire too <EOS>\n",
      "    DEV dec train predicted > i friend is is is the i <EOS> <EOS> so good good <EOS> <EOS>\n",
      "    DEV dec train infer > i m so good <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2581\n",
      "learning rate 0.000953203\n",
      "epoch 5\n",
      "batch 100\n",
      "training minibatch loss: 5.103524684906006\n",
      "  sample 1:\n",
      "    enc input           > awwww nice hey are you going to see bodyguard in theater on december nd ? ? ? <EOS>\n",
      "    dec input           > my bieber shirt black jean and converse or something haha <EOS>\n",
      "    dec train predicted > i whole is <EOS> <EOS> <EOS> i <EOS> i to <EOS>\n",
      "dev minibatch loss: 6.032657146453857\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > well playing banjo doe build up some serious arm strength <EOS>\n",
      "    DEV dec input           > fyi boxing steve martin back in the ring after absence of a year <EOS>\n",
      "    DEV dec train predicted > i is a thing <EOS> to the new <EOS> the <EOS> the own <EOS>\n",
      "    DEV dec train infer > i don t know i m to a a new york <EOS> <EOS>\n",
      "global_step: 2631\n",
      "learning rate 0.000952319\n",
      "epoch 5\n",
      "batch 150\n",
      "training minibatch loss: 4.960519790649414\n",
      "  sample 1:\n",
      "    enc input           > winning made my day <EOS>\n",
      "    dec input           > and thus wrap another which win made you happiest tonight ? <EOS>\n",
      "    dec train predicted > i the a <EOS> <EOS> <EOS> <EOS> the <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.164516925811768\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > glad you guy are ok <EOS>\n",
      "    DEV dec input           > i wa walking down that block just last night this mean nothing but is still weird <EOS>\n",
      "    DEV dec train predicted > the m a a a <EOS> <EOS> a night <EOS> is <EOS> <EOS> <EOS> a <EOS> <EOS>\n",
      "    DEV dec train infer > i m a good thing <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2681\n",
      "learning rate 0.000951435\n",
      "epoch 5\n",
      "batch 200\n",
      "training minibatch loss: 4.877023696899414\n",
      "  sample 1:\n",
      "    enc input           > i must live in a diff world <EOS>\n",
      "    dec input           > na deadass tho <EOS>\n",
      "    dec train predicted > i re i <EOS>\n",
      "dev minibatch loss: 5.5642008781433105\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > happy birth date <EOS>\n",
      "    DEV dec input           > it my birthday today time to get baked and eat some sushi <EOS>\n",
      "    DEV dec train predicted > happy s birthday is <EOS> <EOS> you your <EOS> i <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > happy birthday of my birthday <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2731\n",
      "learning rate 0.000950552\n",
      "epoch 5\n",
      "batch 250\n",
      "training minibatch loss: 5.193253040313721\n",
      "  sample 1:\n",
      "    enc input           > she may be your type but she look like a pig so move on and stop pushing her <EOS>\n",
      "    dec input           > follow the perfect queen look how beautiful she is and look at them lip <EOS>\n",
      "    dec train predicted > i that debate of is a is <EOS> is not much to the <EOS> <EOS>\n",
      "dev minibatch loss: 5.683102130889893\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > s ionnidis ioannidis sigh <EOS>\n",
      "    DEV dec input           > also read up on just about anything written by john ionnidis about bad study <EOS>\n",
      "    DEV dec train predicted > i is to to the the the <EOS> <EOS> <EOS> <EOS> <EOS> the <EOS> <EOS>\n",
      "    DEV dec train infer > i m to be a new thing <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2781\n",
      "learning rate 0.00094967\n",
      "epoch 5\n",
      "batch 300\n",
      "training minibatch loss: 5.203429698944092\n",
      "  sample 1:\n",
      "    enc input           > did anderson cooper dye his hair ? <EOS>\n",
      "    dec input           > this show moved me to tear excited for animal lover to watch on october th <EOS>\n",
      "    dec train predicted > i s is the <EOS> be to <EOS> the <EOS> to be <EOS> the <EOS> <EOS>\n",
      "dev minibatch loss: 5.691000938415527\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > awwww they thought the little effort that they put in wa enough girl how bout now <EOS>\n",
      "    DEV dec input           > they still up right now but they suck right now <EOS>\n",
      "    DEV dec train predicted > i re been to <EOS> <EOS> i re <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m not a na be a good thing <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2831\n",
      "learning rate 0.000948788\n",
      "epoch 5\n",
      "batch 350\n",
      "training minibatch loss: 5.142646312713623\n",
      "  sample 1:\n",
      "    enc input           > for real i be so mad listening to ppl argue bout it <EOS>\n",
      "    dec input           > whoever said there not is stupid <EOS>\n",
      "    dec train predicted > i is you s a a to\n",
      "dev minibatch loss: 5.822326183319092\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > wait both of these should be not <EOS>\n",
      "    DEV dec input           > then the bridge is maybe <EOS>\n",
      "    DEV dec train predicted > i you new just a you\n",
      "    DEV dec train infer > i m a great thing <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 2881\n",
      "learning rate 0.000947908\n",
      "epoch 5\n",
      "batch 400\n",
      "training minibatch loss: 4.885693073272705\n",
      "  sample 1:\n",
      "    enc input           > bad idea wouldn t recommend <EOS>\n",
      "    dec input           > maybe i should light myself on fire too <EOS>\n",
      "    dec train predicted > i i m not to <EOS> the <EOS> <EOS>\n",
      "dev minibatch loss: 5.730291843414307\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you spelled shitbag of humanity wrong <EOS>\n",
      "    DEV dec input           > lt oh look another ignorant racist let s have twitter follower block him <EOS>\n",
      "    DEV dec train predicted > i the the to <EOS> i <EOS> s be been been <EOS> to <EOS>\n",
      "    DEV dec train infer > i m not na be na be a na a na be a great <EOS>\n",
      "global_step: 2931\n",
      "learning rate 0.000947028\n",
      "epoch 5\n",
      "batch 450\n",
      "training minibatch loss: 4.954239845275879\n",
      "  sample 1:\n",
      "    enc input           > nobody is doing anything about it what action can be taken ? none ? <EOS>\n",
      "    dec input           > the fbi ha prosecuted american for compromising information far le classified than what clinton and her staff <EOS>\n",
      "    dec train predicted > trump trump ha a a and the <EOS> and the and <EOS> the trump s trump <EOS> <EOS>\n",
      "dev minibatch loss: 5.753872871398926\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i just need these in my life omg please <EOS>\n",
      "    DEV dec input           > rt to win black rose gold zoeva brush set must be following me so i can dm winner <EOS>\n",
      "    DEV dec train predicted > i i be the <EOS> <EOS> <EOS> <EOS> <EOS> a be a <EOS> <EOS> be m <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m going to be a good time <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 2977\n",
      "learning rate 0.00094622\n",
      "epoch 6\n",
      "batch 0\n",
      "training minibatch loss: 4.701354026794434\n",
      "  sample 1:\n",
      "    enc input           > show their true color cash over anything <EOS>\n",
      "    dec input           > whoever is attending this money grab birthday need to be shot off into space <EOS>\n",
      "    dec train predicted > i is a a <EOS> just a <EOS> to be a <EOS> <EOS> the <EOS>\n",
      "dev minibatch loss: 6.096412658691406\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what s the breakdown of the indian immigrant population in term of religion ? <EOS>\n",
      "    DEV dec input           > indian are the fastest growing illegal immigrant population in the u via <EOS>\n",
      "    DEV dec train predicted > the and not own of in of is in charlotte final <EOS> <EOS>\n",
      "    DEV dec train infer > the powell are a lot of the refugee of the debate <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3027\n",
      "learning rate 0.000945342\n",
      "epoch 6\n",
      "batch 50\n",
      "training minibatch loss: 4.751650333404541\n",
      "  sample 1:\n",
      "    enc input           > wtf you think i m doing ? ? ? ? lol <EOS>\n",
      "    dec input           > you got me lol you best catch up though <EOS>\n",
      "    dec train predicted > i got a a <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.161874294281006\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > can i be excited for the what remains or nah ? <EOS>\n",
      "    DEV dec input           > well go look at the summary that wa posted and you ll know why i m excited <EOS>\n",
      "    DEV dec train predicted > i you to so the website <EOS> <EOS> a <EOS> <EOS> are be <EOS> <EOS> have to <EOS>\n",
      "    DEV dec train infer > the little website <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3077\n",
      "learning rate 0.000944464\n",
      "epoch 6\n",
      "batch 100\n",
      "training minibatch loss: 4.784975051879883\n",
      "  sample 1:\n",
      "    enc input           > dare i even ask what s drug these people are on that made this a good idea ? <EOS>\n",
      "    dec input           > trump will propose nationwide stop and frisk to address violence in black community nite on hannity <EOS>\n",
      "    dec train predicted > trump is be a a to trump <EOS> the the of the <EOS> <EOS> to the <EOS>\n",
      "dev minibatch loss: 6.168242454528809\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > folk who work on open source still got ta pay the bill somehow <EOS>\n",
      "    DEV dec input           > just feel like a great open sourced project for motivated people <EOS>\n",
      "    DEV dec train predicted > i just a a good yorker <EOS> <EOS> <EOS> the <EOS> <EOS>\n",
      "    DEV dec train infer > what s the new york <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3127\n",
      "learning rate 0.000943588\n",
      "epoch 6\n",
      "batch 150\n",
      "training minibatch loss: 4.756792068481445\n",
      "  sample 1:\n",
      "    enc input           > happy to you too <EOS>\n",
      "    dec input           > to my forever family happy to u all xo <EOS>\n",
      "    dec train predicted > i you great friend so so see <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.248312950134277\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > they are ya man in nyc <EOS>\n",
      "    DEV dec input           > these moment with jay <EOS>\n",
      "    DEV dec train predicted > i is is the the\n",
      "    DEV dec train infer > i m a na have a best time of the best time i m not a best <EOS> <EOS>\n",
      "global_step: 3177\n",
      "learning rate 0.000942712\n",
      "epoch 6\n",
      "batch 200\n",
      "training minibatch loss: 5.031868934631348\n",
      "  sample 1:\n",
      "    enc input           > you called a bigot are these latino bigot too ? <EOS>\n",
      "    dec input           > hey politifact also say you and don flat out lie <EOS>\n",
      "    dec train predicted > trump trump are trump trump re trump t <EOS> the <EOS>\n",
      "dev minibatch loss: 6.220100402832031\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > are you representing him in the divorce ? haha <EOS>\n",
      "    DEV dec input           > he s screwed either way i already went through the financials with babs <EOS>\n",
      "    DEV dec train predicted > i is a to <EOS> to m have to the same <EOS> the <EOS>\n",
      "    DEV dec train infer > i m a chance to be a good thing <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3227\n",
      "learning rate 0.000941837\n",
      "epoch 6\n",
      "batch 250\n",
      "training minibatch loss: 4.4211506843566895\n",
      "  sample 1:\n",
      "    enc input           > nope i am here for this too <EOS>\n",
      "    dec input           > what if i m the only one listening when you talk dawson s creek i m so intrigued <EOS>\n",
      "    dec train predicted > i s you m a same time <EOS> to i s to <EOS> a <EOS> m to <EOS> <EOS>\n",
      "dev minibatch loss: 5.843082904815674\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we need a new theology of money <EOS>\n",
      "    DEV dec input           > thought on this thread ? <EOS>\n",
      "    DEV dec train predicted > is to the s <EOS> <EOS>\n",
      "    DEV dec train infer > is a na go to the new time <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3277\n",
      "learning rate 0.000940963\n",
      "epoch 6\n",
      "batch 300\n",
      "training minibatch loss: 4.590932369232178\n",
      "  sample 1:\n",
      "    enc input           > gee i wonder why ? <EOS>\n",
      "    dec input           > sen chris coon donald trump passed on a meeting with the ukrainian president <EOS>\n",
      "    dec train predicted > trump clinton is trump trump s <EOS> the trump <EOS> the debate <EOS> <EOS>\n",
      "dev minibatch loss: 5.970664978027344\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > is your driving really that bad ? <EOS>\n",
      "    DEV dec input           > the face wa bc i know yo as gon na be scared of my driving <EOS>\n",
      "    DEV dec train predicted > i best of a i m i of <EOS> na be a <EOS> the phone <EOS>\n",
      "    DEV dec train infer > i m not a good thing <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3327\n",
      "learning rate 0.00094009\n",
      "epoch 6\n",
      "batch 350\n",
      "training minibatch loss: 5.123495101928711\n",
      "  sample 1:\n",
      "    enc input           > oh yeah ? that s awesome this is wonderful to hear we d appreciate any feedback you have <EOS>\n",
      "    dec input           > just watched really enjoyed <EOS>\n",
      "    dec train predicted > i got the a <EOS>\n",
      "dev minibatch loss: 5.657423973083496\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > wow you re disgusting <EOS>\n",
      "    DEV dec input           > trickle down like hillary s catheter <EOS>\n",
      "    DEV dec train predicted > i to <EOS> the is york <EOS>\n",
      "    DEV dec train infer > i m not a best of the new york <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3377\n",
      "learning rate 0.000939217\n",
      "epoch 6\n",
      "batch 400\n",
      "training minibatch loss: 4.917283058166504\n",
      "  sample 1:\n",
      "    enc input           > think before you tweet <EOS>\n",
      "    dec input           > because you can vote more than once like voter fraud conservative complain <EOS>\n",
      "    dec train predicted > i you need t to <EOS> the up the <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.8892822265625\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > very important info to figure out very confused amp a speechless over here cc <EOS>\n",
      "    DEV dec input           > i m with some gentleman who need an education in <EOS>\n",
      "    DEV dec train predicted > the think voting the departure the are to supporter <EOS> the\n",
      "    DEV dec train infer > is the other state of the other community <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 3427\n",
      "learning rate 0.000938346\n",
      "epoch 6\n",
      "batch 450\n",
      "training minibatch loss: 4.669210433959961\n",
      "  sample 1:\n",
      "    enc input           > the public see right through the establishment fluff after year of keeping u down stop it <EOS>\n",
      "    dec input           > thanks for watching <EOS>\n",
      "    dec train predicted > i to the <EOS>\n",
      "dev minibatch loss: 5.467226505279541\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > old black english teacher hello ? vampire student fuck me daddy <EOS>\n",
      "    DEV dec input           > what s up do you have game on your phone <EOS>\n",
      "    DEV dec train predicted > i s the <EOS> you be a <EOS> the debate <EOS>\n",
      "    DEV dec train infer > i wa a lot of the new time of the debate <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 3473\n",
      "learning rate 0.000937545\n",
      "epoch 7\n",
      "batch 0\n",
      "training minibatch loss: 4.467229843139648\n",
      "  sample 1:\n",
      "    enc input           > the first episode not the second <EOS>\n",
      "    dec input           > have you played tell tale s batman yet ? <EOS>\n",
      "    dec train predicted > i you got <EOS> the <EOS> the <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.622386932373047\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > have a great time i ll want a review please like it could be anything but awesome <EOS>\n",
      "    DEV dec input           > i am so excited can t wait to see tonight thanks <EOS>\n",
      "    DEV dec train predicted > i m so much to i be to be the <EOS> <EOS>\n",
      "    DEV dec train infer > i m a lot of a lot of the life <EOS> <EOS> <EOS>\n",
      "global_step: 3523\n",
      "learning rate 0.000936675\n",
      "epoch 7\n",
      "batch 50\n",
      "training minibatch loss: 4.520092487335205\n",
      "  sample 1:\n",
      "    enc input           > other desk ? <EOS>\n",
      "    dec input           > what the hell ? <EOS>\n",
      "    dec train predicted > i s hell is <EOS>\n",
      "dev minibatch loss: 5.896428108215332\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we may take a break somewhere between la and san diego <EOS>\n",
      "    DEV dec input           > who is coming to ? we ll have prize from pm edt <EOS>\n",
      "    DEV dec train predicted > t are a to the <EOS> re be the out the ? <EOS>\n",
      "    DEV dec train infer > did you be a serious of new of the original conspirator are the vietnam <EOS> <EOS>\n",
      "global_step: 3573\n",
      "learning rate 0.000935805\n",
      "epoch 7\n",
      "batch 100\n",
      "training minibatch loss: 4.649416446685791\n",
      "  sample 1:\n",
      "    enc input           > key win the lottery lol <EOS>\n",
      "    dec input           > where to get grand lol <EOS>\n",
      "    dec train predicted > i is the the of <EOS>\n",
      "dev minibatch loss: 6.107303619384766\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > so true lmfao <EOS>\n",
      "    DEV dec input           > the sign a joanne the scammer gifs aquarius <EOS>\n",
      "    DEV dec train predicted > i best <EOS> best <EOS> th <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m just a good thing <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3623\n",
      "learning rate 0.000934937\n",
      "epoch 7\n",
      "batch 150\n",
      "training minibatch loss: 4.61086893081665\n",
      "  sample 1:\n",
      "    enc input           > i can t watch this anymore wake me when we win the world series <EOS>\n",
      "    dec input           > blown save by committee <EOS>\n",
      "    dec train predicted > i in the <EOS> <EOS>\n",
      "dev minibatch loss: 5.8014349937438965\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > actually read this <EOS>\n",
      "    DEV dec input           > here are some actual fact stop making it worse the myth of black life matter via <EOS>\n",
      "    DEV dec train predicted > i s you one album <EOS> <EOS> a <EOS> <EOS> year <EOS> the <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > this is the new york <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3673\n",
      "learning rate 0.000934069\n",
      "epoch 7\n",
      "batch 200\n",
      "training minibatch loss: 4.79115629196167\n",
      "  sample 1:\n",
      "    enc input           > i wonder what he would do for ? <EOS>\n",
      "    dec input           > sudbury s finest <EOS>\n",
      "    dec train predicted > i s a <EOS>\n",
      "dev minibatch loss: 6.183692932128906\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > congrats recognition well deserved <EOS>\n",
      "    DEV dec input           > i m so excited to be recognised a one of the by amp thank you <EOS>\n",
      "    DEV dec train predicted > just m ready much to see a <EOS> the <EOS> a new <EOS> <EOS> you <EOS>\n",
      "    DEV dec train infer > just just just have a excited to see your favorite <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3723\n",
      "learning rate 0.000933202\n",
      "epoch 7\n",
      "batch 250\n",
      "training minibatch loss: 4.460565567016602\n",
      "  sample 1:\n",
      "    enc input           > i m a firm believer he created said epidemic <EOS>\n",
      "    dec input           > but he also ignored an epidemic that killed off black people so that wa a benefit for them <EOS>\n",
      "    dec train predicted > the he s is the clinton of he trump the than <EOS> not s a stripper of the <EOS>\n",
      "dev minibatch loss: 5.904306411743164\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > this will be the second time tony will miss <EOS>\n",
      "    DEV dec input           > join these handsome men sunday a they discus this week s episode of <EOS>\n",
      "    DEV dec train predicted > i the final to <EOS> <EOS> final are the <EOS> <EOS> choice <EOS> the\n",
      "    DEV dec train infer > i m not a lot of the final argument to be a lot <EOS>\n",
      "global_step: 3773\n",
      "learning rate 0.000932336\n",
      "epoch 7\n",
      "batch 300\n",
      "training minibatch loss: 4.488662242889404\n",
      "  sample 1:\n",
      "    enc input           > can we talk about growing the game during the lockout ? <EOS>\n",
      "    dec input           > eh that s a separate conversation until it isn t in the meantime gim me some money <EOS>\n",
      "    dec train predicted > i i s the good of to i s t the the world <EOS> you <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.937142372131348\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > thought position would refer to something else tbh <EOS>\n",
      "    DEV dec input           > wow at my page for pride on the office of lgbt life website <EOS>\n",
      "    DEV dec train predicted > i he least orange is a she trump truth is a s is <EOS>\n",
      "    DEV dec train infer > i m not a question to be a word <EOS> <EOS> <EOS>\n",
      "global_step: 3823\n",
      "learning rate 0.000931471\n",
      "epoch 7\n",
      "batch 350\n",
      "training minibatch loss: 4.719212055206299\n",
      "  sample 1:\n",
      "    enc input           > do the thing you can do it <EOS>\n",
      "    dec input           > doing something way out of my comfort zone tonight amp procrastinating by tweeting bc i m scared <EOS>\n",
      "    dec train predicted > i the i i of the same <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> i <EOS> am not <EOS>\n",
      "dev minibatch loss: 6.095802307128906\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > pure envy this sob is killing my back <EOS>\n",
      "    DEV dec input           > like my chair swoon <EOS>\n",
      "    DEV dec train predicted > the the question of in\n",
      "    DEV dec train infer > i m not a lot of the world <EOS> <EOS> <EOS>\n",
      "global_step: 3873\n",
      "learning rate 0.000930607\n",
      "epoch 7\n",
      "batch 400\n",
      "training minibatch loss: 4.932308197021484\n",
      "  sample 1:\n",
      "    enc input           > i tweeted this too someone will recognize him surely how are you feeling ? <EOS>\n",
      "    dec input           > just in new photo of the shooting suspect in the can you help police identify him ? <EOS>\n",
      "    dec train predicted > i like the york of the debate of <EOS> the debate be have <EOS> have him <EOS> <EOS>\n",
      "dev minibatch loss: 6.326213836669922\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > yeah i saw it the day after it came out <EOS>\n",
      "    DEV dec input           > yeah i remember i went to the movie to watch believe movie when it came <EOS>\n",
      "    DEV dec train predicted > i i m it m to get same <EOS> get <EOS> i <EOS> i s <EOS>\n",
      "    DEV dec train infer > i m a good day i m to get to get to my as <EOS> <EOS> <EOS>\n",
      "global_step: 3923\n",
      "learning rate 0.000929743\n",
      "epoch 7\n",
      "batch 450\n",
      "training minibatch loss: 4.841257095336914\n",
      "  sample 1:\n",
      "    enc input           > this is a thing you can see in the window of the amsterdam red light district in use <EOS>\n",
      "    dec input           > it s like riding a bike and a dick at the same time <EOS>\n",
      "    dec train predicted > i s a a the new in i new <EOS> the same day <EOS>\n",
      "dev minibatch loss: 5.749940872192383\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > yea that wa rough <EOS>\n",
      "    DEV dec input           > oh god help you i hated that movie <EOS>\n",
      "    DEV dec train predicted > i i i i don m it <EOS> is\n",
      "    DEV dec train infer > i m not a new debate <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 3969\n",
      "learning rate 0.000928949\n",
      "epoch 8\n",
      "batch 0\n",
      "training minibatch loss: 4.642457008361816\n",
      "  sample 1:\n",
      "    enc input           > that s hideous <EOS>\n",
      "    dec input           > if you have a weak stomach look away <EOS>\n",
      "    dec train predicted > i you re a bit of <EOS> in <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 5.927085876464844\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > dude seriously read his debate he should not be your favorite writer <EOS>\n",
      "    DEV dec input           > funny how my fav author get into quarrel with the other thinker i like such a pinker and <EOS>\n",
      "    DEV dec train predicted > and trump trump racist say is a a <EOS> the debate <EOS> <EOS> say to <EOS> fraud <EOS> <EOS>\n",
      "    DEV dec train infer > trump s racist is a trump of the debate <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4019\n",
      "learning rate 0.000928087\n",
      "epoch 8\n",
      "batch 50\n",
      "training minibatch loss: 4.493500709533691\n",
      "  sample 1:\n",
      "    enc input           > my heart is broken i wanted to see you pitch <EOS>\n",
      "    dec input           > don t worry ppl can t get teammate sick taking precautionary measure at home and in clubhouse <EOS>\n",
      "    dec train predicted > i t get i i t wait to to and a <EOS> <EOS> my <EOS> i <EOS> <EOS>\n",
      "dev minibatch loss: 5.894721031188965\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that shit wa a bitch for me too <EOS>\n",
      "    DEV dec input           > it about time i passed my license <EOS>\n",
      "    DEV dec train predicted > i s the <EOS> m to favorite <EOS>\n",
      "    DEV dec train infer > i m a na go to the gym <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4069\n",
      "learning rate 0.000927226\n",
      "epoch 8\n",
      "batch 100\n",
      "training minibatch loss: 4.415208339691162\n",
      "  sample 1:\n",
      "    enc input           > or a faggot lol <EOS>\n",
      "    dec input           > or just responsible <EOS>\n",
      "    dec train predicted > i i a for\n",
      "dev minibatch loss: 6.011373043060303\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > remember when folk tried to argue that fighter under couldn t be star ? <EOS>\n",
      "    DEV dec input           > yeah he s by far the biggest in ufc history i just did a whole thing on this <EOS>\n",
      "    DEV dec train predicted > i that s a a to people opinion ny of <EOS> m went <EOS> layup layup <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m not a fiend for a deplorable <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4119\n",
      "learning rate 0.000926365\n",
      "epoch 8\n",
      "batch 150\n",
      "training minibatch loss: 4.380284309387207\n",
      "  sample 1:\n",
      "    enc input           > good luck you got this <EOS>\n",
      "    dec input           > ready to sing my heart out <EOS>\n",
      "    dec train predicted > i to be the life <EOS> <EOS>\n",
      "dev minibatch loss: 5.918981075286865\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i would love to give you mine <EOS>\n",
      "    DEV dec input           > noooo feckless are so cute i wish i had some <EOS>\n",
      "    DEV dec train predicted > i i i you much <EOS> m <EOS> m a <EOS>\n",
      "    DEV dec train infer > i m so good of the day i m so good of the day <EOS>\n",
      "global_step: 4169\n",
      "learning rate 0.000925506\n",
      "epoch 8\n",
      "batch 200\n",
      "training minibatch loss: 4.465980052947998\n",
      "  sample 1:\n",
      "    enc input           > probably break up with him too right <EOS>\n",
      "    dec input           > ur right i would be <EOS>\n",
      "    dec train predicted > i the <EOS> wa be a\n",
      "dev minibatch loss: 6.187960147857666\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > well rn i m the rightful owner so <EOS>\n",
      "    DEV dec input           > i just give the child to it s rightful owner <EOS>\n",
      "    DEV dec train predicted > i m got to best to get <EOS> a <EOS> <EOS>\n",
      "    DEV dec train infer > i m gon na be a good day <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4219\n",
      "learning rate 0.000924647\n",
      "epoch 8\n",
      "batch 250\n",
      "training minibatch loss: 4.672307014465332\n",
      "  sample 1:\n",
      "    enc input           > word shit dick <EOS>\n",
      "    dec input           > there is truly nothing worse than chapped lip <EOS>\n",
      "    dec train predicted > i s a a to <EOS> the <EOS> <EOS>\n",
      "dev minibatch loss: 6.045161724090576\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > so is global warming and all the other b that democrat spew <EOS>\n",
      "    DEV dec input           > superbug the greatest and most urgent global risk <EOS>\n",
      "    DEV dec train predicted > the is fbi suspect suspect suspect suspect state of\n",
      "    DEV dec train infer > the news is the state of the sweatpants suspect to be a big liar <EOS> <EOS>\n",
      "global_step: 4269\n",
      "learning rate 0.000923789\n",
      "epoch 8\n",
      "batch 300\n",
      "training minibatch loss: 4.60879373550415\n",
      "  sample 1:\n",
      "    enc input           > the sauce not you pardon me <EOS>\n",
      "    dec input           > i refuse to believe that can taste good <EOS>\n",
      "    dec train predicted > i just to be i i be the <EOS>\n",
      "dev minibatch loss: 5.54288911819458\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > reason why school is satan <EOS>\n",
      "    DEV dec input           > i fell down stair at school and re sprained the same ankle i already sprained <EOS>\n",
      "    DEV dec train predicted > i have to <EOS> <EOS> <EOS> <EOS> i the <EOS> new <EOS> <EOS> ever <EOS> <EOS>\n",
      "    DEV dec train infer > this is a new york <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4319\n",
      "learning rate 0.000922931\n",
      "epoch 8\n",
      "batch 350\n",
      "training minibatch loss: 4.414571285247803\n",
      "  sample 1:\n",
      "    enc input           > so happy you love them <EOS>\n",
      "    dec input           > i live for instagram story <EOS>\n",
      "    dec train predicted > happy m a the <EOS> <EOS>\n",
      "dev minibatch loss: 5.9734787940979\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > here is your assignment read stolen legacy then come back with an intelligent tweet <EOS>\n",
      "    DEV dec input           > behave shouldn t you be doing your homework ? <EOS>\n",
      "    DEV dec train predicted > how a t the need a <EOS> name <EOS> <EOS>\n",
      "    DEV dec train infer > no potato a plan of new york <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4369\n",
      "learning rate 0.000922075\n",
      "epoch 8\n",
      "batch 400\n",
      "training minibatch loss: 4.5727362632751465\n",
      "  sample 1:\n",
      "    enc input           > please hannah <EOS>\n",
      "    dec input           > i want to i ll be home <EOS>\n",
      "    dec train predicted > i m to be have be a to\n",
      "dev minibatch loss: 5.876378536224365\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > no i m not even starting on saturday morning cartoon i ve accepted those question go unanswered <EOS>\n",
      "    DEV dec input           > i don t know why the joes never shot the cobra either <EOS>\n",
      "    DEV dec train predicted > i m t know i i one is m to friend i <EOS>\n",
      "    DEV dec train infer > i m sorry i m not so good i m not so good <EOS> <EOS> <EOS>\n",
      "global_step: 4419\n",
      "learning rate 0.000921219\n",
      "epoch 8\n",
      "batch 450\n",
      "training minibatch loss: 4.599717617034912\n",
      "  sample 1:\n",
      "    enc input           > yeah that s right home depot dave bill and sherman gon na f ck you up <EOS>\n",
      "    dec input           > so watch your back rude clerk at home depot <EOS>\n",
      "    dec train predicted > i you the life so thing to <EOS> of <EOS>\n",
      "dev minibatch loss: 6.034024715423584\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > ricky you still trying to convince people that lv is real just stop dude <EOS>\n",
      "    DEV dec input           > they have a site russell rd <EOS>\n",
      "    DEV dec train predicted > i re a chance of is of\n",
      "    DEV dec train infer > i m not a chance of the debate <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4465\n",
      "learning rate 0.000920433\n",
      "epoch 9\n",
      "batch 0\n",
      "training minibatch loss: 4.2931976318359375\n",
      "  sample 1:\n",
      "    enc input           > here come making his way through the italian market just in time to make the steelers eagle game <EOS>\n",
      "    dec input           > said he would walk home if the phillies lost after they were up let go mets <EOS>\n",
      "    dec train predicted > is the s be a in the game ? the the are a ? s <EOS> ?\n",
      "dev minibatch loss: 5.903510570526123\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i can t imagine all the people who have to drive on grand central daily during this hoopla <EOS>\n",
      "    DEV dec input           > note to self laguardia airport is simply <EOS>\n",
      "    DEV dec train predicted > no to be a child <EOS> a <EOS>\n",
      "    DEV dec train infer > black black say trump are not a black black refugee group say trump s campaign <EOS> <EOS> <EOS>\n",
      "global_step: 4515\n",
      "learning rate 0.000919578\n",
      "epoch 9\n",
      "batch 50\n",
      "training minibatch loss: 4.13307523727417\n",
      "  sample 1:\n",
      "    enc input           > repeal and replace with what sir ? <EOS>\n",
      "    dec input           > but you definitely should not have to pay this tax after your co op fails <EOS>\n",
      "    dec train predicted > i i don have be be a be the <EOS> <EOS> a mail <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.269712448120117\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that s le than an iphone dongle <EOS>\n",
      "    DEV dec input           > an issue these day ? ? ? <EOS>\n",
      "    DEV dec train predicted > the news of of in <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > the way that s a a a sane in the world <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 4565\n",
      "learning rate 0.000918725\n",
      "epoch 9\n",
      "batch 100\n",
      "training minibatch loss: 4.297235012054443\n",
      "  sample 1:\n",
      "    enc input           > omg i did that friday they wont wash out <EOS>\n",
      "    dec input           > i spilled popper on my fitted sheet so fuck my sleep tonight <EOS>\n",
      "    dec train predicted > i don not for the nose of to far <EOS> phone <EOS> <EOS>\n",
      "dev minibatch loss: 6.522185802459717\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > no black for birther no latino for wall no female for end of pp <EOS>\n",
      "    DEV dec input           > i still won t tell u how to vote tho <EOS>\n",
      "    DEV dec train predicted > who can googled t be the to you talk to <EOS>\n",
      "    DEV dec train infer > chief will wonder the state department will be a far a far a far a far a charge <EOS>\n",
      "global_step: 4615\n",
      "learning rate 0.000917872\n",
      "epoch 9\n",
      "batch 150\n",
      "training minibatch loss: 4.508285045623779\n",
      "  sample 1:\n",
      "    enc input           > how much a piece <EOS>\n",
      "    dec input           > guy i have four ticket to chance in toronto tmrw somebody pls buy them off of me <EOS>\n",
      "    dec train predicted > this can m a other to the to the the the will have the <EOS> <EOS> the <EOS>\n",
      "dev minibatch loss: 6.306025981903076\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we need a new theology of money <EOS>\n",
      "    DEV dec input           > thought on this thread ? <EOS>\n",
      "    DEV dec train predicted > i you the time <EOS> <EOS>\n",
      "    DEV dec train infer > a a website ? ? ? <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4665\n",
      "learning rate 0.00091702\n",
      "epoch 9\n",
      "batch 200\n",
      "training minibatch loss: 4.262300968170166\n",
      "  sample 1:\n",
      "    enc input           > ugh fat people in bed are so good at cyber <EOS>\n",
      "    dec input           > the hacker weren t russian they were fat people on their bed <EOS>\n",
      "    dec train predicted > i people is t not not are a fault <EOS> the people <EOS>\n",
      "dev minibatch loss: 6.420347690582275\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > greeting from new york when you watch go through the tweet hilarious thought provoking insightful <EOS>\n",
      "    DEV dec input           > i ll know what you re referring to when i watch later tweeting from taiwan <EOS>\n",
      "    DEV dec train predicted > i m be you you can in to the you can the <EOS> <EOS> the <EOS>\n",
      "    DEV dec train infer > i m a woman to support the fact you re a rationale job <EOS> <EOS> <EOS>\n",
      "global_step: 4715\n",
      "learning rate 0.000916169\n",
      "epoch 9\n",
      "batch 250\n",
      "training minibatch loss: 4.38602352142334\n",
      "  sample 1:\n",
      "    enc input           > that moment plus his comment on housing <EOS>\n",
      "    dec input           > that moment is going to stick trump seemingly bragging about not paying federal income tax <EOS>\n",
      "    dec train predicted > and s trump a to be trump in is to the be the trump debate <EOS>\n",
      "dev minibatch loss: 6.222219467163086\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > smh don t disrespect me like that <EOS>\n",
      "    DEV dec input           > nope i m still convinced you went to class and got this from the internet <EOS>\n",
      "    DEV dec train predicted > i i m a a <EOS> <EOS> to my of i ta <EOS> me phone <EOS>\n",
      "    DEV dec train infer > i m just laughing in my phone <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4765\n",
      "learning rate 0.000915319\n",
      "epoch 9\n",
      "batch 300\n",
      "training minibatch loss: 4.122017860412598\n",
      "  sample 1:\n",
      "    enc input           > how often did you get a failing grade for doing too much homework ? <EOS>\n",
      "    dec input           > exposed trump s lack of preparation but clinton seemed over prepared at time <EOS>\n",
      "    dec train predicted > trump trump s not of preparation <EOS> clinton seemed out of <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.863641738891602\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you got that right <EOS>\n",
      "    DEV dec input           > the shouldve tried the men on the field approach much earlier in this game <EOS>\n",
      "    DEV dec train predicted > i fuck got to best is the time <EOS> <EOS> <EOS> <EOS> the <EOS> <EOS>\n",
      "    DEV dec train infer > i m not a lot of a car <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4815\n",
      "learning rate 0.00091447\n",
      "epoch 9\n",
      "batch 350\n",
      "training minibatch loss: 4.207087516784668\n",
      "  sample 1:\n",
      "    enc input           > shut up broke boi <EOS>\n",
      "    dec input           > thanks loser <EOS>\n",
      "    dec train predicted > i for <EOS>\n",
      "dev minibatch loss: 6.295001029968262\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i mean playing with your own boob can be quite amusing that guy know wassup <EOS>\n",
      "    DEV dec input           > oh the first gif <EOS>\n",
      "    DEV dec train predicted > trump trump latest of is\n",
      "    DEV dec train infer > trump is a trump question you are voting to the clinton <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4865\n",
      "learning rate 0.000913621\n",
      "epoch 9\n",
      "batch 400\n",
      "training minibatch loss: 4.696996688842773\n",
      "  sample 1:\n",
      "    enc input           > already ? it wa in theater just week ago <EOS>\n",
      "    dec input           > blazing saddle on right now cinemax <EOS>\n",
      "    dec train predicted > and iphone to the to snowden <EOS>\n",
      "dev minibatch loss: 5.9888482093811035\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i d forgotten how hot he wa <EOS>\n",
      "    DEV dec input           > here s the painting paid k for using other people s charity money <EOS>\n",
      "    DEV dec train predicted > i s the same that to the the the <EOS> to like to to\n",
      "    DEV dec train infer > i m just going to the day <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 4915\n",
      "learning rate 0.000912773\n",
      "epoch 9\n",
      "batch 450\n",
      "training minibatch loss: 4.554635524749756\n",
      "  sample 1:\n",
      "    enc input           > what are those called i ve never seen them ? <EOS>\n",
      "    dec input           > trivia ? which nba slam dunk champion wore this shoe in the white blue colorway ? <EOS>\n",
      "    dec train predicted > i up i is like the <EOS> s me time <EOS> the phone time <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.3313398361206055\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the same thing u are <EOS>\n",
      "    DEV dec input           > what are you thinking about <EOS>\n",
      "    DEV dec train predicted > i s you the to the\n",
      "    DEV dec train infer > i m not in the game i m in the game <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 4961\n",
      "learning rate 0.000911994\n",
      "epoch 10\n",
      "batch 0\n",
      "training minibatch loss: 4.224234580993652\n",
      "  sample 1:\n",
      "    enc input           > fair enough come to sf and be sketchy w me <EOS>\n",
      "    dec input           > hahaha i can t put on twitter <EOS>\n",
      "    dec train predicted > i i m t wait my the <EOS>\n",
      "dev minibatch loss: 5.957417964935303\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > omfg youre so cute i love you sm <EOS>\n",
      "    DEV dec input           > we gon na fail chem together but at least she s gon na look good while doing it <EOS>\n",
      "    DEV dec train predicted > i re na love my <EOS> <EOS> i <EOS> i are so na love <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i love you <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5011\n",
      "learning rate 0.000911148\n",
      "epoch 10\n",
      "batch 50\n",
      "training minibatch loss: 3.8853256702423096\n",
      "  sample 1:\n",
      "    enc input           > if you re talking to me yes my profile pic is of my actual face during summer time <EOS>\n",
      "    dec input           > is that your actual picture ? pct a real a mine <EOS>\n",
      "    dec train predicted > is the ? deal age <EOS> <EOS> a real time few ?\n",
      "dev minibatch loss: 6.6661152839660645\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > this is going to bang pun intended <EOS>\n",
      "    DEV dec input           > trump only want stop and frisk so he ha an excuse for feeling up ivanka <EOS>\n",
      "    DEV dec train predicted > i s at to to i of i s a bit <EOS> my <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i have a bit of a bit i m obsessed on <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5061\n",
      "learning rate 0.000910302\n",
      "epoch 10\n",
      "batch 100\n",
      "training minibatch loss: 4.143047332763672\n",
      "  sample 1:\n",
      "    enc input           > receipt on that ? <EOS>\n",
      "    dec input           > sound just like hillary and her email while secretary of state <EOS>\n",
      "    dec train predicted > i just a a is i friend <EOS> i of debate <EOS>\n",
      "dev minibatch loss: 6.320602893829346\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > before the season started i would have said siemian now i would say keenum <EOS>\n",
      "    DEV dec input           > who will be first qb to be benched ? blaine gabbert case keenum kirk cousin trevor siemian <EOS>\n",
      "    DEV dec train predicted > don s be a <EOS> <EOS> be kidding <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > don is the hell <EOS> ? <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 5111\n",
      "learning rate 0.000909457\n",
      "epoch 10\n",
      "batch 150\n",
      "training minibatch loss: 4.056541919708252\n",
      "  sample 1:\n",
      "    enc input           > thanks any regret ? would it have been possible to replace the router first instead of last ? <EOS>\n",
      "    dec input           > we did it a the last piece of the puzzle here the pr <EOS>\n",
      "    dec train predicted > i are you to good journey of of the same <EOS> <EOS> same <EOS>\n",
      "dev minibatch loss: 6.174872398376465\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > same so i guess they really meant freshman <EOS>\n",
      "    DEV dec input           > however i did lose pound <EOS>\n",
      "    DEV dec train predicted > i the m that the <EOS>\n",
      "    DEV dec train infer > i m not a only argument that s the league <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5161\n",
      "learning rate 0.000908613\n",
      "epoch 10\n",
      "batch 200\n",
      "training minibatch loss: 4.120944976806641\n",
      "  sample 1:\n",
      "    enc input           > she s actually disgusting like who tf go to that after someone hott like u <EOS>\n",
      "    dec input           > he s just upset he downgraded <EOS>\n",
      "    dec train predicted > i s not a <EOS> downgraded <EOS>\n",
      "dev minibatch loss: 6.508123874664307\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what you mean lol <EOS>\n",
      "    DEV dec input           > your giveaway are definitely awesome wish i could figure em out lol <EOS>\n",
      "    DEV dec train predicted > i own like a <EOS> <EOS> <EOS> m be <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m not a little character <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5211\n",
      "learning rate 0.00090777\n",
      "epoch 10\n",
      "batch 250\n",
      "training minibatch loss: 4.177100658416748\n",
      "  sample 1:\n",
      "    enc input           > great to see you <EOS>\n",
      "    dec input           > great meeting with alum at school of management <EOS>\n",
      "    dec train predicted > thank to <EOS> the <EOS> pm <EOS> a <EOS>\n",
      "dev minibatch loss: 6.6330790519714355\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > better than fap meat <EOS>\n",
      "    DEV dec input           > flap meat ? no i m good thanks <EOS>\n",
      "    DEV dec train predicted > the night <EOS> <EOS> <EOS> have ready <EOS> <EOS>\n",
      "    DEV dec train infer > what s the confirmation <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5261\n",
      "learning rate 0.000906928\n",
      "epoch 10\n",
      "batch 300\n",
      "training minibatch loss: 4.268145561218262\n",
      "  sample 1:\n",
      "    enc input           > make sense to visit the airport to speak to someone in person ? <EOS>\n",
      "    dec input           > our centralized baggage office can be contacted at bz <EOS>\n",
      "    dec train predicted > bdubs centralized join st reclaim reclaim contacted in the <EOS>\n",
      "dev minibatch loss: 6.623170375823975\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > proud to be a trump supporter and reject racist genocidal hillary clinton because of this <EOS>\n",
      "    DEV dec input           > this is a day after st debate million in fundraising k attendance in melbourne fl <EOS>\n",
      "    DEV dec train predicted > trump is a trump of trump trump and of trump of to <EOS> the <EOS> <EOS>\n",
      "    DEV dec train infer > trump s griffin of a trump manager based of the president <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5311\n",
      "learning rate 0.000906086\n",
      "epoch 10\n",
      "batch 350\n",
      "training minibatch loss: 4.067634105682373\n",
      "  sample 1:\n",
      "    enc input           > that dead as get me sooo fucking angry <EOS>\n",
      "    dec input           > when you could taste it nd get there nd it s gone <EOS>\n",
      "    dec train predicted > i i re be the <EOS> <EOS> a <EOS> <EOS> s a <EOS>\n",
      "dev minibatch loss: 5.79758882522583\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > im pretty sure it wa completely sarcastic at least i hope <EOS>\n",
      "    DEV dec input           > i read that this morning and didn t understand anything it said <EOS>\n",
      "    DEV dec train predicted > i m it i shit <EOS> i t <EOS> i <EOS> s <EOS>\n",
      "    DEV dec train infer > i m laughing laughing to be a good day <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5361\n",
      "learning rate 0.000905245\n",
      "epoch 10\n",
      "batch 400\n",
      "training minibatch loss: 3.9647748470306396\n",
      "  sample 1:\n",
      "    enc input           > omg she is awesome we re all american how great is that <EOS>\n",
      "    dec input           > clinton is trying to get this video removed watch before lawyer get involved <EOS>\n",
      "    dec train predicted > i is a to be a s <EOS> property and you s <EOS> <EOS>\n",
      "dev minibatch loss: 6.292108058929443\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > story of our life <EOS>\n",
      "    DEV dec input           > i left oakland and the sun wa out i arrive at daly city amp it s dark <EOS>\n",
      "    DEV dec train predicted > what need to and the office s in of m in the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > and the world is a great time <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5411\n",
      "learning rate 0.000904405\n",
      "epoch 10\n",
      "batch 450\n",
      "training minibatch loss: 4.1845622062683105\n",
      "  sample 1:\n",
      "    enc input           > are you in new york ? i m here for unga <EOS>\n",
      "    dec input           > the ability to have cross sector conversation and collaboration is critical to expand <EOS>\n",
      "    dec train predicted > the best i the to in section to i is a <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.015936374664307\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > pretty good <EOS>\n",
      "    DEV dec input           > how are you lady doing ? <EOS>\n",
      "    DEV dec train predicted > i you you a <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m going to be a good day <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5457\n",
      "learning rate 0.000903633\n",
      "epoch 11\n",
      "batch 0\n",
      "training minibatch loss: 3.8661749362945557\n",
      "  sample 1:\n",
      "    enc input           > lmfao omg yes <EOS>\n",
      "    dec input           > lol you this year <EOS>\n",
      "    dec train predicted > i i like is <EOS>\n",
      "dev minibatch loss: 6.68087100982666\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > are you talking about trump ? <EOS>\n",
      "    DEV dec input           > anyone else use to stare at this racist <EOS>\n",
      "    DEV dec train predicted > trump are a a be <EOS> the debate <EOS>\n",
      "    DEV dec train infer > i m a racist occurrence of trump into a racist temperament <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5507\n",
      "learning rate 0.000902794\n",
      "epoch 11\n",
      "batch 50\n",
      "training minibatch loss: 3.9145419597625732\n",
      "  sample 1:\n",
      "    enc input           > we re watching nothing without you whit so far so good <EOS>\n",
      "    dec input           > another great of see it today on amp <EOS>\n",
      "    dec train predicted > the great for the you a <EOS> the discus\n",
      "dev minibatch loss: 6.322043418884277\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we are walking through the city can t take off our clothes yet <EOS>\n",
      "    DEV dec input           > seems modest wardrobe from all the story i hear about folsom <EOS>\n",
      "    DEV dec train predicted > i a <EOS> <EOS> the i same <EOS> have a the <EOS>\n",
      "    DEV dec train infer > the latest of the thing you are not sure that s not good <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5557\n",
      "learning rate 0.000901956\n",
      "epoch 11\n",
      "batch 100\n",
      "training minibatch loss: 4.000321388244629\n",
      "  sample 1:\n",
      "    enc input           > mm in an hour holy crap <EOS>\n",
      "    dec input           > like this in i miss my rd floor apt <EOS>\n",
      "    dec train predicted > the the <EOS> the m me bear city <EOS> <EOS>\n",
      "dev minibatch loss: 6.728851795196533\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i wa on the way there before i saw youd already arrived and set up shop <EOS>\n",
      "    DEV dec input           > ty for the setup <EOS>\n",
      "    DEV dec train predicted > icymi and the admin tool\n",
      "    DEV dec train infer > q you re triggering in general <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5607\n",
      "learning rate 0.000901119\n",
      "epoch 11\n",
      "batch 150\n",
      "training minibatch loss: 4.039147853851318\n",
      "  sample 1:\n",
      "    enc input           > that s the best tweet for the night <EOS>\n",
      "    dec input           > we really cant <EOS>\n",
      "    dec train predicted > i re have be\n",
      "dev minibatch loss: 6.8047194480896\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i didn t know what to call it <EOS>\n",
      "    DEV dec input           > i don t think of the console a the dash ? <EOS>\n",
      "    DEV dec train predicted > i m t have it a same i lot world tweet i\n",
      "    DEV dec train infer > i m not a lot of a lot i m not a good time <EOS> <EOS>\n",
      "global_step: 5657\n",
      "learning rate 0.000900283\n",
      "epoch 11\n",
      "batch 200\n",
      "training minibatch loss: 4.084603309631348\n",
      "  sample 1:\n",
      "    enc input           > beautiful picture <EOS>\n",
      "    dec input           > good evening it s how could i forget ? enjoy <EOS>\n",
      "    dec train predicted > thanks morning i s <EOS> you you be <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 5.961928844451904\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the republican suggestion ha been to arm teacher <EOS>\n",
      "    DEV dec input           > don t worry trump s going to fix it all right ? <EOS>\n",
      "    DEV dec train predicted > you t have you to just to get you s time <EOS> <EOS>\n",
      "    DEV dec train infer > i m just to get a good time of the year <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5707\n",
      "learning rate 0.000899447\n",
      "epoch 11\n",
      "batch 250\n",
      "training minibatch loss: 4.095221519470215\n",
      "  sample 1:\n",
      "    enc input           > really special thank you guy <EOS>\n",
      "    dec input           > celebrate the historic opening of the by learning about the legacy of african american at the white house <EOS>\n",
      "    dec train predicted > in the new of <EOS> the world the to the new <EOS> the american <EOS> the world <EOS> <EOS>\n",
      "dev minibatch loss: 6.477579116821289\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > they also said chad kelley potentially is the best passer in america <EOS>\n",
      "    DEV dec input           > espn s fpi say florida ha a chance to beat tennessee that s funny <EOS>\n",
      "    DEV dec train predicted > this really wife beef <EOS> is really beef of see them <EOS> s really <EOS>\n",
      "    DEV dec train infer > yes of the fcs set written than the head in the door <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5757\n",
      "learning rate 0.000898613\n",
      "epoch 11\n",
      "batch 300\n",
      "training minibatch loss: 4.238305568695068\n",
      "  sample 1:\n",
      "    enc input           > that s what i said too when i saw it <EOS>\n",
      "    dec input           > it s so nice <EOS>\n",
      "    dec train predicted > i s a good <EOS>\n",
      "dev minibatch loss: 6.85348653793335\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > lol omg stop <EOS>\n",
      "    DEV dec input           > grabbed an old ski jacket out of the closet and found a butterscotch candy in me pocket <EOS>\n",
      "    DEV dec train predicted > wait to old taste <EOS> <EOS> <EOS> <EOS> phone <EOS> i <EOS> mouth <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m not amazed in the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5807\n",
      "learning rate 0.000897779\n",
      "epoch 11\n",
      "batch 350\n",
      "training minibatch loss: 3.96785831451416\n",
      "  sample 1:\n",
      "    enc input           > man lol livermore suck on the weekday oakland is way better <EOS>\n",
      "    dec input           > try ubering at livermore bart if u haven t already <EOS>\n",
      "    dec train predicted > britney riding af livermore nd <EOS> you re t rolled at\n",
      "dev minibatch loss: 6.262421131134033\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > some couple should come with an off switch <EOS>\n",
      "    DEV dec input           > didn t see context so i read that pair a meaning romantic couple <EOS>\n",
      "    DEV dec train predicted > i are have the for much m to i is long of <EOS> <EOS>\n",
      "    DEV dec train infer > i m just just just a long thing that is a long week <EOS> <EOS> <EOS>\n",
      "global_step: 5857\n",
      "learning rate 0.000896946\n",
      "epoch 11\n",
      "batch 400\n",
      "training minibatch loss: 4.125178337097168\n",
      "  sample 1:\n",
      "    enc input           > also a few of tax change that primarily deal with investment income here are detail analysis <EOS>\n",
      "    dec input           > doe that mean a tax increase too ? ? i can t afford that <EOS>\n",
      "    dec train predicted > is what matter a hardest fraud <EOS> <EOS> <EOS> <EOS> think t pay <EOS> <EOS>\n",
      "dev minibatch loss: 6.2004618644714355\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > trumpsplaining deliver birtherism x yr minus birtherism x hillary no apology needed <EOS>\n",
      "    DEV dec input           > fact check false like really false <EOS>\n",
      "    DEV dec train predicted > i the the s the the <EOS>\n",
      "    DEV dec train infer > the man s a hell of the truth of the truth it s in the <EOS>\n",
      "global_step: 5907\n",
      "learning rate 0.000896113\n",
      "epoch 11\n",
      "batch 450\n",
      "training minibatch loss: 4.081057071685791\n",
      "  sample 1:\n",
      "    enc input           > why postpone ? ? ? more cowardly congressman being bought off ? <EOS>\n",
      "    dec input           > gop leader postpone irs commissioner impeachment plan until after november election <EOS>\n",
      "    dec train predicted > west congressional postpone systematically rahm impeachment charity on jihad immigrant west <EOS>\n",
      "dev minibatch loss: 6.702469348907471\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > also someone mite put in effort for the wrong reason <EOS>\n",
      "    DEV dec input           > effort is the best indicator of interest paul carrick brunson <EOS>\n",
      "    DEV dec train predicted > i in a closest thing <EOS> the s s <EOS> <EOS>\n",
      "    DEV dec train infer > where are i do the electric trade <EOS> ? <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 5953\n",
      "learning rate 0.000895348\n",
      "epoch 12\n",
      "batch 0\n",
      "training minibatch loss: 3.7420341968536377\n",
      "  sample 1:\n",
      "    enc input           > literally same but i m so bored idk what s worse lol <EOS>\n",
      "    dec input           > work is so boring today and after last week i am so damn thankful for that <EOS>\n",
      "    dec train predicted > i is a much <EOS> <EOS> i my day i m <EOS> much i <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 5.844110488891602\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > right back at ya babe <EOS>\n",
      "    DEV dec input           > since i know you re a few cup deep already <EOS>\n",
      "    DEV dec train predicted > is i think i need gone parallel as is is <EOS>\n",
      "    DEV dec train infer > t give me a parallel bigger <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6003\n",
      "learning rate 0.000894517\n",
      "epoch 12\n",
      "batch 50\n",
      "training minibatch loss: 3.701902151107788\n",
      "  sample 1:\n",
      "    enc input           > i control what i want to do to you <EOS>\n",
      "    dec input           > do it then i want your mouth on me <EOS>\n",
      "    dec train predicted > i you s <EOS> have you own <EOS> you <EOS>\n",
      "dev minibatch loss: 6.783024787902832\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > add the total from the male african american column divide unknown by proportion add to equation <EOS>\n",
      "    DEV dec input           > that wa a legit question <EOS>\n",
      "    DEV dec train predicted > new s the new bombing in\n",
      "    DEV dec train infer > the bomb in bombing of bomb crush news of the reuters bombing suspect in syria <EOS> <EOS>\n",
      "global_step: 6053\n",
      "learning rate 0.000893687\n",
      "epoch 12\n",
      "batch 100\n",
      "training minibatch loss: 3.830366373062134\n",
      "  sample 1:\n",
      "    enc input           > harrison would have walked out on him st day <EOS>\n",
      "    dec input           > harrison ford would be great with hitch <EOS>\n",
      "    dec train predicted > hitch ford i be a <EOS> hitch <EOS>\n",
      "dev minibatch loss: 6.404834747314453\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > no i m saying the exact opposite <EOS>\n",
      "    DEV dec input           > so if yall hit u it s alright ? <EOS>\n",
      "    DEV dec train predicted > the a you know the <EOS> s a <EOS> <EOS>\n",
      "    DEV dec train infer > i m cryin of the same <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6103\n",
      "learning rate 0.000892858\n",
      "epoch 12\n",
      "batch 150\n",
      "training minibatch loss: 3.8441379070281982\n",
      "  sample 1:\n",
      "    enc input           > curious a to whether these number reflect self identifying jew or what what s a jew ? <EOS>\n",
      "    dec input           > jewish population million u israel france canada argentina russia <EOS>\n",
      "    dec train predicted > population population george israel knicks george law resonate by <EOS>\n",
      "dev minibatch loss: 7.106463432312012\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > u mean pregnant with his child again ? <EOS>\n",
      "    DEV dec input           > omg omg omg omg the girl who s baby father punched her in the stomach is pregnant <EOS>\n",
      "    DEV dec train predicted > i i i i <EOS> hell <EOS> wa a <EOS> <EOS> <EOS> <EOS> <EOS> same <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i think i think you re a good song <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6153\n",
      "learning rate 0.000892029\n",
      "epoch 12\n",
      "batch 200\n",
      "training minibatch loss: 3.8012378215789795\n",
      "  sample 1:\n",
      "    enc input           > it wa shameful to watch those question happens the people that asked them should feel ashamed <EOS>\n",
      "    dec input           > and no one care about your electric bus <EOS>\n",
      "    dec train predicted > and it time just of the screw area <EOS>\n",
      "dev minibatch loss: 6.858947277069092\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i think the whole off bway prod might be on youtube <EOS>\n",
      "    DEV dec input           > my kingdom for a ticket to this production <EOS>\n",
      "    DEV dec train predicted > i preference is a lot that tag <EOS> <EOS>\n",
      "    DEV dec train infer > ok i think it s blasphemous in the millenials or the box <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 6203\n",
      "learning rate 0.000891201\n",
      "epoch 12\n",
      "batch 250\n",
      "training minibatch loss: 3.9853036403656006\n",
      "  sample 1:\n",
      "    enc input           > i subscribed but now i m slightly worried that i m effectively vendor locked <EOS>\n",
      "    dec input           > both for me i refuse to use the subscription service i think i m being punished <EOS>\n",
      "    dec train predicted > also i the <EOS> m to use the same x and m it ve to punished <EOS>\n",
      "dev minibatch loss: 6.0478339195251465\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > and your english teacher certainly failed you <EOS>\n",
      "    DEV dec input           > hillary s been failing for year in not getting the job done it will never change <EOS>\n",
      "    DEV dec train predicted > i is a a and the and the a a whole like <EOS> <EOS> be be <EOS>\n",
      "    DEV dec train infer > and i m a chance to bed and people like a flag <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6253\n",
      "learning rate 0.000890374\n",
      "epoch 12\n",
      "batch 300\n",
      "training minibatch loss: 3.938523769378662\n",
      "  sample 1:\n",
      "    enc input           > the purge mcdonalds <EOS>\n",
      "    dec input           > im on a pilgrimage amp no one can stop me <EOS>\n",
      "    dec train predicted > i a the good <EOS> i <EOS> <EOS> i the <EOS>\n",
      "dev minibatch loss: 6.512320041656494\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > pretty good <EOS>\n",
      "    DEV dec input           > how are you lady doing ? <EOS>\n",
      "    DEV dec train predicted > what did you in <EOS> ? <EOS>\n",
      "    DEV dec train infer > the new thing you re a bit <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6303\n",
      "learning rate 0.000889548\n",
      "epoch 12\n",
      "batch 350\n",
      "training minibatch loss: 3.831071376800537\n",
      "  sample 1:\n",
      "    enc input           > probably trying to humanize himself and not being seen a sexist <EOS>\n",
      "    dec input           > what wa that ? wa he trying to be funny ? <EOS>\n",
      "    dec train predicted > he s the s ? the afterwards to be the he <EOS>\n",
      "dev minibatch loss: 6.923548221588135\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > may and also twitter please put it to where i can edit tweet not have to delete them <EOS>\n",
      "    DEV dec input           > yes please also why did you move setup if i kay ask ? <EOS>\n",
      "    DEV dec train predicted > i i i it wa you have to for you have to it <EOS>\n",
      "    DEV dec train infer > i just imagined just seen the space <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6353\n",
      "learning rate 0.000888722\n",
      "epoch 12\n",
      "batch 400\n",
      "training minibatch loss: 4.026056289672852\n",
      "  sample 1:\n",
      "    enc input           > link source for tw <EOS>\n",
      "    dec input           > chelsea manhattan explosion causing injury reported by local amp link in reply <EOS>\n",
      "    dec train predicted > new birth stabbed a york amp <EOS> columbus amp the to the <EOS>\n",
      "dev minibatch loss: 6.385092735290527\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > she s biting her tongue so hard good the office reference <EOS>\n",
      "    DEV dec input           > hillary is straight up jim halpert right now <EOS>\n",
      "    DEV dec train predicted > she stein a a to bite in now <EOS>\n",
      "    DEV dec train infer > kim stein is a withering bigot <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6403\n",
      "learning rate 0.000887898\n",
      "epoch 12\n",
      "batch 450\n",
      "training minibatch loss: 4.0870280265808105\n",
      "  sample 1:\n",
      "    enc input           > you re a babe stop <EOS>\n",
      "    dec input           > my biggest regret in life is that i m too old and too ugly to appear on <EOS>\n",
      "    dec train predicted > i first friend friend the <EOS> a i m just like <EOS> i <EOS> <EOS> be to <EOS>\n",
      "dev minibatch loss: 6.344791889190674\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > bitch it cause i out chopra in ur name <EOS>\n",
      "    DEV dec input           > bitch thats a lot of emojis shit i dont feel special jkjk <EOS>\n",
      "    DEV dec train predicted > is <EOS> the the <EOS> the <EOS> <EOS> m have like <EOS> <EOS>\n",
      "    DEV dec train infer > i m just confused <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 6449\n",
      "learning rate 0.00088714\n",
      "epoch 13\n",
      "batch 0\n",
      "training minibatch loss: 3.70263671875\n",
      "  sample 1:\n",
      "    enc input           > roger mcdonough <EOS>\n",
      "    dec input           > what wa that judge name ? <EOS>\n",
      "    dec train predicted > what s a poll <EOS> ? <EOS>\n",
      "dev minibatch loss: 6.851149559020996\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > delete your career chuck todd <EOS>\n",
      "    DEV dec input           > next time someone question hillary s stamen show them this photo <EOS>\n",
      "    DEV dec train predicted > i course i to about to systematically to <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i don t know i m a a rat <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6499\n",
      "learning rate 0.000886316\n",
      "epoch 13\n",
      "batch 50\n",
      "training minibatch loss: 3.5406951904296875\n",
      "  sample 1:\n",
      "    enc input           > why are u still at taco bell <EOS>\n",
      "    dec input           > wearing my yeezy season to flex on the cashier at taco bell rn <EOS>\n",
      "    dec train predicted > until me life game i cry in the cashier i month zone salad <EOS>\n",
      "dev minibatch loss: 6.484242916107178\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > no it mean we ll probably be getting more selfies of him <EOS>\n",
      "    DEV dec input           > he s not coming back on ig ? <EOS>\n",
      "    DEV dec train predicted > we s not a in <EOS> the ? <EOS>\n",
      "    DEV dec train infer > who s not important for the inner anthem ? ? ? ? <EOS> ? <EOS>\n",
      "global_step: 6549\n",
      "learning rate 0.000885494\n",
      "epoch 13\n",
      "batch 100\n",
      "training minibatch loss: 3.6962058544158936\n",
      "  sample 1:\n",
      "    enc input           > oh good lord <EOS>\n",
      "    dec input           > don t start on fat people <EOS>\n",
      "    dec train predicted > is t be a the <EOS> <EOS>\n",
      "dev minibatch loss: 6.931687355041504\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > show me this vine anytime i tweet i m not going to the gym <EOS>\n",
      "    DEV dec input           > take me lord for i am worthy <EOS>\n",
      "    DEV dec train predicted > i you for you the have at at\n",
      "    DEV dec train infer > uh i m a disaster for the other of the gem who m at the world <EOS>\n",
      "global_step: 6599\n",
      "learning rate 0.000884672\n",
      "epoch 13\n",
      "batch 150\n",
      "training minibatch loss: 3.7462167739868164\n",
      "  sample 1:\n",
      "    enc input           > what if we just became another province ? or we could consolidate ? <EOS>\n",
      "    dec input           > sorry no way we canadian are giving him up <EOS>\n",
      "    dec train predicted > rep it redirect i have t cory me <EOS> <EOS>\n",
      "dev minibatch loss: 6.705856800079346\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > it didnt matter i mean this kid completed of his pass never threw an int <EOS>\n",
      "    DEV dec input           > pennington had worst arm to iq ratio ever great mind but arm wasn t there liked him alot <EOS>\n",
      "    DEV dec train predicted > you is a honest to talk <EOS> <EOS> <EOS> <EOS> <EOS> ? is t like ? <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > how doe you think about the most extra sooner apology to you ? <EOS> <EOS> <EOS>\n",
      "global_step: 6649\n",
      "learning rate 0.000883851\n",
      "epoch 13\n",
      "batch 200\n",
      "training minibatch loss: 3.8682990074157715\n",
      "  sample 1:\n",
      "    enc input           > it a wednesday <EOS>\n",
      "    dec input           > ah but february is a tuesday <EOS>\n",
      "    dec train predicted > this i i is my good <EOS>\n",
      "dev minibatch loss: 7.159985065460205\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > colin powell real housewife of the potomac ? <EOS>\n",
      "    DEV dec input           > powell will be going to fewer dinner party in washington <EOS>\n",
      "    DEV dec train predicted > i i be a to avoid <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > that s the way that shit <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6699\n",
      "learning rate 0.000883031\n",
      "epoch 13\n",
      "batch 250\n",
      "training minibatch loss: 3.844827175140381\n",
      "  sample 1:\n",
      "    enc input           > get out of my mention with this <EOS>\n",
      "    dec input           > hell yeah d <EOS>\n",
      "    dec train predicted > my shit <EOS> <EOS>\n",
      "dev minibatch loss: 6.727499008178711\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > brand new tune <EOS>\n",
      "    DEV dec input           > rihanna you da one sur radio life coute <EOS>\n",
      "    DEV dec train predicted > who annoiting are plan on bewertet bewertet to a\n",
      "    DEV dec train infer > now you are ready to promote the gospel of the uk <EOS> <EOS> <EOS>\n",
      "global_step: 6749\n",
      "learning rate 0.000882211\n",
      "epoch 13\n",
      "batch 300\n",
      "training minibatch loss: 3.7795915603637695\n",
      "  sample 1:\n",
      "    enc input           > i know like no fair he really seems to be enjoying europe more then north america <EOS>\n",
      "    dec input           > yes i wa cry so much <EOS>\n",
      "    dec train predicted > i shit love so <EOS> much <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 7.019731521606445\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > software is eating predatory lending aka this time is different <EOS>\n",
      "    DEV dec input           > software is eating discrimination <EOS>\n",
      "    DEV dec train predicted > i is what a <EOS>\n",
      "    DEV dec train infer > i m confused <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6799\n",
      "learning rate 0.000881392\n",
      "epoch 13\n",
      "batch 350\n",
      "training minibatch loss: 3.53167462348938\n",
      "  sample 1:\n",
      "    enc input           > c mon m flower doesn t belong w the scum of the earth by a long shot <EOS>\n",
      "    dec input           > at least gennifer flower will be in good company next to mark cuban <EOS>\n",
      "    dec train predicted > and least the flower ha be voting the with there of the cuban <EOS>\n",
      "dev minibatch loss: 6.704638957977295\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > nothing to do with smart everything to do with money and marketing <EOS>\n",
      "    DEV dec input           > brilliant and proof that we aren t getting any smarter are we ? <EOS>\n",
      "    DEV dec train predicted > it and i it a re t a to of to a in <EOS>\n",
      "    DEV dec train infer > it s a problem friction not a younger <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6849\n",
      "learning rate 0.000880575\n",
      "epoch 13\n",
      "batch 400\n",
      "training minibatch loss: 3.7961080074310303\n",
      "  sample 1:\n",
      "    enc input           > go crawl back in ur rat hole u dirty communist <EOS>\n",
      "    dec input           > so u like gun then is what ya saying <EOS>\n",
      "    dec train predicted > hey you are the <EOS> <EOS> it you <EOS> <EOS>\n",
      "dev minibatch loss: 6.800251007080078\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i heard trump s pound hacker love it tho ? <EOS>\n",
      "    DEV dec input           > is once again being defined by failed promise and a lack of option <EOS>\n",
      "    DEV dec train predicted > i it a <EOS> a it the <EOS> to i good of th <EOS>\n",
      "    DEV dec train infer > i miss you <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6899\n",
      "learning rate 0.000879757\n",
      "epoch 13\n",
      "batch 450\n",
      "training minibatch loss: 3.8555309772491455\n",
      "  sample 1:\n",
      "    enc input           > only one l have love it <EOS>\n",
      "    dec input           > that s a good one <EOS>\n",
      "    dec train predicted > i s a good day <EOS>\n",
      "dev minibatch loss: 6.505393981933594\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > go to the beach <EOS>\n",
      "    DEV dec input           > bc it s thursday everyone s at school or work <EOS>\n",
      "    DEV dec train predicted > i i s a <EOS> in cheapest <EOS> night the <EOS>\n",
      "    DEV dec train infer > i am not a boyfriend <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6945\n",
      "learning rate 0.000879006\n",
      "epoch 14\n",
      "batch 0\n",
      "training minibatch loss: 3.4117684364318848\n",
      "  sample 1:\n",
      "    enc input           > not pb should be off the air for promoting with their frontline broadcast <EOS>\n",
      "    dec input           > since fox suspended andrea tantaros for unauthorized book promo shld t be suspended for unauthorized trump promo ? <EOS>\n",
      "    dec train predicted > did colin suspended andrea tantaros for unauthorized flower promo shld we be suspended for unauthorized flower promo <EOS> <EOS>\n",
      "dev minibatch loss: 6.803926467895508\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > so fucking hot <EOS>\n",
      "    DEV dec input           > great morning for a public pant flash <EOS>\n",
      "    DEV dec train predicted > beef jimin <EOS> the car egg <EOS> in\n",
      "    DEV dec train infer > i m playing on the falcon <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 6995\n",
      "learning rate 0.000878191\n",
      "epoch 14\n",
      "batch 50\n",
      "training minibatch loss: 3.5820462703704834\n",
      "  sample 1:\n",
      "    enc input           > oh my gosh it wa i am still high <EOS>\n",
      "    dec input           > ha i would ve froze too so happy for you what a night <EOS>\n",
      "    dec train predicted > is you m you been like good much <EOS> you <EOS> ? fuck <EOS>\n",
      "dev minibatch loss: 7.205031394958496\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > keeping it real on what we say versus what we do <EOS>\n",
      "    DEV dec input           > hah love this too <EOS>\n",
      "    DEV dec train predicted > the to the image <EOS>\n",
      "    DEV dec train infer > i think he s fighting the same of skirt of the bathroom image <EOS>\n",
      "global_step: 7045\n",
      "learning rate 0.000877375\n",
      "epoch 14\n",
      "batch 100\n",
      "training minibatch loss: 3.5359084606170654\n",
      "  sample 1:\n",
      "    enc input           > it is it s my first flight and first time here <EOS>\n",
      "    dec input           > it s so big i need to get out more enjoy your time <EOS>\n",
      "    dec train predicted > i s so good <EOS> just to be out of and this game <EOS>\n",
      "dev minibatch loss: 6.062431335449219\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > thank you me too <EOS>\n",
      "    DEV dec input           > thank you for the information <EOS>\n",
      "    DEV dec train predicted > happy you <EOS> you best <EOS>\n",
      "    DEV dec train infer > happy birthday darius <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7095\n",
      "learning rate 0.000876561\n",
      "epoch 14\n",
      "batch 150\n",
      "training minibatch loss: 3.5937585830688477\n",
      "  sample 1:\n",
      "    enc input           > here is ur homework now is good time to subscribe to troma now st mo free <EOS>\n",
      "    dec input           > okay thank you i will send it to them a well <EOS>\n",
      "    dec train predicted > i i you for m get it to help <EOS> good <EOS>\n",
      "dev minibatch loss: 7.262205123901367\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > they are gon na release a patch when ever they have everything sorted duh <EOS>\n",
      "    DEV dec input           > what patch ? ? <EOS>\n",
      "    DEV dec train predicted > is many is the i\n",
      "    DEV dec train infer > when many people are not a lot of fashion graphic <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7145\n",
      "learning rate 0.000875748\n",
      "epoch 14\n",
      "batch 200\n",
      "training minibatch loss: 3.5015759468078613\n",
      "  sample 1:\n",
      "    enc input           > new music jungleboy go crazy prod by juneonnabeat <EOS>\n",
      "    dec input           > long live <EOS>\n",
      "    dec train predicted > you studio to\n",
      "dev minibatch loss: 7.661449432373047\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > she is a saint are you intentionally trying to insult catholic ? <EOS>\n",
      "    DEV dec input           > mother teresa a myth a celebrity or a hero ? <EOS>\n",
      "    DEV dec train predicted > hillary of is trump campaign historic campaign she historic of <EOS>\n",
      "    DEV dec train infer > alicia debate hillary is hillary hillary clinton <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7195\n",
      "learning rate 0.000874935\n",
      "epoch 14\n",
      "batch 250\n",
      "training minibatch loss: 3.507828950881958\n",
      "  sample 1:\n",
      "    enc input           > think we can all agree he s mine but have ur fun <EOS>\n",
      "    dec input           > fall back he mine <EOS>\n",
      "    dec train predicted > i to that got <EOS>\n",
      "dev minibatch loss: 6.870209217071533\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > fyi they look like boob <EOS>\n",
      "    DEV dec input           > i have bumped down this price so many time pls someone just fuckin buy it <EOS>\n",
      "    DEV dec train predicted > the bet a on for is won much <EOS> <EOS> <EOS> should ever a <EOS> <EOS>\n",
      "    DEV dec train infer > the gq of the people of a big team test <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7245\n",
      "learning rate 0.000874123\n",
      "epoch 14\n",
      "batch 300\n",
      "training minibatch loss: 3.477863073348999\n",
      "  sample 1:\n",
      "    enc input           > todd is the best <EOS>\n",
      "    dec input           > knocking my sock off amp learning to present like a <EOS>\n",
      "    dec train predicted > new the favorite <EOS> <EOS> a to the <EOS> the new\n",
      "dev minibatch loss: 6.939886569976807\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > rewarding hard work <EOS>\n",
      "    DEV dec input           > i m remember those day <EOS>\n",
      "    DEV dec train predicted > ok m asking of asking to\n",
      "    DEV dec train infer > wishing you like the bestest report who re a great time <EOS> <EOS> <EOS>\n",
      "global_step: 7295\n",
      "learning rate 0.000873312\n",
      "epoch 14\n",
      "batch 350\n",
      "training minibatch loss: 3.7859933376312256\n",
      "  sample 1:\n",
      "    enc input           > ohhhhhhh i see he tryna get the kobe bryant last tour treatment lmao foh <EOS>\n",
      "    dec input           > lmao why do nigga not care about paul pierce retiring ? <EOS>\n",
      "    dec train predicted > oh he do you have wait <EOS> him pierce retiring <EOS> <EOS>\n",
      "dev minibatch loss: 7.661400318145752\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > kind of funny stein literally disappeared from ny politics in <EOS>\n",
      "    DEV dec input           > doing a radio interview where they mention nyc council so alert popped <EOS>\n",
      "    DEV dec train predicted > i it fuck is <EOS> i re <EOS> <EOS> ? <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > this is not a rat <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 7345\n",
      "learning rate 0.000872501\n",
      "epoch 14\n",
      "batch 400\n",
      "training minibatch loss: 3.889765977859497\n",
      "  sample 1:\n",
      "    enc input           > you will be missed <EOS>\n",
      "    dec input           > turn out i m probably not running away and joining the but you can this weekend story <EOS>\n",
      "    dec train predicted > i out the m a a a in and the to debate i can be s <EOS> <EOS>\n",
      "dev minibatch loss: 6.994228363037109\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > bruh i already decided to sleep in my first class <EOS>\n",
      "    DEV dec input           > it doesnt matter how much sleep i get im always tired <EOS>\n",
      "    DEV dec train predicted > i s suck to i i <EOS> m to so to and\n",
      "    DEV dec train infer > i m so glad i miss you to see you <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7395\n",
      "learning rate 0.000871692\n",
      "epoch 14\n",
      "batch 450\n",
      "training minibatch loss: 3.7803585529327393\n",
      "  sample 1:\n",
      "    enc input           > agree if his salary cap hit wa k he probably would ve been booted <EOS>\n",
      "    dec input           > didn t they almost let the heart and soul of the organization walk this summer ? <EOS>\n",
      "    dec train predicted > i t you have be all best of summer to the best i the week <EOS> <EOS>\n",
      "dev minibatch loss: 7.302970886230469\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we were outside for awhile and saw paul and todd wish we would have known you were there <EOS>\n",
      "    DEV dec input           > it wa not a traditional wedding reception asbury park yacht club <EOS>\n",
      "    DEV dec train predicted > i out a a professional <EOS> i to to ? <EOS> <EOS>\n",
      "    DEV dec train infer > i m so good at my snap haha <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7441\n",
      "learning rate 0.000870947\n",
      "epoch 15\n",
      "batch 0\n",
      "training minibatch loss: 3.374021530151367\n",
      "  sample 1:\n",
      "    enc input           > lol true story <EOS>\n",
      "    dec input           > how is it already week ? ? ? <EOS>\n",
      "    dec train predicted > i is a <EOS> <EOS> <EOS> i ? i\n",
      "dev minibatch loss: 6.318314075469971\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what book are you reading ? <EOS>\n",
      "    DEV dec input           > what that book collection do ? <EOS>\n",
      "    DEV dec train predicted > the many s is is you <EOS>\n",
      "    DEV dec train infer > the result of the bronx <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7491\n",
      "learning rate 0.000870139\n",
      "epoch 15\n",
      "batch 50\n",
      "training minibatch loss: 3.180351495742798\n",
      "  sample 1:\n",
      "    enc input           > i wa trying to say that without typing it out <EOS>\n",
      "    dec input           > hacker news <EOS>\n",
      "    dec train predicted > i news is\n",
      "dev minibatch loss: 7.313443660736084\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i think purple guard work on it but i haven t tried it <EOS>\n",
      "    DEV dec input           > yeah it s newer <EOS>\n",
      "    DEV dec train predicted > i you are a you\n",
      "    DEV dec train infer > i thought you re a great time to be summed up to work <EOS>\n",
      "global_step: 7541\n",
      "learning rate 0.000869332\n",
      "epoch 15\n",
      "batch 100\n",
      "training minibatch loss: 3.3568921089172363\n",
      "  sample 1:\n",
      "    enc input           > you must be joking incoherent rambling embarrassing nonsense on national security issue <EOS>\n",
      "    dec input           > trump blew her away <EOS>\n",
      "    dec train predicted > xenophobic appears her lying for\n",
      "dev minibatch loss: 7.489686965942383\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that would actually be a pretty good tagline for the trucking startup uber just bought <EOS>\n",
      "    DEV dec input           > more like the rig economy a in it s rigged <EOS>\n",
      "    DEV dec train predicted > bbc alert a york amp <EOS> area netflix s swimming swimming\n",
      "    DEV dec train infer > founder alert founder are due to see the emergency project <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7591\n",
      "learning rate 0.000868525\n",
      "epoch 15\n",
      "batch 150\n",
      "training minibatch loss: 3.413120746612549\n",
      "  sample 1:\n",
      "    enc input           > hahahaha priceless <EOS>\n",
      "    dec input           > mayor de blasio trying to double talk his way out of another failure <EOS>\n",
      "    dec train predicted > trump s blasio trump to protect insult trump flower in <EOS> the sean <EOS>\n",
      "dev minibatch loss: 7.136605262756348\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > it s not what they want to begin with <EOS>\n",
      "    DEV dec input           > some people rather lose the whole relationship than to fix the problem at hand <EOS>\n",
      "    DEV dec train predicted > do is are speak to ten pro ? you get sense ten ? <EOS> ?\n",
      "    DEV dec train infer > i m not about ten iron pro leak allows ? ? he wasn t promoting ? <EOS>\n",
      "global_step: 7641\n",
      "learning rate 0.000867719\n",
      "epoch 15\n",
      "batch 200\n",
      "training minibatch loss: 3.6140549182891846\n",
      "  sample 1:\n",
      "    enc input           > thanks but it doesnt come with air <EOS>\n",
      "    dec input           > good luck i know you love to travel <EOS>\n",
      "    dec train predicted > happy morning you love you re you see <EOS>\n",
      "dev minibatch loss: 6.852997303009033\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > my nigga the only purple pickle she gettin come with her free meal from grace <EOS>\n",
      "    DEV dec input           > give her the purple pickle <EOS>\n",
      "    DEV dec train predicted > why the a stage <EOS> <EOS>\n",
      "    DEV dec train infer > hillary is a racist bigot is a racist <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7691\n",
      "learning rate 0.000866914\n",
      "epoch 15\n",
      "batch 250\n",
      "training minibatch loss: 3.373358964920044\n",
      "  sample 1:\n",
      "    enc input           > minute in the morning minute in the evening <EOS>\n",
      "    dec input           > meditation is crucial i tell you <EOS>\n",
      "    dec train predicted > meditation is the <EOS> am me deposit\n",
      "dev minibatch loss: 7.014916896820068\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > tyvm the amp the <EOS>\n",
      "    DEV dec input           > tyvm the <EOS>\n",
      "    DEV dec train predicted > tyvm the tyvm\n",
      "    DEV dec train infer > tyvm the dresser <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7741\n",
      "learning rate 0.000866109\n",
      "epoch 15\n",
      "batch 300\n",
      "training minibatch loss: 3.576594114303589\n",
      "  sample 1:\n",
      "    enc input           > she got them from the martian everybody know your tin hat need adjustment <EOS>\n",
      "    dec input           > source say hillary had debate question week before debate intern seen delivering package to office <EOS>\n",
      "    dec train predicted > fox campaign crook clinton a attack a is a clinton host bribe a a bribe <EOS>\n",
      "dev minibatch loss: 7.393248558044434\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > tight i really started gronk over ebron <EOS>\n",
      "    DEV dec input           > we suck well at least compared to pat <EOS>\n",
      "    DEV dec train predicted > first need in ci the to to smile <EOS>\n",
      "    DEV dec train infer > unfortunately much <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7791\n",
      "learning rate 0.000865305\n",
      "epoch 15\n",
      "batch 350\n",
      "training minibatch loss: 3.475569009780884\n",
      "  sample 1:\n",
      "    enc input           > so you more petty than before <EOS>\n",
      "    dec input           > i changed a lot dj khaled voice <EOS>\n",
      "    dec train predicted > i love my worst of khaled khaled <EOS>\n",
      "dev minibatch loss: 6.885459899902344\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that kid close deal <EOS>\n",
      "    DEV dec input           > saw this today amp now my perception of the term young professional is screwed up <EOS>\n",
      "    DEV dec train predicted > jesus the of in the <EOS> rnc ? a playoff of jesus is <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > they re sued of a physical lb of damon claim that ? <EOS> <EOS> <EOS>\n",
      "global_step: 7841\n",
      "learning rate 0.000864502\n",
      "epoch 15\n",
      "batch 400\n",
      "training minibatch loss: 3.6247177124023438\n",
      "  sample 1:\n",
      "    enc input           > monthly time ha arrived but it hasnt been this bad before <EOS>\n",
      "    dec input           > wh why are you in pain ? <EOS>\n",
      "    dec train predicted > and the ? you in the of <EOS>\n",
      "dev minibatch loss: 6.900514602661133\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > unworthy of retort <EOS>\n",
      "    DEV dec input           > don t blame me blame obama <EOS>\n",
      "    DEV dec train predicted > why t support the to the <EOS>\n",
      "    DEV dec train infer > which voight somehow support you re hiding critical to the view <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7891\n",
      "learning rate 0.0008637\n",
      "epoch 15\n",
      "batch 450\n",
      "training minibatch loss: 3.710360527038574\n",
      "  sample 1:\n",
      "    enc input           > we need more then that <EOS>\n",
      "    dec input           > candy for jet fan post gam <EOS>\n",
      "    dec train predicted > the of gross <EOS> designer <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 6.769272327423096\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > awww i felt the same way when he followed me <EOS>\n",
      "    DEV dec input           > yes but it wasn t but honestly i wasn t ready like at all <EOS>\n",
      "    DEV dec train predicted > hey i i s t a i i m t a <EOS> we the <EOS>\n",
      "    DEV dec train infer > oh i m so proud of the week i m so excited i m so excited <EOS>\n",
      "Saving session\n",
      "global_step: 7937\n",
      "learning rate 0.000862963\n",
      "epoch 16\n",
      "batch 0\n",
      "training minibatch loss: 3.1869237422943115\n",
      "  sample 1:\n",
      "    enc input           > okayyyyy done <EOS>\n",
      "    dec input           > just sue them <EOS>\n",
      "    dec train predicted > s apologize the ?\n",
      "dev minibatch loss: 6.449753761291504\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i see what you did there <EOS>\n",
      "    DEV dec input           > with every makeup product you buy me you are helping me reach the goal <EOS>\n",
      "    DEV dec train predicted > i you the i i have the <EOS> <EOS> going <EOS> <EOS> <EOS> same <EOS>\n",
      "    DEV dec train infer > i m going to the antler and i m strap <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 7987\n",
      "learning rate 0.000862162\n",
      "epoch 16\n",
      "batch 50\n",
      "training minibatch loss: 3.3334991931915283\n",
      "  sample 1:\n",
      "    enc input           > listen to solange new album <EOS>\n",
      "    dec input           > i m pretty awake i might a well clean up <EOS>\n",
      "    dec train predicted > i m looking much this m be much <EOS> and <EOS>\n",
      "dev minibatch loss: 7.420138359069824\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > colin powell real housewife of the potomac ? <EOS>\n",
      "    DEV dec input           > powell will be going to fewer dinner party in washington <EOS>\n",
      "    DEV dec train predicted > sitter are the the to avoid the <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > fuck the way that s a goat <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8037\n",
      "learning rate 0.000861362\n",
      "epoch 16\n",
      "batch 100\n",
      "training minibatch loss: 3.358841896057129\n",
      "  sample 1:\n",
      "    enc input           > we achieved back to back loss season frack <EOS>\n",
      "    dec input           > first time all season that i don t care if the lose <EOS>\n",
      "    dec train predicted > good day <EOS> of <EOS> <EOS> m t think <EOS> it same <EOS>\n",
      "dev minibatch loss: 7.269283771514893\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > it s really clear now you do not like paige <EOS>\n",
      "    DEV dec input           > tie her arm and leg together and make her do the stag run she won t last long <EOS>\n",
      "    DEV dec train predicted > i i ahhh is i not <EOS> i me phone a disproves <EOS> <EOS> <EOS> you be <EOS> <EOS>\n",
      "    DEV dec train infer > i m not dead that wa a bunch of the shit <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8087\n",
      "learning rate 0.000860562\n",
      "epoch 16\n",
      "batch 150\n",
      "training minibatch loss: 3.2594711780548096\n",
      "  sample 1:\n",
      "    enc input           > we were comparing them and i showed why bush is even worse than trump <EOS>\n",
      "    dec input           > bush isn t on the ballot <EOS>\n",
      "    dec train predicted > why is t true the fact <EOS>\n",
      "dev minibatch loss: 6.8715667724609375\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > if it s just a nerve pinch it can cause burning and numbness it s obviously impactful <EOS>\n",
      "    DEV dec input           > i just googled ulnar nerve it s the funny bone i m not laughing <EOS>\n",
      "    DEV dec train predicted > doe think call him to ? s claim birther ? is can processed blurt <EOS>\n",
      "    DEV dec train infer > doe it mean obama s gum in california pitbulls over his senator is plausible <EOS> <EOS> <EOS>\n",
      "global_step: 8137\n",
      "learning rate 0.000859764\n",
      "epoch 16\n",
      "batch 200\n",
      "training minibatch loss: 3.2937586307525635\n",
      "  sample 1:\n",
      "    enc input           > it s a wrap for old weeknd in the new starboy video <EOS>\n",
      "    dec input           > watch the weeknd s starboy video right now <EOS>\n",
      "    dec train predicted > the the weeknd are starboy leader in now <EOS>\n",
      "dev minibatch loss: 6.619020462036133\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > at home wow <EOS>\n",
      "    DEV dec input           > my mom wa a teacher and she had one at home and i m still sane <EOS>\n",
      "    DEV dec train predicted > do goodness is a streak to drink s ta weather my <EOS> then ve <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > where do you give ? <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8187\n",
      "learning rate 0.000858966\n",
      "epoch 16\n",
      "batch 250\n",
      "training minibatch loss: 3.147789478302002\n",
      "  sample 1:\n",
      "    enc input           > he would ve called you more name but his mom called him upstairs for dinner <EOS>\n",
      "    dec input           > someone just called me a white trash neoliberal dumbfuck right before blocking me lol <EOS>\n",
      "    dec train predicted > andy else misspelled me a hypothetical neoliberal neoliberal dumbfuck down now blocking blocking <EOS> <EOS>\n",
      "dev minibatch loss: 7.752960681915283\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > lawsuit brother <EOS>\n",
      "    DEV dec input           > damn near dude i fucking hate mustard <EOS>\n",
      "    DEV dec train predicted > desiigner the s we want have the <EOS>\n",
      "    DEV dec train infer > morning gregg <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8237\n",
      "learning rate 0.000858169\n",
      "epoch 16\n",
      "batch 300\n",
      "training minibatch loss: 3.451828718185425\n",
      "  sample 1:\n",
      "    enc input           > he si very corrupt seriously can t listen to him today he deserves jail really <EOS>\n",
      "    dec input           > we can t have law enforcement controlled by politician amp won t prosecute corruption or treason <EOS>\n",
      "    dec train predicted > we have t prosecute to enforcement controlled dept trump he have t prosecute interest <EOS> treason <EOS>\n",
      "dev minibatch loss: 7.384078025817871\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > thanks for <EOS>\n",
      "    DEV dec input           > this tuesday join and for brew amp book enjoy three exclusive hour at <EOS>\n",
      "    DEV dec train predicted > i is is the i a <EOS> the <EOS> <EOS> day <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i hope to be able to hear this <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8287\n",
      "learning rate 0.000857372\n",
      "epoch 16\n",
      "batch 350\n",
      "training minibatch loss: 3.484212636947632\n",
      "  sample 1:\n",
      "    enc input           > morning beth and all <EOS>\n",
      "    dec input           > have a great saturday laura amp all <EOS>\n",
      "    dec train predicted > morning u gz interactors interactors <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 7.363650798797607\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i love that she s such a big kid at heart <EOS>\n",
      "    DEV dec input           > katy at shanghai disneyland resort the complete instagram story <EOS>\n",
      "    DEV dec train predicted > i of the meme and you reason reason of <EOS>\n",
      "    DEV dec train infer > i love the pizza <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8337\n",
      "learning rate 0.000856576\n",
      "epoch 16\n",
      "batch 400\n",
      "training minibatch loss: 3.384458303451538\n",
      "  sample 1:\n",
      "    enc input           > did you ever make it to alameda ? there re some nifty house there <EOS>\n",
      "    dec input           > all the house in san francisco look like giant frosted cake <EOS>\n",
      "    dec train predicted > my my door and san francisco <EOS> like my frosted in <EOS>\n",
      "dev minibatch loss: 6.579833984375\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > very nice man <EOS>\n",
      "    DEV dec input           > shout out to for hitting me with that new hotness <EOS>\n",
      "    DEV dec train predicted > i to to a a my <EOS> my <EOS> day <EOS>\n",
      "    DEV dec train infer > this s a great day <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8387\n",
      "learning rate 0.000855782\n",
      "epoch 16\n",
      "batch 450\n",
      "training minibatch loss: 3.5055742263793945\n",
      "  sample 1:\n",
      "    enc input           > this been my baby since i wa <EOS>\n",
      "    dec input           > alicia key just gave me chill <EOS>\n",
      "    dec train predicted > i god is ha me a <EOS>\n",
      "dev minibatch loss: 7.419861316680908\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > can you convince to step in a the designated survivor of this election ? please ? ? <EOS>\n",
      "    DEV dec input           > this is what many people are feeling today <EOS>\n",
      "    DEV dec train predicted > you is what do people are voting a information\n",
      "    DEV dec train infer > you are voting to be rid of the debate <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8433\n",
      "learning rate 0.000855051\n",
      "epoch 17\n",
      "batch 0\n",
      "training minibatch loss: 3.3234195709228516\n",
      "  sample 1:\n",
      "    enc input           > caveat fintech co pretending they re not regulated but obviously are those might have a serious reckoning <EOS>\n",
      "    dec input           > more likely within the bank themselves imo scrutiny for the regulated fintech company is crazy <EOS>\n",
      "    dec train predicted > intentional potato to the other in scrutiny scrutiny for the regulated fintech company is sexual <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 6.9466166496276855\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i braided it it so cool <EOS>\n",
      "    DEV dec input           > your hair look really amazing <EOS>\n",
      "    DEV dec train predicted > i avi is like like <EOS>\n",
      "    DEV dec train infer > i m so glad you re so happy to be happy <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8483\n",
      "learning rate 0.000854257\n",
      "epoch 17\n",
      "batch 50\n",
      "training minibatch loss: 3.0337393283843994\n",
      "  sample 1:\n",
      "    enc input           > ocean s leave me alone i m half asleep <EOS>\n",
      "    dec input           > the ocean no place for a squirrel <EOS>\n",
      "    dec train predicted > the rent i wrong in the squirrel <EOS>\n",
      "dev minibatch loss: 6.952493190765381\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > don t be sad be glad <EOS>\n",
      "    DEV dec input           > woke up super depressed amp don t even know why or how to fix this lol <EOS>\n",
      "    DEV dec train predicted > i a a very that not t know know i i i i make the is <EOS>\n",
      "    DEV dec train infer > kane need to do me to read to cbs <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8533\n",
      "learning rate 0.000853465\n",
      "epoch 17\n",
      "batch 100\n",
      "training minibatch loss: 3.185666084289551\n",
      "  sample 1:\n",
      "    enc input           > he wa awful he exposed his thin skin cranky old man <EOS>\n",
      "    dec input           > what wa wrong with anything he said he called her out crooked <EOS>\n",
      "    dec train predicted > he s wrong he her he wa he s him fault <EOS> <EOS>\n",
      "dev minibatch loss: 6.727516174316406\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > omg yes i wa screaming the first time i heard this in game <EOS>\n",
      "    DEV dec input           > speaking of amazing soundtrack <EOS>\n",
      "    DEV dec train predicted > i in the <EOS> <EOS>\n",
      "    DEV dec train infer > i m so glad i m glad i m going to have to see the movie <EOS> <EOS>\n",
      "global_step: 8583\n",
      "learning rate 0.000852673\n",
      "epoch 17\n",
      "batch 150\n",
      "training minibatch loss: 3.463459014892578\n",
      "  sample 1:\n",
      "    enc input           > there are civilized white black asian mexican etc then we have terrorist group like blm <EOS>\n",
      "    dec input           > referring to a race a animal is racist <EOS>\n",
      "    dec train predicted > first to a great <EOS> a s a <EOS>\n",
      "dev minibatch loss: 7.375837802886963\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > kinda disturbing <EOS>\n",
      "    DEV dec input           > if your main claim to be president is your business we should talk about that <EOS>\n",
      "    DEV dec train predicted > i you resume streak i go back <EOS> to finger <EOS> can treasonous <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m gon na rotate myself <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8633\n",
      "learning rate 0.000851881\n",
      "epoch 17\n",
      "batch 200\n",
      "training minibatch loss: 3.413301467895508\n",
      "  sample 1:\n",
      "    enc input           > a a gaseous orange orb is fond of spouting sad <EOS>\n",
      "    dec input           > is doing what arkansas state trooper can no longer do deliver to a clinton <EOS>\n",
      "    dec train predicted > is a a arkansas question trooper is t own ? a this a race <EOS>\n",
      "dev minibatch loss: 7.321628570556641\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what day will you be there ? <EOS>\n",
      "    DEV dec input           > each of these deadpool variant will be available at hope to sign them for <EOS>\n",
      "    DEV dec train predicted > i a you a <EOS> you you a to <EOS> you you <EOS> <EOS> me\n",
      "    DEV dec train infer > i m just to be a lot i m not a good thing <EOS> <EOS> <EOS>\n",
      "global_step: 8683\n",
      "learning rate 0.000851091\n",
      "epoch 17\n",
      "batch 250\n",
      "training minibatch loss: 3.3420355319976807\n",
      "  sample 1:\n",
      "    enc input           > congratulation guy <EOS>\n",
      "    dec input           > i just published starting from scratch <EOS>\n",
      "    dec train predicted > i m want rt to this <EOS>\n",
      "dev minibatch loss: 7.006505489349365\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i deadass miss those day so much <EOS>\n",
      "    DEV dec input           > lmaoooo we used to be so weak at thisss omggggg <EOS>\n",
      "    DEV dec train predicted > nah i got to me cute poetic <EOS> xo xo so\n",
      "    DEV dec train infer > peep i m sorry i m hating till you re sorry <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8733\n",
      "learning rate 0.000850301\n",
      "epoch 17\n",
      "batch 300\n",
      "training minibatch loss: 3.41043758392334\n",
      "  sample 1:\n",
      "    enc input           > wa kind enough to make his film showing over the weekend a benefit <EOS>\n",
      "    dec input           > where ? hbo ? <EOS>\n",
      "    dec train predicted > i is hbo ? <EOS>\n",
      "dev minibatch loss: 7.018138408660889\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i want to cry <EOS>\n",
      "    DEV dec input           > here are the best sign at <EOS>\n",
      "    DEV dec train predicted > i s you best <EOS> <EOS> least\n",
      "    DEV dec train infer > i m going to be kidding me to be homework <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8783\n",
      "learning rate 0.000849512\n",
      "epoch 17\n",
      "batch 350\n",
      "training minibatch loss: 3.3831613063812256\n",
      "  sample 1:\n",
      "    enc input           > haha honestly the best for of meditation is running for me <EOS>\n",
      "    dec input           > aw man lol my bad bro u need to chill relax and blow one <EOS>\n",
      "    dec train predicted > i i i i burrito morning i like to go that <EOS> blow burrito <EOS>\n",
      "dev minibatch loss: 7.351502895355225\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > they won t it ll be refined coconut oil to remove the taste <EOS>\n",
      "    DEV dec input           > i hope they don t taste of coconut <EOS>\n",
      "    DEV dec train predicted > oh m you re t let good managing <EOS>\n",
      "    DEV dec train infer > happy the th song to the beach <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8833\n",
      "learning rate 0.000848723\n",
      "epoch 17\n",
      "batch 400\n",
      "training minibatch loss: 3.5162155628204346\n",
      "  sample 1:\n",
      "    enc input           > but that s low key how that chick from last week is lmao <EOS>\n",
      "    dec input           > this is what i picture when i say non nubian queen <EOS>\n",
      "    dec train predicted > what is what you have <EOS> you love nubian nubian wednesday <EOS>\n",
      "dev minibatch loss: 6.890547275543213\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > excellent article i posted a comment on the power of small viral action h t <EOS>\n",
      "    DEV dec input           > the force that surround u just tested the vulnerability of the st amendment and failed <EOS>\n",
      "    DEV dec train predicted > richard print print say bill behavior declining behavior moral behavior history rage shooter and lynn <EOS>\n",
      "    DEV dec train infer > how many of the bank richard change textbook between their behavior and raising veggie brutality <EOS>\n",
      "global_step: 8883\n",
      "learning rate 0.000847936\n",
      "epoch 17\n",
      "batch 450\n",
      "training minibatch loss: 3.351154327392578\n",
      "  sample 1:\n",
      "    enc input           > thanks shannon <EOS>\n",
      "    dec input           > congrats man <EOS>\n",
      "    dec train predicted > congrats on <EOS>\n",
      "dev minibatch loss: 7.210086345672607\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > also they re only ? ? <EOS>\n",
      "    DEV dec input           > lol they don t go on sale until the th so not sold out we got ta go <EOS>\n",
      "    DEV dec train predicted > i the re t think to the the each nutsack of you the out <EOS> are them hinted <EOS>\n",
      "    DEV dec train infer > they re interested like the shooter wth <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 8929\n",
      "learning rate 0.000847212\n",
      "epoch 18\n",
      "batch 0\n",
      "training minibatch loss: 3.065612316131592\n",
      "  sample 1:\n",
      "    enc input           > all the single lady by <EOS>\n",
      "    dec input           > what s the book omg <EOS>\n",
      "    dec train predicted > my s my book <EOS> <EOS>\n",
      "dev minibatch loss: 6.905324459075928\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > this is u tbh with <EOS>\n",
      "    DEV dec input           > when you and your best friend know your fave participant is making it to <EOS>\n",
      "    DEV dec train predicted > i i know playlist bella playlist playlist this fridge tweetstorm squeaky borrow a borrow borrow\n",
      "    DEV dec train infer > i m bored i can t be a bunch of month of this <EOS> <EOS>\n",
      "global_step: 8979\n",
      "learning rate 0.000846425\n",
      "epoch 18\n",
      "batch 50\n",
      "training minibatch loss: 3.1227591037750244\n",
      "  sample 1:\n",
      "    enc input           > industry of despair and poverty <EOS>\n",
      "    dec input           > half these appointment are bullshit it a money making machine for the agency social worker court etc <EOS>\n",
      "    dec train predicted > a the agency <EOS> a a in half in a in the agency agency agency court etc <EOS>\n",
      "dev minibatch loss: 7.426178455352783\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i forgot you re here i m here through midweek happy hr or lunch date ? <EOS>\n",
      "    DEV dec input           > how long are you kicking it in the bay area ? <EOS>\n",
      "    DEV dec train predicted > ben s i you <EOS> <EOS> <EOS> your golf amp <EOS> <EOS>\n",
      "    DEV dec train infer > jason who s a sweet jason is so cute <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 9029\n",
      "learning rate 0.00084564\n",
      "epoch 18\n",
      "batch 100\n",
      "training minibatch loss: 3.193148612976074\n",
      "  sample 1:\n",
      "    enc input           > i ain t bring shit back <EOS>\n",
      "    dec input           > i hope your brought back chocolate from euro <EOS>\n",
      "    dec train predicted > you m you cuzzo back euro <EOS> euro <EOS>\n",
      "dev minibatch loss: 6.7022705078125\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > haha i don t wan na school you kid on the old cod man <EOS>\n",
      "    DEV dec input           > shitttt snag that hoe back mw is coming up next <EOS>\n",
      "    DEV dec train predicted > i princess as wa ram is my dry up lad night\n",
      "    DEV dec train infer > i waved with my wrap <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9079\n",
      "learning rate 0.000844855\n",
      "epoch 18\n",
      "batch 150\n",
      "training minibatch loss: 3.3788280487060547\n",
      "  sample 1:\n",
      "    enc input           > donald j trump <EOS>\n",
      "    dec input           > fox news poll who is qualified to be president ? <EOS>\n",
      "    dec train predicted > hillary poll poll clinton will a to be president <EOS> <EOS>\n",
      "dev minibatch loss: 7.683045864105225\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that shit wa a bitch for me too <EOS>\n",
      "    DEV dec input           > it about time i passed my license <EOS>\n",
      "    DEV dec train predicted > i s a that wa to fashion in\n",
      "    DEV dec train infer > i m hoping you have a good thing <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9129\n",
      "learning rate 0.000844071\n",
      "epoch 18\n",
      "batch 200\n",
      "training minibatch loss: 3.2471764087677\n",
      "  sample 1:\n",
      "    enc input           > great to hear vital <EOS>\n",
      "    dec input           > had great workout with girl <EOS>\n",
      "    dec train predicted > had a workout <EOS> digital <EOS>\n",
      "dev minibatch loss: 7.086631774902344\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > trolling a a sheikh lol wasn t a serious request <EOS>\n",
      "    DEV dec input           > got ta hear both side <EOS>\n",
      "    DEV dec train predicted > ahh a be a tax wa\n",
      "    DEV dec train infer > what monkey wa a fake trainwreck i laughed <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9179\n",
      "learning rate 0.000843288\n",
      "epoch 18\n",
      "batch 250\n",
      "training minibatch loss: 3.1416122913360596\n",
      "  sample 1:\n",
      "    enc input           > i am blessed got ta give back <EOS>\n",
      "    dec input           > for not many spend time in for their <EOS>\n",
      "    dec train predicted > i the the thing time with the pacific <EOS>\n",
      "dev minibatch loss: 6.915750503540039\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > love you too babe <EOS>\n",
      "    DEV dec input           > i love my girl she s better than any other girl out there <EOS>\n",
      "    DEV dec train predicted > this love you follower love bbygirl love than my day week <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > my god and happy tea for my tea <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9229\n",
      "learning rate 0.000842505\n",
      "epoch 18\n",
      "batch 300\n",
      "training minibatch loss: 3.4600019454956055\n",
      "  sample 1:\n",
      "    enc input           > who are you again ? <EOS>\n",
      "    dec input           > am i included in that ? <EOS>\n",
      "    dec train predicted > fox i included to the <EOS> <EOS>\n",
      "dev minibatch loss: 7.218595027923584\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > amp her milly rock is sturdy <EOS>\n",
      "    DEV dec input           > my edge gone and she s not even halfway through the set rih <EOS>\n",
      "    DEV dec train predicted > projection niece is <EOS> winston ll ready a a a the territory <EOS> <EOS>\n",
      "    DEV dec train infer > niece ha a niece of new interview to see this <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9279\n",
      "learning rate 0.000841723\n",
      "epoch 18\n",
      "batch 350\n",
      "training minibatch loss: 3.508814573287964\n",
      "  sample 1:\n",
      "    enc input           > gabbert is trash <EOS>\n",
      "    dec input           > there s one er player working out on the side field at the moment it s colin kaepernick <EOS>\n",
      "    dec train predicted > this s crazy of one to in in the plane s s the beatles <EOS> s classy the <EOS>\n",
      "dev minibatch loss: 7.066452980041504\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > oh my want so many of the piece <EOS>\n",
      "    DEV dec input           > collection for gorgeous leather with a twist <EOS>\n",
      "    DEV dec train predicted > i <EOS> course and <EOS> people th and\n",
      "    DEV dec train infer > i m so cry <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9329\n",
      "learning rate 0.000840942\n",
      "epoch 18\n",
      "batch 400\n",
      "training minibatch loss: 3.2627902030944824\n",
      "  sample 1:\n",
      "    enc input           > while belgium luxembourg ireland cayman and switzerland have loaded up it s still a trillion market and growing <EOS>\n",
      "    dec input           > foreign central bank have cut their treasury holding to the lowest since <EOS>\n",
      "    dec train predicted > report report bank are been the treasury fair to the final cell <EOS>\n",
      "dev minibatch loss: 7.395328998565674\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > just gon na kiss your body warm cover smell lt <EOS>\n",
      "    DEV dec input           > text trade pic or call me now <EOS>\n",
      "    DEV dec train predicted > i my this of my <EOS> i <EOS>\n",
      "    DEV dec train infer > i m hoping i m hoping i love this morning <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9379\n",
      "learning rate 0.000840162\n",
      "epoch 18\n",
      "batch 450\n",
      "training minibatch loss: 3.3863041400909424\n",
      "  sample 1:\n",
      "    enc input           > is your office the same a alonzo s in training day ? <EOS>\n",
      "    dec input           > no i have an office <EOS>\n",
      "    dec train predicted > i i get a <EOS> <EOS>\n",
      "dev minibatch loss: 7.476948261260986\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > new revenue idea we re taking of every purchase so expect lot of ad for jet fighter <EOS>\n",
      "    DEV dec input           > so do you think i m in the market for a th generation military fighter jet ? <EOS>\n",
      "    DEV dec train predicted > great a you need to can looking the world <EOS> the many <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > great morning <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 9425\n",
      "learning rate 0.000839444\n",
      "epoch 19\n",
      "batch 0\n",
      "training minibatch loss: 2.8148787021636963\n",
      "  sample 1:\n",
      "    enc input           > feeling safe in the world is an anacronym <EOS>\n",
      "    dec input           > lot of gray swan this morning <EOS>\n",
      "    dec train predicted > coded of nc swan <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 6.857266426086426\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i love him and joe started this <EOS>\n",
      "    DEV dec input           > you fucked up you lost visitation right you re not worthy to be called his uncle <EOS>\n",
      "    DEV dec train predicted > i re up a re <EOS> <EOS> <EOS> have not a <EOS> <EOS> strength <EOS> own <EOS>\n",
      "    DEV dec train infer > i m not sure you re not sure it s a hell <EOS> <EOS> <EOS>\n",
      "global_step: 9475\n",
      "learning rate 0.000838665\n",
      "epoch 19\n",
      "batch 50\n",
      "training minibatch loss: 2.9591140747070312\n",
      "  sample 1:\n",
      "    enc input           > this tweet look familiar <EOS>\n",
      "    dec input           > donald trump i never said that the internet <EOS>\n",
      "    dec train predicted > trump trump said have said trump a trump <EOS>\n",
      "dev minibatch loss: 6.855839729309082\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what season you in ? <EOS>\n",
      "    DEV dec input           > lmaooooo ian bf cheating on him with a baddie <EOS>\n",
      "    DEV dec train predicted > yea what what me i high <EOS> me bike <EOS>\n",
      "    DEV dec train infer > what s the sleepy <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9525\n",
      "learning rate 0.000837887\n",
      "epoch 19\n",
      "batch 100\n",
      "training minibatch loss: 2.9407641887664795\n",
      "  sample 1:\n",
      "    enc input           > gon na get a knock off gold coin shirt cold groin <EOS>\n",
      "    dec input           > long live cold groin <EOS>\n",
      "    dec train predicted > hey waiting at groin <EOS>\n",
      "dev minibatch loss: 6.547210693359375\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > to phil boyle <EOS>\n",
      "    DEV dec input           > and some odd hour left about to start climbing some ladder <EOS>\n",
      "    DEV dec train predicted > and how of for <EOS> <EOS> to <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > in the last night <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9575\n",
      "learning rate 0.00083711\n",
      "epoch 19\n",
      "batch 150\n",
      "training minibatch loss: 3.3785603046417236\n",
      "  sample 1:\n",
      "    enc input           > it s not raining either <EOS>\n",
      "    dec input           > it s dead not hot enough for rain boot y all are buggin <EOS>\n",
      "    dec train predicted > you s cold rain good to but your rain ? all are cold <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 8.124956130981445\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > dim sum of all fear hi anthony good morning <EOS>\n",
      "    DEV dec input           > will someone drive me to flushing for dim sum brunch ? <EOS>\n",
      "    DEV dec train predicted > good the be to a the <EOS> the <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > good morning good morning <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9625\n",
      "learning rate 0.000836333\n",
      "epoch 19\n",
      "batch 200\n",
      "training minibatch loss: 3.133808135986328\n",
      "  sample 1:\n",
      "    enc input           > your not the only one glad it ended <EOS>\n",
      "    dec input           > my head literally hurt listening to this idiot talk <EOS>\n",
      "    dec train predicted > this courage is like listening to <EOS> <EOS> read <EOS>\n",
      "dev minibatch loss: 7.21498441696167\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you re the journalist that used aloud instead of allowed fantastic thorough work a always <EOS>\n",
      "    DEV dec input           > my latest piece for on new trend of blaming me progressive journalist for trump <EOS>\n",
      "    DEV dec train predicted > just family family football the the year and the year and <EOS> for the <EOS>\n",
      "    DEV dec train infer > the potato are begging to determine what are not a crazy party crazy <EOS> <EOS> <EOS>\n",
      "global_step: 9675\n",
      "learning rate 0.000835556\n",
      "epoch 19\n",
      "batch 250\n",
      "training minibatch loss: 3.113502025604248\n",
      "  sample 1:\n",
      "    enc input           > need for this p use skype and be good and don t have an ego <EOS>\n",
      "    dec input           > xb p pm et free entry prize v snd nd click now <EOS>\n",
      "    dec train predicted > xb nd et et free entry prize v snd nd xb for <EOS>\n",
      "dev minibatch loss: 8.092700958251953\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > intriguing the different place different outlet go with this up out only <EOS>\n",
      "    DEV dec input           > thursday s most read what if sprawl is the only solution to affordable city ? <EOS>\n",
      "    DEV dec train predicted > the th round thing the you you live a same game <EOS> maintain <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m stuck in the best game in the same of th <EOS> <EOS> <EOS>\n",
      "global_step: 9725\n",
      "learning rate 0.000834781\n",
      "epoch 19\n",
      "batch 300\n",
      "training minibatch loss: 2.9531524181365967\n",
      "  sample 1:\n",
      "    enc input           > jack adam hall in cesar chavez <EOS>\n",
      "    dec input           > where can we donate ? ? <EOS>\n",
      "    dec train predicted > where s you explain <EOS> <EOS>\n",
      "dev minibatch loss: 7.408724784851074\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > wrong on so many count <EOS>\n",
      "    DEV dec input           > please get that fucking vegetable fruit whatever it is off that plate cc <EOS>\n",
      "    DEV dec train predicted > i have the out <EOS> playing <EOS> <EOS> <EOS> <EOS> the <EOS> s <EOS>\n",
      "    DEV dec train infer > hey the best of the rest of the summer is on the world of wednesday <EOS> <EOS>\n",
      "global_step: 9775\n",
      "learning rate 0.000834006\n",
      "epoch 19\n",
      "batch 350\n",
      "training minibatch loss: 3.165510892868042\n",
      "  sample 1:\n",
      "    enc input           > no she ha been pulled into a meeting <EOS>\n",
      "    dec input           > is she not attending ? <EOS>\n",
      "    dec train predicted > who you congratulating a ? ?\n",
      "dev minibatch loss: 7.439566612243652\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > so if they are so wonderful why doesn t she let them move into her hood ? <EOS>\n",
      "    DEV dec input           > today there are million of law abiding peaceful muslim in the united state <EOS>\n",
      "    DEV dec train predicted > is is s a idiot course woman she her she her economy woman quo\n",
      "    DEV dec train infer > is either for trump for finishing her <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9825\n",
      "learning rate 0.000833232\n",
      "epoch 19\n",
      "batch 400\n",
      "training minibatch loss: 3.261679172515869\n",
      "  sample 1:\n",
      "    enc input           > you owe me quite a pretty penny <EOS>\n",
      "    dec input           > amp you are wrong ed way wrongful of a bookie <EOS>\n",
      "    dec train predicted > is you re a in wrongful wrongful of the bookie <EOS>\n",
      "dev minibatch loss: 7.8959550857543945\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > maybe you should try thinking for yourself instead of being led around by the nose by the medium <EOS>\n",
      "    DEV dec input           > say the man voting for an orange con man <EOS>\n",
      "    DEV dec train predicted > i i first i for time contract story <EOS> <EOS>\n",
      "    DEV dec train infer > i m not a bad of my life on <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9875\n",
      "learning rate 0.000832459\n",
      "epoch 19\n",
      "batch 450\n",
      "training minibatch loss: 3.5124027729034424\n",
      "  sample 1:\n",
      "    enc input           > yes omg <EOS>\n",
      "    dec input           > totally perhaps this evening from the studio <EOS>\n",
      "    dec train predicted > i got i pt at <EOS> pt <EOS>\n",
      "dev minibatch loss: 7.134413242340088\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > but what about this ? <EOS>\n",
      "    DEV dec input           > new policy i m unfollowing anyone who us the word guru in anything other than a mocking fashion <EOS>\n",
      "    DEV dec train predicted > this one winter ve nearby to re got a craziest of i the <EOS> yr it window <EOS> <EOS>\n",
      "    DEV dec train infer > i m talkin about to see what s one of the beatles ? <EOS> <EOS> <EOS>\n",
      "global_step: 9921\n",
      "learning rate 0.000831748\n",
      "epoch 20\n",
      "batch 0\n",
      "training minibatch loss: 2.7693796157836914\n",
      "  sample 1:\n",
      "    enc input           > you should <EOS>\n",
      "    dec input           > really miss maybe someday i ll be able to go back <EOS>\n",
      "    dec train predicted > i i you i <EOS> m be a to be <EOS> <EOS>\n",
      "dev minibatch loss: 7.310378551483154\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > keeping it real on what we say versus what we do <EOS>\n",
      "    DEV dec input           > hah love this too <EOS>\n",
      "    DEV dec train predicted > it but to week <EOS>\n",
      "    DEV dec train infer > wait what to get to be kidding me <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 9971\n",
      "learning rate 0.000830976\n",
      "epoch 20\n",
      "batch 50\n",
      "training minibatch loss: 2.9651503562927246\n",
      "  sample 1:\n",
      "    enc input           > glad you re okay curious to see what this wa is <EOS>\n",
      "    dec input           > so crazy wa in an uber on nd amp th <EOS>\n",
      "    dec train predicted > so glad and so my hour and fame <EOS> th amp\n",
      "dev minibatch loss: 7.43680477142334\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > sorry to you and her <EOS>\n",
      "    DEV dec input           > my wife s grandfather passed away today been a rough weekend overall hope you all fared better <EOS>\n",
      "    DEV dec train predicted > i body claudia birthday day in today all a great <EOS> <EOS> <EOS> <EOS> re <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m blocked i m blocked to kiss my homework <EOS> <EOS> <EOS>\n",
      "global_step: 10021\n",
      "learning rate 0.000830205\n",
      "epoch 20\n",
      "batch 100\n",
      "training minibatch loss: 2.980762004852295\n",
      "  sample 1:\n",
      "    enc input           > not that great <EOS>\n",
      "    dec input           > gon na die when ye performs fade in front of me <EOS>\n",
      "    dec train predicted > woah na eat for ye performs wheel by bein <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 7.87733268737793\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what ? i made it on roblox then d printed it lmao <EOS>\n",
      "    DEV dec input           > how do you transfer it into roblox ? <EOS>\n",
      "    DEV dec train predicted > it much you get though s a ? <EOS>\n",
      "    DEV dec train infer > it s a hell i went in the same in april <EOS> <EOS> <EOS>\n",
      "global_step: 10071\n",
      "learning rate 0.000829435\n",
      "epoch 20\n",
      "batch 150\n",
      "training minibatch loss: 3.068190336227417\n",
      "  sample 1:\n",
      "    enc input           > delete your account <EOS>\n",
      "    dec input           > marshall take his cue from a year old like most hillary supporter <EOS>\n",
      "    dec train predicted > hillary have hillary oz a neera single of culture radical exclusive traveled <EOS>\n",
      "dev minibatch loss: 7.362045764923096\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i can t resist the spotlight <EOS>\n",
      "    DEV dec input           > if gaming ha taught me anything it is that this cat ha a side quest for me <EOS>\n",
      "    DEV dec train predicted > who you protest to in to to will a the weekend will a consonant in <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > who may be african american men <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10121\n",
      "learning rate 0.000828665\n",
      "epoch 20\n",
      "batch 200\n",
      "training minibatch loss: 3.0962061882019043\n",
      "  sample 1:\n",
      "    enc input           > tonight round <EOS>\n",
      "    dec input           > yesterday wa so great man let repeat <EOS>\n",
      "    dec train predicted > happy planned so happy happy i much <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 7.622653961181641\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > hey u don t know definition of supporter they believe trump bling thank you very much <EOS>\n",
      "    DEV dec input           > an atlantic wall ? to keep out muslim ? of trump supporter want it they explain everything vol <EOS>\n",
      "    DEV dec train predicted > happy england for just you be you u reference be u reference you all reference are you <EOS> <EOS>\n",
      "    DEV dec train infer > i love you love you all <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10171\n",
      "learning rate 0.000827896\n",
      "epoch 20\n",
      "batch 250\n",
      "training minibatch loss: 3.0549731254577637\n",
      "  sample 1:\n",
      "    enc input           > a matched pair <EOS>\n",
      "    dec input           > along with of american saying we need a new direction <EOS>\n",
      "    dec train predicted > california and his a own ? can to successful direction <EOS>\n",
      "dev minibatch loss: 7.6862287521362305\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > aww thanks love u lot cant wait till next weekend <EOS>\n",
      "    DEV dec input           > happy birthday princess love you amp have you have an amazing day wish we had more pic together <EOS>\n",
      "    DEV dec train predicted > vip birthday i <EOS> <EOS> enjoy see a enjoy kept vip vip <EOS> <EOS> you <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > happy birthday to <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10221\n",
      "learning rate 0.000827128\n",
      "epoch 20\n",
      "batch 300\n",
      "training minibatch loss: 3.0394508838653564\n",
      "  sample 1:\n",
      "    enc input           > welp i know who got em lol <EOS>\n",
      "    dec input           > thot friend fun a hell <EOS>\n",
      "    dec train predicted > thot dad my <EOS> dog of\n",
      "dev minibatch loss: 7.715157985687256\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > intermission is an irish movie you really ought to watch also <EOS>\n",
      "    DEV dec input           > is this our intermission ? <EOS>\n",
      "    DEV dec train predicted > my that drunk voice embarrassment dedicated\n",
      "    DEV dec train infer > this is a fucking dedicated crushed <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10271\n",
      "learning rate 0.00082636\n",
      "epoch 20\n",
      "batch 350\n",
      "training minibatch loss: 3.2118725776672363\n",
      "  sample 1:\n",
      "    enc input           > degree with fog and wind nothing compared to snow like you <EOS>\n",
      "    dec input           > cold ? what s cold to you ? <EOS>\n",
      "    dec train predicted > cold what what s the ? the ? <EOS>\n",
      "dev minibatch loss: 7.1709113121032715\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > probably make you exactly a sad a we make god sad for not believing in him <EOS>\n",
      "    DEV dec input           > god doesn t believe in atheist ? damn that make me sad <EOS>\n",
      "    DEV dec train predicted > is is t be that the firth <EOS> participating s a president president\n",
      "    DEV dec train infer > i m not correcting a realization that realization it in the issue <EOS> <EOS> <EOS>\n",
      "global_step: 10321\n",
      "learning rate 0.000825593\n",
      "epoch 20\n",
      "batch 400\n",
      "training minibatch loss: 3.2828168869018555\n",
      "  sample 1:\n",
      "    enc input           > the side event follows the official signing of at the un hq by he <EOS>\n",
      "    dec input           > a part of her official duty hm will facilitate nigeria side event at on <EOS>\n",
      "    dec train predicted > a financial of a say financial hm is facilitate facilitate version is at <EOS> <EOS>\n",
      "dev minibatch loss: 8.540420532226562\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > when is this on ? <EOS>\n",
      "    DEV dec input           > v chicago fire <EOS>\n",
      "    DEV dec train predicted > describe injures show throwback\n",
      "    DEV dec train infer > i m going to be a new zone on month <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10371\n",
      "learning rate 0.000824827\n",
      "epoch 20\n",
      "batch 450\n",
      "training minibatch loss: 3.237539052963257\n",
      "  sample 1:\n",
      "    enc input           > sure what is it ? <EOS>\n",
      "    dec input           > hi could you pls spare a moment to try out my app i m looking for input <EOS>\n",
      "    dec train predicted > i i you get be the new to get the <EOS> next i ve important for a <EOS>\n",
      "dev minibatch loss: 7.286760330200195\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > why is dad so illiterate <EOS>\n",
      "    DEV dec input           > i wa just about to post this <EOS>\n",
      "    DEV dec train predicted > the have a a of win this month\n",
      "    DEV dec train infer > i can t wait to be a month at the month <EOS> <EOS> <EOS>\n",
      "global_step: 10417\n",
      "learning rate 0.000824123\n",
      "epoch 21\n",
      "batch 0\n",
      "training minibatch loss: 3.117034673690796\n",
      "  sample 1:\n",
      "    enc input           > goddamnit don t make me do mean thing to you <EOS>\n",
      "    dec input           > vin scully is extremely boring <EOS>\n",
      "    dec train predicted > vin scully is boring boring at\n",
      "dev minibatch loss: 7.3785929679870605\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that s a hit bro you really went in loving the style on the whole track <EOS>\n",
      "    DEV dec input           > i sampled my favorite song of all time to make this beat <EOS>\n",
      "    DEV dec train predicted > just m minute voice hand to the minute or the a movie is\n",
      "    DEV dec train infer > sometimes it s been a new street in charlotte <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10467\n",
      "learning rate 0.000823358\n",
      "epoch 21\n",
      "batch 50\n",
      "training minibatch loss: 2.9266440868377686\n",
      "  sample 1:\n",
      "    enc input           > happy belated birthday <EOS>\n",
      "    dec input           > thanks you guy for the birthday wish yesterday d i had a really nice day <EOS>\n",
      "    dec train predicted > i the in are the best one i i you m a good good day <EOS>\n",
      "dev minibatch loss: 8.003844261169434\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what the ever loving fuck <EOS>\n",
      "    DEV dec input           > mark halperin just said clinton never fought successfully in a presidential campaign to win <EOS>\n",
      "    DEV dec train predicted > where the the been the ram end the in the train train that the <EOS>\n",
      "    DEV dec train infer > when they mention the electric war and ram ? <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10517\n",
      "learning rate 0.000822594\n",
      "epoch 21\n",
      "batch 100\n",
      "training minibatch loss: 3.0147852897644043\n",
      "  sample 1:\n",
      "    enc input           > sorry will do computer crashed on weekend just getting it back <EOS>\n",
      "    dec input           > hussein could you shoot me back an email when you have the chance ? <EOS>\n",
      "    dec train predicted > hussein did you have you out a sex you you re a pure to <EOS>\n",
      "dev minibatch loss: 7.567729473114014\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > brand new tune <EOS>\n",
      "    DEV dec input           > share with you the african proverb of the day <EOS>\n",
      "    DEV dec train predicted > husker ft the ? anthem one of the uk <EOS>\n",
      "    DEV dec train infer > husker one dj helped the audio anthem json share share bn to nwayo bn <EOS> <EOS>\n",
      "global_step: 10567\n",
      "learning rate 0.000821831\n",
      "epoch 21\n",
      "batch 150\n",
      "training minibatch loss: 3.037534475326538\n",
      "  sample 1:\n",
      "    enc input           > but we won so i ll be happy <EOS>\n",
      "    dec input           > have you seen the offense ? eli cruz beckham jr baby <EOS>\n",
      "    dec train predicted > we you serve the hog with <EOS> i beckham s place <EOS>\n",
      "dev minibatch loss: 7.66130256652832\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > played on a team that wa bad at defense a a whole <EOS>\n",
      "    DEV dec input           > he s a terrible defender and got his coach fired <EOS>\n",
      "    DEV dec train predicted > research s crushed research system of the a research at <EOS>\n",
      "    DEV dec train infer > another wall look like brooklyn but i m honored at the end of the th <EOS> <EOS>\n",
      "global_step: 10617\n",
      "learning rate 0.000821068\n",
      "epoch 21\n",
      "batch 200\n",
      "training minibatch loss: 3.2718772888183594\n",
      "  sample 1:\n",
      "    enc input           > people like this don t end up in jail they end up in public office <EOS>\n",
      "    dec input           > i wonder how many people will go to jail for this <EOS>\n",
      "    dec train predicted > how wonder how many people have be to jail for the week\n",
      "dev minibatch loss: 7.633162498474121\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > get yourself a leo fire and water create balance <EOS>\n",
      "    DEV dec input           > i want to feel something already what the fuck <EOS>\n",
      "    DEV dec train predicted > i m to be a on <EOS> you short you\n",
      "    DEV dec train infer > i m a good football cup is a good game <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10667\n",
      "learning rate 0.000820306\n",
      "epoch 21\n",
      "batch 250\n",
      "training minibatch loss: 2.8781397342681885\n",
      "  sample 1:\n",
      "    enc input           > awesome yeah keep that ignorance strong my brother <EOS>\n",
      "    dec input           > he s never going to answer cuz there isn t one thing that s actually racist <EOS>\n",
      "    dec train predicted > what s a saying to know the he s t a about he s bullshit a <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 7.106821060180664\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > excellent article i posted a comment on the power of small viral action h t <EOS>\n",
      "    DEV dec input           > the force that surround u just tested the vulnerability of the st amendment and failed <EOS>\n",
      "    DEV dec train predicted > how latest are s behavior behavior cost than behavior behavior enemy history history raising wood to\n",
      "    DEV dec train infer > history is political political government state is using behavior and the government are not full via <EOS>\n",
      "global_step: 10717\n",
      "learning rate 0.000819545\n",
      "epoch 21\n",
      "batch 300\n",
      "training minibatch loss: 3.045736789703369\n",
      "  sample 1:\n",
      "    enc input           > sorry i missed last night i wa exhausted <EOS>\n",
      "    dec input           > uh oh we are probably saying bye to dolph <EOS>\n",
      "    dec train predicted > uh uh uh re giving to uh for dolph <EOS>\n",
      "dev minibatch loss: 7.281449794769287\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that wa the only way we could get him out of there <EOS>\n",
      "    DEV dec input           > look how improved be oline wa on the right when newhouse left <EOS>\n",
      "    DEV dec train predicted > i sure about i the about in twitter same <EOS> he cunt <EOS>\n",
      "    DEV dec train infer > hey tell the objectively the classic plate at the mutated night <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10767\n",
      "learning rate 0.000818784\n",
      "epoch 21\n",
      "batch 350\n",
      "training minibatch loss: 3.2057738304138184\n",
      "  sample 1:\n",
      "    enc input           > it s a great marketing opportunity for them <EOS>\n",
      "    dec input           > when the pr and marketing people switch on their phone tomorrow morning <EOS>\n",
      "    dec train predicted > how the game <EOS> switch ? switch a a stadium <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 7.037740707397461\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > haha thanks <EOS>\n",
      "    DEV dec input           > aw your sign is cute <EOS>\n",
      "    DEV dec train predicted > i <EOS> bday <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > happy birthday to the follow homie <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10817\n",
      "learning rate 0.000818024\n",
      "epoch 21\n",
      "batch 400\n",
      "training minibatch loss: 3.378322124481201\n",
      "  sample 1:\n",
      "    enc input           > congrats best of luck in the future <EOS>\n",
      "    dec input           > want to thank everyone that helped with the decision but here it is <EOS>\n",
      "    dec train predicted > just to get you to a it a last <EOS> now <EOS> wa <EOS>\n",
      "dev minibatch loss: 7.9422502517700195\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > well we all know who lester is voting for i used to think he wa impartial not anymore <EOS>\n",
      "    DEV dec input           > nbc s asked question on birtherism and nothing on the clinton foundation <EOS>\n",
      "    DEV dec train predicted > no clinton chief he he trump <EOS> he didn the debate ? <EOS>\n",
      "    DEV dec train infer > hopefully he should be known to the net <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 10867\n",
      "learning rate 0.000817265\n",
      "epoch 21\n",
      "batch 450\n",
      "training minibatch loss: 3.254453182220459\n",
      "  sample 1:\n",
      "    enc input           > eh wheeler hasn t been with the team all season harvey is another one though <EOS>\n",
      "    dec input           > harvey and wheeler <EOS>\n",
      "    dec train predicted > harvey at wheeler at\n",
      "dev minibatch loss: 7.261223316192627\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > warrior poet all <EOS>\n",
      "    DEV dec input           > this team ha heart i m proud of my team for playing their heart out for minute <EOS>\n",
      "    DEV dec train predicted > this is reminds been a said like to the life <EOS> a <EOS> solution <EOS> <EOS> the <EOS>\n",
      "    DEV dec train infer > this is a genius <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 10913\n",
      "learning rate 0.000816567\n",
      "epoch 22\n",
      "batch 0\n",
      "training minibatch loss: 2.9726369380950928\n",
      "  sample 1:\n",
      "    enc input           > i wa gon na ask you to show me around when i come <EOS>\n",
      "    dec input           > i swear everybody in chicago only hit me up when they need something <EOS>\n",
      "    dec train predicted > i m my i my m just up up to i re to <EOS>\n",
      "dev minibatch loss: 7.7218708992004395\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > for the wrong reason keep that shit in la and away from oakland <EOS>\n",
      "    DEV dec input           > to be fair the instigator seemed way out of line here <EOS>\n",
      "    DEV dec train predicted > and see fitzpatrick <EOS> door within on to about the <EOS> <EOS>\n",
      "    DEV dec train infer > fitzpatrick and pass to leave u action s in a cabinet th <EOS> <EOS>\n",
      "global_step: 10963\n",
      "learning rate 0.000815809\n",
      "epoch 22\n",
      "batch 50\n",
      "training minibatch loss: 2.6274356842041016\n",
      "  sample 1:\n",
      "    enc input           > my wife and i love you guy i think ive seen every video you guy are fantastic <EOS>\n",
      "    dec input           > thanks jesse truly appreciate the support <EOS>\n",
      "    dec train predicted > thanks jesse a support u support <EOS>\n",
      "dev minibatch loss: 7.0712714195251465\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > wow thanks <EOS>\n",
      "    DEV dec input           > just say no to the container store cc <EOS>\n",
      "    DEV dec train predicted > happy recently to one a new is is <EOS>\n",
      "    DEV dec train infer > happy birthday to the world <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11013\n",
      "learning rate 0.000815052\n",
      "epoch 22\n",
      "batch 100\n",
      "training minibatch loss: 3.0243685245513916\n",
      "  sample 1:\n",
      "    enc input           > i wa jealous of you before for so many reason add this one to the list <EOS>\n",
      "    dec input           > a shake shack walking distance from my house open tomorrow so see you later <EOS>\n",
      "    dec train predicted > so radio shack in york in my house <EOS> like at slow you <EOS> <EOS>\n",
      "dev minibatch loss: 8.283794403076172\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the little baby bomber sanchez didn t lip off or it would have happened <EOS>\n",
      "    DEV dec input           > i wish martin and sanchez would have a molina phillips circa moment here <EOS>\n",
      "    DEV dec train predicted > btw m i wa watch killed be watch bit of <EOS> <EOS> i <EOS>\n",
      "    DEV dec train infer > rlly watched the position feminine is so glad i ll have a great position for the bathroom <EOS>\n",
      "global_step: 11063\n",
      "learning rate 0.000814296\n",
      "epoch 22\n",
      "batch 150\n",
      "training minibatch loss: 2.9752631187438965\n",
      "  sample 1:\n",
      "    enc input           > worth the wait lol <EOS>\n",
      "    dec input           > great stuff you wait all day for the nfl to start and then your team doe this <EOS>\n",
      "    dec train predicted > thanks to i know to awesome on the season s the and dog m new <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 7.900981903076172\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > same here i ve been on edge all morning literally in the worst mood <EOS>\n",
      "    DEV dec input           > girl i don t even know what to do with myself <EOS>\n",
      "    DEV dec train predicted > i i got t know pull where bruce pull it sauce <EOS>\n",
      "    DEV dec train infer > i m got to pull to pull me to pull myself yet <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11113\n",
      "learning rate 0.00081354\n",
      "epoch 22\n",
      "batch 200\n",
      "training minibatch loss: 3.178934097290039\n",
      "  sample 1:\n",
      "    enc input           > you re the third worst marvel character <EOS>\n",
      "    dec input           > deadpool wa the second worst marvel movie ever after of course green lantern <EOS>\n",
      "    dec train predicted > i wa the biggest chad person movie ever tribe that course people train <EOS>\n",
      "dev minibatch loss: 7.796782970428467\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > now i have this resonates in my house <EOS>\n",
      "    DEV dec input           > did you read my piece on that ? the night trump supporter found me out a a jew <EOS>\n",
      "    DEV dec train predicted > grime you be the knee <EOS> the s <EOS> internet i s is <EOS> <EOS> <EOS> song heavy <EOS>\n",
      "    DEV dec train infer > i love this song <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11163\n",
      "learning rate 0.000812785\n",
      "epoch 22\n",
      "batch 250\n",
      "training minibatch loss: 3.0124411582946777\n",
      "  sample 1:\n",
      "    enc input           > even worse wa the first album i ever bought probably when i wa about year old <EOS>\n",
      "    dec input           > i saw him live in so you ve succeeded in making me feel old at any rate <EOS>\n",
      "    dec train predicted > i m the at at philly i re heard <EOS> the the only like <EOS> the time <EOS>\n",
      "dev minibatch loss: 8.141709327697754\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > gorgeous photograph paul jon have a wonderful day hug amp blessing <EOS>\n",
      "    DEV dec input           > the awakening of another dawn <EOS>\n",
      "    DEV dec train predicted > thanks best crash the news of\n",
      "    DEV dec train infer > huge we will be a huge fan yesterday of the year <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 11213\n",
      "learning rate 0.000812031\n",
      "epoch 22\n",
      "batch 300\n",
      "training minibatch loss: 3.0888659954071045\n",
      "  sample 1:\n",
      "    enc input           > im no longer impressed <EOS>\n",
      "    dec input           > lifted straight from wikipedia <EOS>\n",
      "    dec train predicted > lifted in <EOS> wikipedia <EOS>\n",
      "dev minibatch loss: 7.218155860900879\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > and literacy i m guessing ? <EOS>\n",
      "    DEV dec input           > if liberal were exterminated so would racism <EOS>\n",
      "    DEV dec train predicted > i you i so so glad have <EOS>\n",
      "    DEV dec train infer > i have a doubt of the same thing <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11263\n",
      "learning rate 0.000811277\n",
      "epoch 22\n",
      "batch 350\n",
      "training minibatch loss: 3.1262784004211426\n",
      "  sample 1:\n",
      "    enc input           > thank ya much griff <EOS>\n",
      "    dec input           > to you amp <EOS>\n",
      "    dec train predicted > to you <EOS> <EOS>\n",
      "dev minibatch loss: 7.445458889007568\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > tyvm the amp the <EOS>\n",
      "    DEV dec input           > tyvm the <EOS>\n",
      "    DEV dec train predicted > tyvm the <EOS>\n",
      "    DEV dec train infer > tyvm the last last night <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11313\n",
      "learning rate 0.000810525\n",
      "epoch 22\n",
      "batch 400\n",
      "training minibatch loss: 3.088280439376831\n",
      "  sample 1:\n",
      "    enc input           > they deserve to be penalize <EOS>\n",
      "    dec input           > rnc chairman party could penalize former candidate who don t back trump <EOS>\n",
      "    dec train predicted > donald chairman reason would penalize death teen he would t t her crowd\n",
      "dev minibatch loss: 8.112349510192871\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you mind if i gif this ? <EOS>\n",
      "    DEV dec input           > when you have sucked for month and finally get a hit and forget how to run the base <EOS>\n",
      "    DEV dec train predicted > i i know a <EOS> my <EOS> property i a bitch <EOS> implode <EOS> me implode <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m not a bad of my life <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11363\n",
      "learning rate 0.000809772\n",
      "epoch 22\n",
      "batch 450\n",
      "training minibatch loss: 3.3882827758789062\n",
      "  sample 1:\n",
      "    enc input           > come home with me the best is yet to be love you <EOS>\n",
      "    dec input           > black started the republican party in much of the south they are just coming home again <EOS>\n",
      "    dec train predicted > i subscriber a woman woman to a of the same i re going going a <EOS> <EOS>\n",
      "dev minibatch loss: 7.02491569519043\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > it s xbox only but the game is out in north america at am edt <EOS>\n",
      "    DEV dec input           > hey man is there a way to get fifa on the p already ? <EOS>\n",
      "    DEV dec train predicted > trump no you this the corporate to cook in in the st <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > this is the best thing <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11409\n",
      "learning rate 0.000809081\n",
      "epoch 23\n",
      "batch 0\n",
      "training minibatch loss: 2.8199944496154785\n",
      "  sample 1:\n",
      "    enc input           > wish i could ve been there <EOS>\n",
      "    dec input           > me supporting josh is a close a it get to good game little one <EOS>\n",
      "    dec train predicted > a on <EOS> is a good a a wa a a a a in <EOS>\n",
      "dev minibatch loss: 7.7200517654418945\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i heard him <EOS>\n",
      "    DEV dec input           > brett is the shark whisperer <EOS>\n",
      "    DEV dec train predicted > i boy gon best in a\n",
      "    DEV dec train infer > can t wait to see the hangout <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11459\n",
      "learning rate 0.00080833\n",
      "epoch 23\n",
      "batch 50\n",
      "training minibatch loss: 3.012291669845581\n",
      "  sample 1:\n",
      "    enc input           > why is that where she s been ? <EOS>\n",
      "    dec input           > do you live under a rock ? <EOS>\n",
      "    dec train predicted > everyone you be in the withering ? <EOS>\n",
      "dev minibatch loss: 7.644404888153076\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > pandering clinton she s a joke <EOS>\n",
      "    DEV dec input           > this is a man who ha called woman pig and liar <EOS>\n",
      "    DEV dec train predicted > hillary campaign a great who need secretary secretary secretary in campaign in\n",
      "    DEV dec train infer > trump clinton ha campaign manager for trump secretary of state of campaign campaign campaign <EOS> <EOS> <EOS>\n",
      "global_step: 11509\n",
      "learning rate 0.00080758\n",
      "epoch 23\n",
      "batch 100\n",
      "training minibatch loss: 2.9547693729400635\n",
      "  sample 1:\n",
      "    enc input           > lol bc when i stopped watching this summer i just got all my news from you <EOS>\n",
      "    dec input           > tbh i don t watch i just go through bb account and rt funny stuff <EOS>\n",
      "    dec train predicted > i i m t watch i m tweeted right my i <EOS> i i <EOS> <EOS>\n",
      "dev minibatch loss: 7.124561309814453\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > hey i left the name out <EOS>\n",
      "    DEV dec input           > i can t believe you posted this you re nut <EOS>\n",
      "    DEV dec train predicted > i m t wait i i this <EOS> d gon <EOS>\n",
      "    DEV dec train infer > i m sure you got a food one <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11559\n",
      "learning rate 0.00080683\n",
      "epoch 23\n",
      "batch 150\n",
      "training minibatch loss: 2.8767459392547607\n",
      "  sample 1:\n",
      "    enc input           > nobody is doing anything about it what action can be taken ? none ? <EOS>\n",
      "    dec input           > the fbi ha prosecuted american for compromising information far le classified than what clinton and her staff <EOS>\n",
      "    dec train predicted > europe fbi are prosecuted kaepernick about compromising staff compromising classified classified than the are <EOS> thi staff <EOS>\n",
      "dev minibatch loss: 7.762117385864258\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > when one wields the spatula that is rebase i all commits become clay <EOS>\n",
      "    DEV dec input           > i rebase i all day it s the best workflow imho <EOS>\n",
      "    DEV dec train predicted > jesus guess not believe be what is not same of game <EOS>\n",
      "    DEV dec train infer > jesus moderate the classic debate in the civil <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11609\n",
      "learning rate 0.000806082\n",
      "epoch 23\n",
      "batch 200\n",
      "training minibatch loss: 2.9712278842926025\n",
      "  sample 1:\n",
      "    enc input           > odds of being killed by refugee in billion falling down stair in car accident in <EOS>\n",
      "    dec input           > lie like a child trump s wedding wa in gift to clinton foundation were in <EOS>\n",
      "    dec train predicted > clinton in clinton clinton clinton is wife return in a he lie foundation in in case\n",
      "dev minibatch loss: 6.895936489105225\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we can help you in design and development of your website at reasonable price visit our portfolio <EOS>\n",
      "    DEV dec input           > i need someone to make me a website <EOS>\n",
      "    DEV dec train predicted > i know to to create a website website <EOS>\n",
      "    DEV dec train infer > i want a website stake from group asap <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11659\n",
      "learning rate 0.000805334\n",
      "epoch 23\n",
      "batch 250\n",
      "training minibatch loss: 2.9931163787841797\n",
      "  sample 1:\n",
      "    enc input           > he s just awful weld should have taken prez lead gj is lost and sniffling ? <EOS>\n",
      "    dec input           > really wonder what trump of clinton s answer would be to this question it s a good question <EOS>\n",
      "    dec train predicted > lester really he trump is a is choice ? be a a question ? s <EOS> few poll <EOS>\n",
      "dev minibatch loss: 7.353829860687256\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i m going for sure lmao i wana see you guy there <EOS>\n",
      "    DEV dec input           > yoo let be out <EOS>\n",
      "    DEV dec train predicted > i is s out to\n",
      "    DEV dec train infer > when i m back to halloween <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11709\n",
      "learning rate 0.000804586\n",
      "epoch 23\n",
      "batch 300\n",
      "training minibatch loss: 3.114495038986206\n",
      "  sample 1:\n",
      "    enc input           > faux hawk <EOS>\n",
      "    dec input           > want to try a new hairstyle <EOS>\n",
      "    dec train predicted > want to be to new hairstyle <EOS>\n",
      "dev minibatch loss: 8.605547904968262\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the only objective person fearless not another <EOS>\n",
      "    DEV dec input           > there is no better term to describe american who would vote for a blatant ignorant racist <EOS>\n",
      "    DEV dec train predicted > i are cry cry attack <EOS> cry <EOS> i say be <EOS> cry die act act <EOS>\n",
      "    DEV dec train infer > i ll be a man i ll be a man <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 11759\n",
      "learning rate 0.00080384\n",
      "epoch 23\n",
      "batch 350\n",
      "training minibatch loss: 3.0387656688690186\n",
      "  sample 1:\n",
      "    enc input           > just remember she will get richer amp you will get poorer because of it <EOS>\n",
      "    dec input           > that s what basically what trump doe but whatever hahahahahaha viva clinton <EOS>\n",
      "    dec train predicted > why s why why that hahahahahaha is a hahahahahaha hahahahahaha viva shit <EOS>\n",
      "dev minibatch loss: 7.5887627601623535\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > colin powell real housewife of the potomac ? <EOS>\n",
      "    DEV dec input           > powell will be going to fewer dinner party in washington <EOS>\n",
      "    DEV dec train predicted > horse that the a to be <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > the nazi department in nazi shift in nazi nazi whatever nazi commissioner <EOS> <EOS>\n",
      "global_step: 11809\n",
      "learning rate 0.000803094\n",
      "epoch 23\n",
      "batch 400\n",
      "training minibatch loss: 3.0731260776519775\n",
      "  sample 1:\n",
      "    enc input           > i took a nap but woke up to see y all doing all right <EOS>\n",
      "    dec input           > i want better for u let me come cheer with y all <EOS>\n",
      "    dec train predicted > update m to with vega cheer you cheer cheer for you all <EOS>\n",
      "dev minibatch loss: 6.84931755065918\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > this is utica ny <EOS>\n",
      "    DEV dec input           > where un upstate ? <EOS>\n",
      "    DEV dec train predicted > i i <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i m going to go to laser <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11859\n",
      "learning rate 0.000802348\n",
      "epoch 23\n",
      "batch 450\n",
      "training minibatch loss: 2.8476994037628174\n",
      "  sample 1:\n",
      "    enc input           > she is a prostitute so bill know her personally eh <EOS>\n",
      "    dec input           > donald trump called her miss piggy and miss housekeeping her name is alicia machado <EOS>\n",
      "    dec train predicted > trump trump called her support piggy and support housekeeping her clinton is alicia machado <EOS>\n",
      "dev minibatch loss: 8.450037956237793\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > im live on cnn <EOS>\n",
      "    DEV dec input           > everyone turn on cnn and keep watching im about to go live <EOS>\n",
      "    DEV dec train predicted > the s the the <EOS> the the the friend the the the <EOS>\n",
      "    DEV dec train infer > the s of the world s only the best of the world is a milo known the best <EOS>\n",
      "global_step: 11905\n",
      "learning rate 0.000801663\n",
      "epoch 24\n",
      "batch 0\n",
      "training minibatch loss: 2.855083703994751\n",
      "  sample 1:\n",
      "    enc input           > that s an awesome idea <EOS>\n",
      "    dec input           > this seems like an amazing opportunity to co create <EOS>\n",
      "    dec train predicted > the is like a of account to be see the\n",
      "dev minibatch loss: 7.726210117340088\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > awww i felt the same way when he followed me <EOS>\n",
      "    DEV dec input           > yes but it wasn t but honestly i wasn t ready like at all <EOS>\n",
      "    DEV dec train predicted > okay the i s t the the much m t going to the the the\n",
      "    DEV dec train infer > hey the family <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 11955\n",
      "learning rate 0.000800919\n",
      "epoch 24\n",
      "batch 50\n",
      "training minibatch loss: 3.0131664276123047\n",
      "  sample 1:\n",
      "    enc input           > only if it mean we can have cute study date in the common room <EOS>\n",
      "    dec input           > welcome to the family accept your destiny <EOS>\n",
      "    dec train predicted > huge to the largest share the destiny sincerely\n",
      "dev minibatch loss: 7.523578643798828\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > ty i think it ll increase like mad time to replace workspace with cc <EOS>\n",
      "    DEV dec input           > thought you might be interested in this infographic here it might be helpful to what you re doing <EOS>\n",
      "    DEV dec train predicted > strongly that re missing closing to the is <EOS> <EOS> is not <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i don t have a juno ride and missing then my apartment are missing <EOS> <EOS> <EOS>\n",
      "global_step: 12005\n",
      "learning rate 0.000800176\n",
      "epoch 24\n",
      "batch 100\n",
      "training minibatch loss: 2.850572109222412\n",
      "  sample 1:\n",
      "    enc input           > my mom is here too <EOS>\n",
      "    dec input           > about to watch with my mom we reset her twitter password in case she need to tweet <EOS>\n",
      "    dec train predicted > i the be my a mind in ll it than and in warning <EOS> ll to be <EOS>\n",
      "dev minibatch loss: 7.651374340057373\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > straight up <EOS>\n",
      "    DEV dec input           > lord paulie just put the key in lol <EOS>\n",
      "    DEV dec train predicted > i it it a up new to the <EOS>\n",
      "    DEV dec train infer > i m not going to read the whole phone <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12055\n",
      "learning rate 0.000799433\n",
      "epoch 24\n",
      "batch 150\n",
      "training minibatch loss: 2.734278678894043\n",
      "  sample 1:\n",
      "    enc input           > and you never know maybe one day i will enjoy some texas <EOS>\n",
      "    dec input           > impossible i thoroughly enjoy my ny proper pizza <EOS>\n",
      "    dec train predicted > totally to thoroughly thoroughly me thoroughly <EOS> pizza <EOS>\n",
      "dev minibatch loss: 6.854365348815918\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > let the game begin <EOS>\n",
      "    DEV dec input           > i usually don t play lmao but i feel like playing tonight <EOS>\n",
      "    DEV dec train predicted > i m remember t get friday <EOS> i m on the <EOS> <EOS>\n",
      "    DEV dec train infer > bumgarner can not wait to get off stranger hairstyle ? <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12105\n",
      "learning rate 0.000798691\n",
      "epoch 24\n",
      "batch 200\n",
      "training minibatch loss: 2.6932384967803955\n",
      "  sample 1:\n",
      "    enc input           > were you there ? did you feel it ? <EOS>\n",
      "    dec input           > just a couple of block from my nyc office scary stuff <EOS>\n",
      "    dec train predicted > i to distribution of my to a cocktail hat hat stuff <EOS>\n",
      "dev minibatch loss: 8.48994255065918\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > split two way <EOS>\n",
      "    DEV dec input           > continuing to look through old invoice and realized my first rate wa hr what wa yours ? <EOS>\n",
      "    DEV dec train predicted > i to be out <EOS> <EOS> <EOS> jean <EOS> jean <EOS> are <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > no at least we know how to be complete <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12155\n",
      "learning rate 0.00079795\n",
      "epoch 24\n",
      "batch 250\n",
      "training minibatch loss: 3.1087417602539062\n",
      "  sample 1:\n",
      "    enc input           > holy shit thats so nice wtf i you <EOS>\n",
      "    dec input           > how is it even possible to be that gorgeous i just <EOS>\n",
      "    dec train predicted > this s this i <EOS> <EOS> see fucking <EOS> <EOS> m will\n",
      "dev minibatch loss: 8.068711280822754\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i like it very sexy for you momma <EOS>\n",
      "    DEV dec input           > someone said this dress is too risqu for what do you think ? <EOS>\n",
      "    DEV dec train predicted > so else it is ? <EOS> overhyped <EOS> my i you think <EOS> <EOS>\n",
      "    DEV dec train infer > so much of my whisper i m trapped in night world <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12205\n",
      "learning rate 0.00079721\n",
      "epoch 24\n",
      "batch 300\n",
      "training minibatch loss: 2.8554067611694336\n",
      "  sample 1:\n",
      "    enc input           > thanks dear friend <EOS>\n",
      "    dec input           > thank you have a nice evening <EOS>\n",
      "    dec train predicted > happy you <EOS> to great day <EOS>\n",
      "dev minibatch loss: 7.816570281982422\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > so totally they didn t survive that try in the zoo <EOS>\n",
      "    DEV dec input           > don t judge someone based on their past they don t live there anymore <EOS>\n",
      "    DEV dec train predicted > and t have the or in the ball team can t care <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > lol that is nothing enough but they involved it s embarrassing <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12255\n",
      "learning rate 0.00079647\n",
      "epoch 24\n",
      "batch 350\n",
      "training minibatch loss: 2.7952184677124023\n",
      "  sample 1:\n",
      "    enc input           > love u girl thank you <EOS>\n",
      "    dec input           > happy sweet girl love and miss you enjoy it <EOS>\n",
      "    dec train predicted > happy birthday friend love to love you <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 8.106003761291504\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > good grief which company is this ? <EOS>\n",
      "    DEV dec input           > to continue the botanical theme petal on floor lamp <EOS>\n",
      "    DEV dec train predicted > i you that panty of <EOS> <EOS> bedtime <EOS> <EOS>\n",
      "    DEV dec train infer > i m late to be chill for him <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 12305\n",
      "learning rate 0.000795731\n",
      "epoch 24\n",
      "batch 400\n",
      "training minibatch loss: 3.1868107318878174\n",
      "  sample 1:\n",
      "    enc input           > haha thank you <EOS>\n",
      "    dec input           > you get down in the kitchen <EOS>\n",
      "    dec train predicted > happy re a <EOS> the beautiful <EOS>\n",
      "dev minibatch loss: 7.955255031585693\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > will ticket be electronic downloads <EOS>\n",
      "    DEV dec input           > tomorrow you can grab early bird ticket at am pacific for at band <EOS>\n",
      "    DEV dec train predicted > tix we re gr the <EOS> see we the we saturday the the <EOS>\n",
      "    DEV dec train infer > join your resume will be available <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12355\n",
      "learning rate 0.000794992\n",
      "epoch 24\n",
      "batch 450\n",
      "training minibatch loss: 3.034193515777588\n",
      "  sample 1:\n",
      "    enc input           > we were comparing them and i showed why bush is even worse than trump <EOS>\n",
      "    dec input           > bush isn t on the ballot <EOS>\n",
      "    dec train predicted > mystical would t a the ballot that\n",
      "dev minibatch loss: 7.632726192474365\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > here what i came up with b coffee <EOS>\n",
      "    DEV dec input           > there wa no tampering it wa people who had a negative opinion none of which wa actually used <EOS>\n",
      "    DEV dec train predicted > this s cheating kind and s been changed played a good jersey <EOS> <EOS> me s <EOS> <EOS> to\n",
      "    DEV dec train infer > this is really really really really really really similar to the shield is in political shift <EOS>\n",
      "Saving session\n",
      "global_step: 12401\n",
      "learning rate 0.000794313\n",
      "epoch 25\n",
      "batch 0\n",
      "training minibatch loss: 2.979462146759033\n",
      "  sample 1:\n",
      "    enc input           > forreal lmao bank boutta drop <EOS>\n",
      "    dec input           > he boutta get that part <EOS>\n",
      "    dec train predicted > i s popped it day <EOS>\n",
      "dev minibatch loss: 8.285324096679688\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > omfg youre so cute i love you sm <EOS>\n",
      "    DEV dec input           > we gon na fail chem together but at least she s gon na look good while doing it <EOS>\n",
      "    DEV dec train predicted > i re na love you <EOS> <EOS> i <EOS> i s <EOS> na love <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > my son friend and my favs <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12451\n",
      "learning rate 0.000793576\n",
      "epoch 25\n",
      "batch 50\n",
      "training minibatch loss: 2.7006711959838867\n",
      "  sample 1:\n",
      "    enc input           > that s why they won last year lmao idiot <EOS>\n",
      "    dec input           > he s an qb nothing more <EOS>\n",
      "    dec train predicted > why s a chair even on <EOS>\n",
      "dev minibatch loss: 7.244277477264404\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > it s usually hour on tuesday and thursday i m not sure about today s stream <EOS>\n",
      "    DEV dec input           > how long is your regular schedule is going to last <EOS>\n",
      "    DEV dec train predicted > no finally is it favorite miracle <EOS> getting to <EOS> home\n",
      "    DEV dec train infer > thank you so much for my nose <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12501\n",
      "learning rate 0.00079284\n",
      "epoch 25\n",
      "batch 100\n",
      "training minibatch loss: 2.695777654647827\n",
      "  sample 1:\n",
      "    enc input           > i am so goddamn tired don t do this to yourselves kid <EOS>\n",
      "    dec input           > and i m still up a if i don t have an am tomorrow <EOS>\n",
      "    dec train predicted > i i m not a to lot i m t make a hour tbh <EOS>\n",
      "dev minibatch loss: 7.532867908477783\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we had to catch a flight at midnight <EOS>\n",
      "    DEV dec input           > why didn t i get hang with you guy more <EOS>\n",
      "    DEV dec train predicted > sound i t you come to on sound <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > this is so a good episode about to <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12551\n",
      "learning rate 0.000792104\n",
      "epoch 25\n",
      "batch 150\n",
      "training minibatch loss: 2.654991626739502\n",
      "  sample 1:\n",
      "    enc input           > isn t that what the iphone seven is for ? <EOS>\n",
      "    dec input           > youd have to see it <EOS>\n",
      "    dec train predicted > youd have to be what <EOS>\n",
      "dev minibatch loss: 7.614167213439941\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i ll try it next time <EOS>\n",
      "    DEV dec input           > the pork spicy taco across from tasty pot is pretty fuckin fire too <EOS>\n",
      "    DEV dec train predicted > get only park had is all <EOS> <EOS> <EOS> <EOS> sweet sweet <EOS> <EOS>\n",
      "    DEV dec train infer > i m so sweet before i ll be so happy <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12601\n",
      "learning rate 0.000791369\n",
      "epoch 25\n",
      "batch 200\n",
      "training minibatch loss: 2.8106343746185303\n",
      "  sample 1:\n",
      "    enc input           > take a deep breath and move forward the primary are over <EOS>\n",
      "    dec input           > nevada had everything to do with momentum moving forward context is crucial <EOS>\n",
      "    dec train predicted > nevada can to to hear for momentum coming forward amp is bringing <EOS>\n",
      "dev minibatch loss: 8.03951358795166\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > chapman field the magical marsh <EOS>\n",
      "    DEV dec input           > oh my gosh when did u take the first one ? and where ? i love it <EOS>\n",
      "    DEV dec train predicted > look i glass i you you win them same time to <EOS> <EOS> doe <EOS> ll them <EOS>\n",
      "    DEV dec train infer > hardly i ve came to find me in the reality reality party are not banana <EOS> <EOS>\n",
      "global_step: 12651\n",
      "learning rate 0.000790635\n",
      "epoch 25\n",
      "batch 250\n",
      "training minibatch loss: 2.821737289428711\n",
      "  sample 1:\n",
      "    enc input           > im still an aries cusp but <EOS>\n",
      "    dec input           > tf im a libra now ? not the business <EOS>\n",
      "    dec train predicted > i i not libra i <EOS> <EOS> me libra <EOS>\n",
      "dev minibatch loss: 7.9815993309021\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > but when i get you to myself you know what s going down what s going down <EOS>\n",
      "    DEV dec input           > everything for a reason there s thing you had to learn from them <EOS>\n",
      "    DEV dec train predicted > i turnover id id i s me you have me take me me <EOS>\n",
      "    DEV dec train infer > is my tat with the seat michael to see you seen me <EOS> <EOS> <EOS>\n",
      "global_step: 12701\n",
      "learning rate 0.000789901\n",
      "epoch 25\n",
      "batch 300\n",
      "training minibatch loss: 2.9525909423828125\n",
      "  sample 1:\n",
      "    enc input           > i ll be on around i m at work right now <EOS>\n",
      "    dec input           > got ta raid tonight if you guy can get on <EOS>\n",
      "    dec train predicted > gr ta raid you we you re can want together sf\n",
      "dev minibatch loss: 7.563505172729492\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > nothing to do with smart everything to do with money and marketing <EOS>\n",
      "    DEV dec input           > brilliant and proof that we aren t getting any smarter are we ? <EOS>\n",
      "    DEV dec train predicted > sold council striking i happens re t ketchup censoring airtime to a chasing <EOS>\n",
      "    DEV dec train infer > the mac of mac <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12751\n",
      "learning rate 0.000789168\n",
      "epoch 25\n",
      "batch 350\n",
      "training minibatch loss: 2.904188394546509\n",
      "  sample 1:\n",
      "    enc input           > you re welcome joy hope to have you join the family very soon <EOS>\n",
      "    dec input           > thanks to both tom amp patrick for the open house and tour <EOS>\n",
      "    dec train predicted > great to the <EOS> journey patrick <EOS> the best tour thanks catcher <EOS>\n",
      "dev minibatch loss: 8.229866027832031\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > this is a beautiful story to <EOS>\n",
      "    DEV dec input           > this penguin swim mile every year for reunion with the man who saved his life <EOS>\n",
      "    DEV dec train predicted > a artist is is is year and month <EOS> god hollywood <EOS> is my life life\n",
      "    DEV dec train infer > this is huge jacket in dev black <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12801\n",
      "learning rate 0.000788435\n",
      "epoch 25\n",
      "batch 400\n",
      "training minibatch loss: 2.7665183544158936\n",
      "  sample 1:\n",
      "    enc input           > yes i love that table have fun explore the taxidermy museum <EOS>\n",
      "    dec input           > i m so excited to work on top of a circle kimmel harding nelson center for the <EOS>\n",
      "    dec train predicted > i m visiting jealous to see on netflix for the comic kimmel harding nelson tag for my tag\n",
      "dev minibatch loss: 7.890956401824951\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > this is like nyc art shithead mad libs <EOS>\n",
      "    DEV dec input           > my favorite place to people watch is outside the gowanus textile art zen center <EOS>\n",
      "    DEV dec train predicted > well pay number to pay dont this a this website of <EOS> i <EOS> <EOS>\n",
      "    DEV dec train infer > website in a website for a report report ? <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 12851\n",
      "learning rate 0.000787704\n",
      "epoch 25\n",
      "batch 450\n",
      "training minibatch loss: 2.620370388031006\n",
      "  sample 1:\n",
      "    enc input           > you re welcome <EOS>\n",
      "    dec input           > wow thx hun <EOS>\n",
      "    dec train predicted > thanks i for <EOS>\n",
      "dev minibatch loss: 7.128297328948975\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > ok so this is just like fear factor <EOS>\n",
      "    DEV dec input           > currently writing a book on what not to do on first date from experience will update <EOS>\n",
      "    DEV dec train predicted > a and a people by my people a wear my my people and my <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > a turned in a new cookie learning to wear this music <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12897\n",
      "learning rate 0.000787031\n",
      "epoch 26\n",
      "batch 0\n",
      "training minibatch loss: 2.8697566986083984\n",
      "  sample 1:\n",
      "    enc input           > i didn t even have to scroll to find a misogynist retweet on that dude s timeline <EOS>\n",
      "    dec input           > if your different opinion is antisemitism then that make you a bigot it s very simple <EOS>\n",
      "    dec train predicted > i you friend episode is antisemitism antisemitism you i you you bigot <EOS> s a simple <EOS>\n",
      "dev minibatch loss: 8.096831321716309\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > haha thanks man i do try grant know it s true hence the silence <EOS>\n",
      "    DEV dec input           > i got ta meet this guy chirp are legendary <EOS>\n",
      "    DEV dec train predicted > it thought the be you time <EOS> the a <EOS>\n",
      "    DEV dec train infer > you are a good day <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 12947\n",
      "learning rate 0.000786301\n",
      "epoch 26\n",
      "batch 50\n",
      "training minibatch loss: 2.5651683807373047\n",
      "  sample 1:\n",
      "    enc input           > we very much were <EOS>\n",
      "    dec input           > we were <EOS>\n",
      "    dec train predicted > i are a\n",
      "dev minibatch loss: 8.936763763427734\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > knock him dead literally for preference but figuratively is equally acceptable <EOS>\n",
      "    DEV dec input           > let s do this <EOS>\n",
      "    DEV dec train predicted > they s do that kind\n",
      "    DEV dec train infer > that s the weak is a weak dude can t k k right now <EOS> <EOS> <EOS>\n",
      "global_step: 12997\n",
      "learning rate 0.000785571\n",
      "epoch 26\n",
      "batch 100\n",
      "training minibatch loss: 2.68774676322937\n",
      "  sample 1:\n",
      "    enc input           > the ba rd had the gall to lie abt it incriminate hrc all gop r corrupt <EOS>\n",
      "    dec input           > so colin powell did purposely use private email and device amp did advice by via <EOS>\n",
      "    dec train predicted > obamas colin powell at the matter email email for email event backed t for <EOS> <EOS>\n",
      "dev minibatch loss: 8.635048866271973\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > thank you god bless u din <EOS>\n",
      "    DEV dec input           > happy birthday ate nicey enjoy your day po and god bless <EOS>\n",
      "    DEV dec train predicted > happy birthday following amp re the family <EOS> <EOS> you <EOS> <EOS>\n",
      "    DEV dec train infer > happy birthday andrew <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 13047\n",
      "learning rate 0.000784842\n",
      "epoch 26\n",
      "batch 150\n",
      "training minibatch loss: 2.635627508163452\n",
      "  sample 1:\n",
      "    enc input           > one of the best debut album she came out with gun blazing <EOS>\n",
      "    dec input           > this album wa flaw le <EOS>\n",
      "    dec train predicted > this stuff flaw flaw flaw flaw\n",
      "dev minibatch loss: 7.9987592697143555\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > better than fap meat <EOS>\n",
      "    DEV dec input           > flap meat ? no i m good thanks <EOS>\n",
      "    DEV dec train predicted > the year <EOS> <EOS> <EOS> could not to for\n",
      "    DEV dec train infer > raspberry electrocuted to the newborn of the hottest trip <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 13097\n",
      "learning rate 0.000784114\n",
      "epoch 26\n",
      "batch 200\n",
      "training minibatch loss: 2.8421733379364014\n",
      "  sample 1:\n",
      "    enc input           > thx carmen lt u da best <EOS>\n",
      "    dec input           > hbd this seems like the appropriate social medium platform for such a post <EOS>\n",
      "    dec train predicted > hbd how seems seems a appropriate medium medium platform is the a couple <EOS>\n",
      "dev minibatch loss: 8.734498023986816\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > thanks for sharing jos <EOS>\n",
      "    DEV dec input           > how not to define influencer marketing by va <EOS>\n",
      "    DEV dec train predicted > how are wa be you for article <EOS> <EOS>\n",
      "    DEV dec train infer > it s a workforce fun how they re married to the world <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 13147\n",
      "learning rate 0.000783386\n",
      "epoch 26\n",
      "batch 250\n",
      "training minibatch loss: 2.654970407485962\n",
      "  sample 1:\n",
      "    enc input           > from their website look legit to me <EOS>\n",
      "    dec input           > doe somebody have the actual written policy program ? i can t seem to find it <EOS>\n",
      "    dec train predicted > he the had a anti rest policy question he he can t wait to think it <EOS>\n",
      "dev minibatch loss: 7.914003849029541\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > ok but do you understand jellyfish are magic <EOS>\n",
      "    DEV dec input           > no hit them with a shovel and bury that bitch <EOS>\n",
      "    DEV dec train predicted > and seems meat in me parallel in parallel the a seems\n",
      "    DEV dec train infer > seems like the audio show you re like ourselves to help the flag <EOS> <EOS> <EOS>\n",
      "global_step: 13197\n",
      "learning rate 0.000782659\n",
      "epoch 26\n",
      "batch 300\n",
      "training minibatch loss: 2.5725460052490234\n",
      "  sample 1:\n",
      "    enc input           > this scam is still going on ? <EOS>\n",
      "    dec input           > a draftkings contractor won over million playing draftkings this weekend <EOS>\n",
      "    dec train predicted > the draftkings contractor had a a draftkings draftkings <EOS> weekend <EOS>\n",
      "dev minibatch loss: 7.360091686248779\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > once come out of stealth it ll be hard to imagine number without it <EOS>\n",
      "    DEV dec input           > three word ctrl shift enter matrix function ed into oblivion <EOS>\n",
      "    DEV dec train predicted > watch decline up to to to <EOS> <EOS> <EOS> the i\n",
      "    DEV dec train infer > update is dead i m overrated in the rotation <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 13247\n",
      "learning rate 0.000781933\n",
      "epoch 26\n",
      "batch 350\n",
      "training minibatch loss: 2.917328119277954\n",
      "  sample 1:\n",
      "    enc input           > haha thanks kyle <EOS>\n",
      "    dec input           > happy bday to the homie <EOS>\n",
      "    dec train predicted > happy bday for my homie <EOS>\n",
      "dev minibatch loss: 7.396860599517822\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > community need more knowledge of stuff like this melee player knew the frame of everything in lol <EOS>\n",
      "    DEV dec input           > learn something new every day <EOS>\n",
      "    DEV dec train predicted > so to ocean other other in\n",
      "    DEV dec train infer > sunshine are extremely awake when are other uber management <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 13297\n",
      "learning rate 0.000781207\n",
      "epoch 26\n",
      "batch 400\n",
      "training minibatch loss: 2.854018211364746\n",
      "  sample 1:\n",
      "    enc input           > it s fucked up that the most popular tweet invoking me is this <EOS>\n",
      "    dec input           > what a sick dude <EOS>\n",
      "    dec train predicted > this the the <EOS> i\n",
      "dev minibatch loss: 7.759518146514893\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > oh that is not mine i could never live like that <EOS>\n",
      "    DEV dec input           > the delete button is your friend if it s important they ll send it again <EOS>\n",
      "    DEV dec train predicted > i same of is a nose <EOS> i wa fucked a do be me a <EOS>\n",
      "    DEV dec train infer > i m literally jealous listening to my shower <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 13347\n",
      "learning rate 0.000780482\n",
      "epoch 26\n",
      "batch 450\n",
      "training minibatch loss: 3.0009121894836426\n",
      "  sample 1:\n",
      "    enc input           > the warrior would still win <EOS>\n",
      "    dec input           > harambe will win mvp mark my word <EOS>\n",
      "    dec train predicted > harambe is be mvp mvp the new <EOS>\n",
      "dev minibatch loss: 7.9783759117126465\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > but what if god made evolution ? <EOS>\n",
      "    DEV dec input           > god claim to be the creator evolution claim to come up from the slime origin of life <EOS>\n",
      "    DEV dec train predicted > immunity rating within kill section floor of <EOS> <EOS> kill <EOS> <EOS> the planet <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > donald trump shoot with a birth security and dc security news of th largest grade is <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 13393\n",
      "learning rate 0.000779816\n",
      "epoch 27\n",
      "batch 0\n",
      "training minibatch loss: 2.412706136703491\n",
      "  sample 1:\n",
      "    enc input           > which too often end up with the pipe and elbow patch look of i take myself extremely seriously <EOS>\n",
      "    dec input           > i like the what the fuck did you just say to me ? one <EOS>\n",
      "    dec train predicted > i m you one you one is i think ever it you ? <EOS> <EOS>\n",
      "dev minibatch loss: 8.426342010498047\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i believe the last of poll have shown hrc ahead to among lvs while reuters showed <EOS>\n",
      "    DEV dec input           > after the two week just had this ha to be tough news for the team <EOS>\n",
      "    DEV dec train predicted > i the fuck adult i ignorant a year a be a into <EOS> year uk <EOS>\n",
      "    DEV dec train infer > why i consider the trick people like you using the usa a trick <EOS> <EOS> <EOS>\n",
      "global_step: 13443\n",
      "learning rate 0.000779092\n",
      "epoch 27\n",
      "batch 50\n",
      "training minibatch loss: 2.5669734477996826\n",
      "  sample 1:\n",
      "    enc input           > we all been there <EOS>\n",
      "    dec input           > real niggaz around the world are relating <EOS>\n",
      "    dec train predicted > you niggaz to the world stay relating <EOS>\n",
      "dev minibatch loss: 8.1466646194458\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > who care about pitt ? <EOS>\n",
      "    DEV dec input           > he take this very seriously source claim that pitt abused kid are exaggerated or fabricated <EOS>\n",
      "    DEV dec train predicted > we need out gender brilliant went foundation to attractive is <EOS> <EOS> form <EOS> went <EOS>\n",
      "    DEV dec train infer > clinton say people fargo benefit <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 13493\n",
      "learning rate 0.000778369\n",
      "epoch 27\n",
      "batch 100\n",
      "training minibatch loss: 2.5542376041412354\n",
      "  sample 1:\n",
      "    enc input           > when the waiter asks together or separate and he say together <EOS>\n",
      "    dec input           > caption this <EOS>\n",
      "    dec train predicted > caption this one\n",
      "dev minibatch loss: 8.151070594787598\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that is pretty fucking drunk <EOS>\n",
      "    DEV dec input           > i am a drunk a it is appropriate to be given the circumstance <EOS>\n",
      "    DEV dec train predicted > just m wearing speed of new is a <EOS> offer a a debate <EOS>\n",
      "    DEV dec train infer > just turn out the debate of a new phone in the uk possibly ever <EOS>\n",
      "global_step: 13543\n",
      "learning rate 0.000777647\n",
      "epoch 27\n",
      "batch 150\n",
      "training minibatch loss: 2.5664374828338623\n",
      "  sample 1:\n",
      "    enc input           > so i just had to evacuate my house <EOS>\n",
      "    dec input           > why is someone ringing my doorbell <EOS>\n",
      "    dec train predicted > i should why ringing my doorbell <EOS>\n",
      "dev minibatch loss: 9.287848472595215\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > look at all the people who make being unpleasant a life choice have fun with that <EOS>\n",
      "    DEV dec input           > it s undisputed fact <EOS>\n",
      "    DEV dec train predicted > i s a to that\n",
      "    DEV dec train infer > a bush should not stupid like the internet truly losing <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 13593\n",
      "learning rate 0.000776925\n",
      "epoch 27\n",
      "batch 200\n",
      "training minibatch loss: 2.7460224628448486\n",
      "  sample 1:\n",
      "    enc input           > this season is already testing our team <EOS>\n",
      "    dec input           > report garoppolo out for thursday amp beyond <EOS>\n",
      "    dec train predicted > wow garoppolo on for code s th <EOS>\n",
      "dev minibatch loss: 8.060301780700684\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > brand new tune <EOS>\n",
      "    DEV dec input           > rihanna you da one sur radio life coute <EOS>\n",
      "    DEV dec train predicted > ebei wa have ready to is s s been\n",
      "    DEV dec train infer > ebei video on ely soro from prod beatz amp now <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 13643\n",
      "learning rate 0.000776204\n",
      "epoch 27\n",
      "batch 250\n",
      "training minibatch loss: 2.9201085567474365\n",
      "  sample 1:\n",
      "    enc input           > then is a crooked a and should resign <EOS>\n",
      "    dec input           > james comey director reject call to reopen email case <EOS>\n",
      "    dec train predicted > james comey director director director to reopen email woman <EOS> ?\n",
      "dev minibatch loss: 8.026883125305176\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what the ever loving fuck <EOS>\n",
      "    DEV dec input           > mark halperin just said clinton never fought successfully in a presidential campaign to win <EOS>\n",
      "    DEV dec train predicted > i the kicker killed the s killed the in the train movie that license <EOS>\n",
      "    DEV dec train infer > i m not a heavy town <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 13693\n",
      "learning rate 0.000775484\n",
      "epoch 27\n",
      "batch 300\n",
      "training minibatch loss: 2.585378646850586\n",
      "  sample 1:\n",
      "    enc input           > can t wait to meet up with you at <EOS>\n",
      "    dec input           > which of you fabulous people will i be seeing at ? <EOS>\n",
      "    dec train predicted > join is you fabulous is are you be to to the <EOS>\n",
      "dev minibatch loss: 8.18338394165039\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > straight up <EOS>\n",
      "    DEV dec input           > lord paulie just put the key in lol <EOS>\n",
      "    DEV dec train predicted > well this this a me box <EOS> the <EOS>\n",
      "    DEV dec train infer > well this is a fuck for this year i can t wait for the world <EOS>\n",
      "global_step: 13743\n",
      "learning rate 0.000774764\n",
      "epoch 27\n",
      "batch 350\n",
      "training minibatch loss: 2.7959396839141846\n",
      "  sample 1:\n",
      "    enc input           > ty in that case pls enjoy join <EOS>\n",
      "    dec input           > i think he want to make the movie <EOS>\n",
      "    dec train predicted > i m i s to be the bagel in\n",
      "dev minibatch loss: 8.721142768859863\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > burn the witch <EOS>\n",
      "    DEV dec input           > the danger of modern technology <EOS>\n",
      "    DEV dec train predicted > the main teacher a <EOS> make\n",
      "    DEV dec train infer > you re going to bring a great hat <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 13793\n",
      "learning rate 0.000774045\n",
      "epoch 27\n",
      "batch 400\n",
      "training minibatch loss: 2.7311084270477295\n",
      "  sample 1:\n",
      "    enc input           > design sprint sexual health sign up here through <EOS>\n",
      "    dec input           > design taboo subject so much more straight from the lt or more precisely uterus i m organizing this <EOS>\n",
      "    dec train predicted > forgive taboo subject just much to starting from the ball ? the iphone uterus i m organizing it <EOS>\n",
      "dev minibatch loss: 8.442154884338379\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > a did many fantasy player <EOS>\n",
      "    DEV dec input           > brandon marshall on knee scare which look like a sprain now i thought it wa over <EOS>\n",
      "    DEV dec train predicted > the the s the <EOS> me s like the bike stadium <EOS> m that s a <EOS>\n",
      "    DEV dec train infer > nice the new land ? <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 13843\n",
      "learning rate 0.000773327\n",
      "epoch 27\n",
      "batch 450\n",
      "training minibatch loss: 2.5497734546661377\n",
      "  sample 1:\n",
      "    enc input           > after that neo japan stunt i just wanted to make sure it wa still there <EOS>\n",
      "    dec input           > neo america <EOS>\n",
      "    dec train predicted > neo america <EOS>\n",
      "dev minibatch loss: 7.347307205200195\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > oh so you re one of those guy casey <EOS>\n",
      "    DEV dec input           > it still ha the engine in the wrong space but holy shit this thing look bonkers <EOS>\n",
      "    DEV dec train predicted > i s are to best <EOS> the noon place <EOS> i rn <EOS> weekend <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > i hate you <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 13889\n",
      "learning rate 0.000772666\n",
      "epoch 28\n",
      "batch 0\n",
      "training minibatch loss: 2.4287092685699463\n",
      "  sample 1:\n",
      "    enc input           > it s the original creator and voice actor <EOS>\n",
      "    dec input           > yeaaaaah i thought i heard that chance of being dookie tho <EOS>\n",
      "    dec train predicted > i i just i m roughly kid that my dookie <EOS> <EOS>\n",
      "dev minibatch loss: 7.43646240234375\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > how incompetent can we be ? don t blame on funding <EOS>\n",
      "    DEV dec input           > turmoil at the cdc while they re trying to fight new public health threat with no emergency funding <EOS>\n",
      "    DEV dec train predicted > protect letterman oracle oracle mode incentivize re ruled to protect tmrw than al disruption amp refute logo embarrassing <EOS>\n",
      "    DEV dec train infer > spend on instagram pls <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 13939\n",
      "learning rate 0.000771949\n",
      "epoch 28\n",
      "batch 50\n",
      "training minibatch loss: 2.712725877761841\n",
      "  sample 1:\n",
      "    enc input           > probably yeah <EOS>\n",
      "    dec input           > reading through the debate tweet on both side make me wonder if we have the candidate we deserve <EOS>\n",
      "    dec train predicted > we on the debate account <EOS> the candidate board me slightly we you were them same <EOS> deserve to\n",
      "dev minibatch loss: 8.651983261108398\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > thank you <EOS>\n",
      "    DEV dec input           > good luck girly <EOS>\n",
      "    DEV dec train predicted > happy one i <EOS>\n",
      "    DEV dec train infer > happy birthday to the amazing <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 13989\n",
      "learning rate 0.000771233\n",
      "epoch 28\n",
      "batch 100\n",
      "training minibatch loss: 2.564908266067505\n",
      "  sample 1:\n",
      "    enc input           > yeast sample for all <EOS>\n",
      "    dec input           > i got you next time i make my way to ny <EOS>\n",
      "    dec train predicted > i m to a ? i m the friend <EOS> my <EOS>\n",
      "dev minibatch loss: 8.079643249511719\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > ya phone from china watch from california ? <EOS>\n",
      "    DEV dec input           > they ship the individually ? that seems absurd <EOS>\n",
      "    DEV dec train predicted > i want lock weeknd message <EOS> s said <EOS>\n",
      "    DEV dec train infer > what s the ice is maddening <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14039\n",
      "learning rate 0.000770517\n",
      "epoch 28\n",
      "batch 150\n",
      "training minibatch loss: 2.8126938343048096\n",
      "  sample 1:\n",
      "    enc input           > thats great <EOS>\n",
      "    dec input           > i are handing out ice cream today at pm on hollywood blvd come hang <EOS>\n",
      "    dec train predicted > i m handing to to cream <EOS> <EOS> pm <EOS> dec blvd god hang <EOS>\n",
      "dev minibatch loss: 8.58265209197998\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > my pm ice cream is saying oh hai to your pm blue cheese <EOS>\n",
      "    DEV dec input           > eating a smoky blue cheese at est probably wont prove to be a great idea but eff it <EOS>\n",
      "    DEV dec train predicted > please for people time for are <EOS> <EOS> are want <EOS> go a good choice <EOS> not ? ?\n",
      "    DEV dec train infer > can t wait for the <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14089\n",
      "learning rate 0.000769802\n",
      "epoch 28\n",
      "batch 200\n",
      "training minibatch loss: 2.780654191970825\n",
      "  sample 1:\n",
      "    enc input           > with unfounded anxiety about vaccine or being scared of wifi signal ? <EOS>\n",
      "    dec input           > i wasn t there but i knocked clinton trump off the stage w my solution <EOS>\n",
      "    dec train predicted > i made t stopped that i just clinton trump off the solution in <EOS> solution <EOS>\n",
      "dev minibatch loss: 8.694698333740234\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > actually read this <EOS>\n",
      "    DEV dec input           > here are some actual fact stop making it worse the myth of black life matter via <EOS>\n",
      "    DEV dec train predicted > here s the of thing <EOS> and a v than th bay the <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > this is a great thing in the people who are a great thing <EOS> <EOS> <EOS>\n",
      "global_step: 14139\n",
      "learning rate 0.000769088\n",
      "epoch 28\n",
      "batch 250\n",
      "training minibatch loss: 2.5926525592803955\n",
      "  sample 1:\n",
      "    enc input           > hey i saw you in the park the other day <EOS>\n",
      "    dec input           > lizard on a walk <EOS>\n",
      "    dec train predicted > you on the bitch <EOS>\n",
      "dev minibatch loss: 8.088927268981934\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > lol also here s the setlist for tonight if you re interested <EOS>\n",
      "    DEV dec input           > ozzy v insistent on group clapping more later <EOS>\n",
      "    DEV dec train predicted > i to daddy right twitter <EOS> <EOS> daddy and\n",
      "    DEV dec train infer > don t even know that vegan taste like <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14189\n",
      "learning rate 0.000768374\n",
      "epoch 28\n",
      "batch 300\n",
      "training minibatch loss: 2.4990992546081543\n",
      "  sample 1:\n",
      "    enc input           > you re still waiting on that pony you asked your parent for when you were too right ? <EOS>\n",
      "    dec input           > i want a warm substantive policy discussion between the two potential leader of the free world <EOS>\n",
      "    dec train predicted > me agree a warm substantive policy slave potential my potential potential potential of potential hardest <EOS> <EOS>\n",
      "dev minibatch loss: 7.871346950531006\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > and if only a streamer can get her light up that might help kappa <EOS>\n",
      "    DEV dec input           > the feel when your fireteam play without their tv s on <EOS>\n",
      "    DEV dec train predicted > hired history just are focus will in be focus plug bragging themselves\n",
      "    DEV dec train infer > artist opposed to successful of themselves the successful share police are successful trump value of dollar <EOS>\n",
      "global_step: 14239\n",
      "learning rate 0.000767661\n",
      "epoch 28\n",
      "batch 350\n",
      "training minibatch loss: 2.5700933933258057\n",
      "  sample 1:\n",
      "    enc input           > thank you so much and yes spending it with andrea is the best gift <EOS>\n",
      "    dec input           > happy birthday haley what a great way to celebrate nyc <EOS>\n",
      "    dec train predicted > happy best haley <EOS> you great one to celebrate the <EOS>\n",
      "dev minibatch loss: 8.344114303588867\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > rightfully so since this is a candidate who actually had a viable chance of running amp possibly winning <EOS>\n",
      "    DEV dec input           > sander strongly discourages voting for a third party candidate <EOS>\n",
      "    DEV dec train predicted > who have be read and trump huge conference from who\n",
      "    DEV dec train infer > who know trump s chief issued a press issued totally we thought to be a sleeze <EOS>\n",
      "global_step: 14289\n",
      "learning rate 0.000766948\n",
      "epoch 28\n",
      "batch 400\n",
      "training minibatch loss: 2.835958957672119\n",
      "  sample 1:\n",
      "    enc input           > that gif wa tagged a violet on giphy and i wa so offended <EOS>\n",
      "    dec input           > honestly me of the time to begin with <EOS>\n",
      "    dec train predicted > just the of the time to begin with <EOS>\n",
      "dev minibatch loss: 8.347992897033691\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i actually have a headache haha <EOS>\n",
      "    DEV dec input           > hahaha a best i can <EOS>\n",
      "    DEV dec train predicted > peacefully i year fan thought bless\n",
      "    DEV dec train infer > dang boy <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14339\n",
      "learning rate 0.000766237\n",
      "epoch 28\n",
      "batch 450\n",
      "training minibatch loss: 2.647026300430298\n",
      "  sample 1:\n",
      "    enc input           > i didn t even have to scroll to find a misogynist retweet on that dude s timeline <EOS>\n",
      "    dec input           > if your different opinion is antisemitism then that make you a bigot it s very simple <EOS>\n",
      "    dec train predicted > i you bigot bigot is antisemitism bigot i is you a bigot i is true a <EOS>\n",
      "dev minibatch loss: 7.962343692779541\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > it wa a sparkle special it smell fire <EOS>\n",
      "    DEV dec input           > lmfaooo cut those cheek <EOS>\n",
      "    DEV dec train predicted > watching it the s <EOS>\n",
      "    DEV dec train infer > laughing is a bummer <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14385\n",
      "learning rate 0.000765582\n",
      "epoch 29\n",
      "batch 0\n",
      "training minibatch loss: 2.4734652042388916\n",
      "  sample 1:\n",
      "    enc input           > doe he have a twitter <EOS>\n",
      "    dec input           > lmaooo basically he so extra too im so excited to see how this go lmfao <EOS>\n",
      "    dec train predicted > poor i so zaid extra heaven i not extra for finish heaven he season emoji <EOS>\n",
      "dev minibatch loss: 8.16970443725586\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i refuse to take my wrist band off bc i can t part with the memory yet <EOS>\n",
      "    DEV dec input           > omg i would ve been sobbing on the ground for sure i love him <EOS>\n",
      "    DEV dec train predicted > and i m have missing to for my day <EOS> thanksgiving <EOS> get to and\n",
      "    DEV dec train infer > but i m so bored <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14435\n",
      "learning rate 0.000764872\n",
      "epoch 29\n",
      "batch 50\n",
      "training minibatch loss: 2.4432005882263184\n",
      "  sample 1:\n",
      "    enc input           > the same people outraged because his right r violated are the same who are outraged by using his <EOS>\n",
      "    dec input           > seattle mariner suspend catcher steve clevenger for the rest of the season after he posted controversial tweet <EOS>\n",
      "    dec train predicted > the mariner suspend catcher the clevenger for the rest of the controversial <EOS> the want controversial party <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 8.519397735595703\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > making broken research code work is a good learning experience <EOS>\n",
      "    DEV dec input           > it s awesome there implementation work too sadly most released paper have broken stuff <EOS>\n",
      "    DEV dec train predicted > i s a a really were <EOS> <EOS> <EOS> a expensive team in <EOS> <EOS>\n",
      "    DEV dec train infer > tapping bad agent <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14485\n",
      "learning rate 0.000764162\n",
      "epoch 29\n",
      "batch 100\n",
      "training minibatch loss: 2.7819766998291016\n",
      "  sample 1:\n",
      "    enc input           > my progress wa save the game itself wa uninstalled <EOS>\n",
      "    dec input           > that suck do you any of your game save on server and not the system ? <EOS>\n",
      "    dec train predicted > what suck what you get point the name ? on <EOS> ? cement better iphone <EOS> <EOS>\n",
      "dev minibatch loss: 7.82340145111084\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > glad to be here it s boo <EOS>\n",
      "    DEV dec input           > absolutely thrilled to talk all thing strategy brand and tomorrow at read more here <EOS>\n",
      "    DEV dec train predicted > brad the to see it the to <EOS> <EOS> brad <EOS> <EOS> <EOS> entertainment <EOS>\n",
      "    DEV dec train infer > my new york swag in saturday <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14535\n",
      "learning rate 0.000763453\n",
      "epoch 29\n",
      "batch 150\n",
      "training minibatch loss: 2.6175034046173096\n",
      "  sample 1:\n",
      "    enc input           > awwww thanks <EOS>\n",
      "    dec input           > you re blessed <EOS>\n",
      "    dec train predicted > yes re blessed <EOS>\n",
      "dev minibatch loss: 7.770832061767578\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > who s heart is more black tho ? <EOS>\n",
      "    DEV dec input           > show your partner how much you care with our relationship bracelet shop <EOS>\n",
      "    DEV dec train predicted > is a new i book book ll is book book amp <EOS> is\n",
      "    DEV dec train infer > i m so proud of the book i love it <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14585\n",
      "learning rate 0.000762744\n",
      "epoch 29\n",
      "batch 200\n",
      "training minibatch loss: 2.7379636764526367\n",
      "  sample 1:\n",
      "    enc input           > dunno i don t really blog any more but maybe <EOS>\n",
      "    dec input           > will you be writing a post ? <EOS>\n",
      "    dec train predicted > did you be honest ? trial ? ?\n",
      "dev minibatch loss: 8.318351745605469\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what did you shoot ? <EOS>\n",
      "    DEV dec input           > standing on tee needing a par to shoot how did i do ? <EOS>\n",
      "    DEV dec train predicted > hey into red to that bunch in throw my fast you throw <EOS> <EOS>\n",
      "    DEV dec train infer > hey imma really really really really fast added <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14635\n",
      "learning rate 0.000762037\n",
      "epoch 29\n",
      "batch 250\n",
      "training minibatch loss: 2.6539785861968994\n",
      "  sample 1:\n",
      "    enc input           > and i gave you a legit answer with a great hint <EOS>\n",
      "    dec input           > that wa a legit question <EOS>\n",
      "    dec train predicted > i s a opposed opposed on\n",
      "dev minibatch loss: 7.982003688812256\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > at the moment the mets all too familiar unfortunately <EOS>\n",
      "    DEV dec input           > i don t know what s going to kill me quicker the mets or diabetes <EOS>\n",
      "    DEV dec train predicted > the guess t think fair to others in spark <EOS> <EOS> <EOS> election <EOS> others <EOS>\n",
      "    DEV dec train infer > white thing the protest are disappointing in the protest <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14685\n",
      "learning rate 0.000761329\n",
      "epoch 29\n",
      "batch 300\n",
      "training minibatch loss: 2.531527519226074\n",
      "  sample 1:\n",
      "    enc input           > appreciate it fam <EOS>\n",
      "    dec input           > happy fuckin birthday to my bro <EOS>\n",
      "    dec train predicted > happy birthday birthday to the bday <EOS>\n",
      "dev minibatch loss: 9.662038803100586\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > not anti white just anti racist sister banging fucktards like you make all white look bad <EOS>\n",
      "    DEV dec input           > whitebread ? your mask is slipping anti white <EOS>\n",
      "    DEV dec train predicted > focus how how health you a out a health\n",
      "    DEV dec train infer > a long health queen is a queen <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14735\n",
      "learning rate 0.000760623\n",
      "epoch 29\n",
      "batch 350\n",
      "training minibatch loss: 2.6362979412078857\n",
      "  sample 1:\n",
      "    enc input           > same reason why i would never in my life make line for a pair of jordan s lol <EOS>\n",
      "    dec input           > these be the one laughing at other people for not having cool enough electronics on the internet <EOS>\n",
      "    dec train predicted > a bad sure hard a in the era is a a a argument electronics on the internet <EOS>\n",
      "dev minibatch loss: 8.805891036987305\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > slut that you are of course you are you should pose with drumpft current wife naked and fucking <EOS>\n",
      "    DEV dec input           > hi donald you know i m in your corner and will definitely be at the debate <EOS>\n",
      "    DEV dec train predicted > i why i can <EOS> am so the own than there you better better the cycle <EOS>\n",
      "    DEV dec train infer > yes this is annoyed <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14785\n",
      "learning rate 0.000759917\n",
      "epoch 29\n",
      "batch 400\n",
      "training minibatch loss: 2.633660078048706\n",
      "  sample 1:\n",
      "    enc input           > good morning gorgeous how are you ? ? <EOS>\n",
      "    dec input           > happy game day <EOS>\n",
      "    dec train predicted > good birthday day i\n",
      "dev minibatch loss: 7.929816722869873\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > tho i think none of those pitcher would mind if they lost the cy young to jose <EOS>\n",
      "    DEV dec input           > not sure kershaw scherzer bumgarner cueto thor lester or hendricks would argue with you <EOS>\n",
      "    DEV dec train predicted > it a i but is is <EOS> <EOS> is <EOS> you be about the <EOS>\n",
      "    DEV dec train infer > i had a post one for a hell <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14835\n",
      "learning rate 0.000759212\n",
      "epoch 29\n",
      "batch 450\n",
      "training minibatch loss: 2.6364169120788574\n",
      "  sample 1:\n",
      "    enc input           > the trade will be out on december <EOS>\n",
      "    dec input           > i need money so i can start this <EOS>\n",
      "    dec train predicted > i m the to <EOS> can get <EOS> <EOS>\n",
      "dev minibatch loss: 8.166175842285156\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > damn and i wa thinking momma didn t raise no bitch <EOS>\n",
      "    DEV dec input           > i m not trying to be in my coffin sorry <EOS>\n",
      "    DEV dec train predicted > i haven at at to be able a airline <EOS> i\n",
      "    DEV dec train infer > i m so excited to be at the hour at my life <EOS> <EOS> <EOS>\n",
      "global_step: 14881\n",
      "learning rate 0.000758564\n",
      "epoch 30\n",
      "batch 0\n",
      "training minibatch loss: 2.468684196472168\n",
      "  sample 1:\n",
      "    enc input           > the important one the first one <EOS>\n",
      "    dec input           > depends on which n <EOS>\n",
      "    dec train predicted > depends in the bottle <EOS>\n",
      "dev minibatch loss: 8.269232749938965\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > now now that s an advanced test just want to confirm basic competence <EOS>\n",
      "    DEV dec input           > i would like him to take the ap u government exam <EOS>\n",
      "    DEV dec train predicted > listen m talk laying measure talk the al of measure grab <EOS>\n",
      "    DEV dec train infer > i m wanting to be a bunch of al these screenshots <EOS> <EOS> <EOS>\n",
      "global_step: 14931\n",
      "learning rate 0.000757859\n",
      "epoch 30\n",
      "batch 50\n",
      "training minibatch loss: 2.4902312755584717\n",
      "  sample 1:\n",
      "    enc input           > thank you so much liv <EOS>\n",
      "    dec input           > that s so awesome girl <EOS>\n",
      "    dec train predicted > you s a cute ton <EOS>\n",
      "dev minibatch loss: 8.442988395690918\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > sorry i think you said banned you i can drop you from these <EOS>\n",
      "    DEV dec input           > perfect disguise or just a nice guy with integrity and drive <EOS>\n",
      "    DEV dec train predicted > i man about now i blind comparison <EOS> blind <EOS> factory <EOS>\n",
      "    DEV dec train infer > of course i know i m so happy of someone and hardly <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 14981\n",
      "learning rate 0.000757156\n",
      "epoch 30\n",
      "batch 100\n",
      "training minibatch loss: 2.2797465324401855\n",
      "  sample 1:\n",
      "    enc input           > nah it happening even in portrait mode <EOS>\n",
      "    dec input           > hahaha you got that after rotating your device ? <EOS>\n",
      "    dec train predicted > hahaha you can me off rotating me three ? <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 7.620280742645264\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > show me this vine anytime i tweet i m not going to the gym <EOS>\n",
      "    DEV dec input           > take me lord for i am worthy <EOS>\n",
      "    DEV dec train predicted > did you for you your follow thanks darling\n",
      "    DEV dec train infer > thanks for your follow to your feedback <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15031\n",
      "learning rate 0.000756454\n",
      "epoch 30\n",
      "batch 150\n",
      "training minibatch loss: 2.6668403148651123\n",
      "  sample 1:\n",
      "    enc input           > give high to <EOS>\n",
      "    dec input           > thanks for the prop <EOS>\n",
      "    dec train predicted > thank for the follow <EOS>\n",
      "dev minibatch loss: 8.157946586608887\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > why would you talk trash on your gfs school lol bye <EOS>\n",
      "    DEV dec input           > supporting my girl on jv dawgggg screw the varsity <EOS>\n",
      "    DEV dec train predicted > i that fuck ever the <EOS> mug me fuck mug\n",
      "    DEV dec train infer > ugh that s a character <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15081\n",
      "learning rate 0.000755752\n",
      "epoch 30\n",
      "batch 200\n",
      "training minibatch loss: 2.2517547607421875\n",
      "  sample 1:\n",
      "    enc input           > if you watched you heard plenty of personal tale of people she helped <EOS>\n",
      "    dec input           > plus convention coverage wa the khan brilliant amp bill with balloon <EOS>\n",
      "    dec train predicted > self convention khan ? colin khan interpretation and bill and balloon <EOS>\n",
      "dev minibatch loss: 8.421998023986816\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > look forward to meeting her <EOS>\n",
      "    DEV dec input           > last meeting at a your but i will leave you in good hand with <EOS>\n",
      "    DEV dec train predicted > i heat i least follow game i ll be you with the <EOS> <EOS> you\n",
      "    DEV dec train infer > thanks for the extent of <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15131\n",
      "learning rate 0.00075505\n",
      "epoch 30\n",
      "batch 250\n",
      "training minibatch loss: 2.682461977005005\n",
      "  sample 1:\n",
      "    enc input           > yeah it s way too close for how ridiculous he is <EOS>\n",
      "    dec input           > nobody did everyone seems to think that trump ha a real chance though <EOS>\n",
      "    dec train predicted > hillary belief he lied about do he debate s born nazi problem to <EOS>\n",
      "dev minibatch loss: 7.798861980438232\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i can t wait you were delivered a load of material tonight <EOS>\n",
      "    DEV dec input           > vp debate w tonight kristen wiig amp owen wilson and music from <EOS>\n",
      "    DEV dec train predicted > the is like io in to for io and and i <EOS> my\n",
      "    DEV dec train infer > my recent and a warm message i have to be beer <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15181\n",
      "learning rate 0.00075435\n",
      "epoch 30\n",
      "batch 300\n",
      "training minibatch loss: 2.6873929500579834\n",
      "  sample 1:\n",
      "    enc input           > you re so cute when you talk sexy like that <EOS>\n",
      "    dec input           > love autocorrect <EOS>\n",
      "    dec train predicted > beautiful autocorrect <EOS>\n",
      "dev minibatch loss: 8.768218994140625\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > im live on cnn <EOS>\n",
      "    DEV dec input           > everyone turn on cnn and keep watching im about to go live <EOS>\n",
      "    DEV dec train predicted > the is to a <EOS> i a a a a the <EOS> <EOS>\n",
      "    DEV dec train infer > the giant is getting very very very very fun <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15231\n",
      "learning rate 0.000753649\n",
      "epoch 30\n",
      "batch 350\n",
      "training minibatch loss: 2.6157567501068115\n",
      "  sample 1:\n",
      "    enc input           > if i wasn t here i d be there <EOS>\n",
      "    dec input           > it s about to be on <EOS>\n",
      "    dec train predicted > it s kind to invest a it\n",
      "dev minibatch loss: 8.157003402709961\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the towel is in the bidet <EOS>\n",
      "    DEV dec input           > no that s the sink that european use to clean their as w every private bathroom ha one <EOS>\n",
      "    DEV dec train predicted > the the s a new in s up to win throw replica game <EOS> device device <EOS> device <EOS>\n",
      "    DEV dec train infer > san down h s in san francisco give out h fighting <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15281\n",
      "learning rate 0.00075295\n",
      "epoch 30\n",
      "batch 400\n",
      "training minibatch loss: 2.572262763977051\n",
      "  sample 1:\n",
      "    enc input           > he is american psycho <EOS>\n",
      "    dec input           > is this a child or what ? <EOS>\n",
      "    dec train predicted > and this a good ? trump ? <EOS>\n",
      "dev minibatch loss: 8.335049629211426\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that wasn t me who tweeted that <EOS>\n",
      "    DEV dec input           > i thought you ain t like this song <EOS>\n",
      "    DEV dec train predicted > donald think that were t even a movie that\n",
      "    DEV dec train infer > donald trump is a hamilton of the security suspect in <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15331\n",
      "learning rate 0.000752251\n",
      "epoch 30\n",
      "batch 450\n",
      "training minibatch loss: 2.5910542011260986\n",
      "  sample 1:\n",
      "    enc input           > what tf wa the context for this one <EOS>\n",
      "    dec input           > silly string is like purell for cat max <EOS>\n",
      "    dec train predicted > guy string <EOS> purell purell <EOS> bail max <EOS>\n",
      "dev minibatch loss: 7.871269702911377\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > also extra inning is far and away the best sport streaming service is crushing it <EOS>\n",
      "    DEV dec input           > it s honestly such a pleasure to be able to watch a game in new york city <EOS>\n",
      "    DEV dec train predicted > usually wa been a a new of a a to see the big <EOS> the <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > usually released and released over usually usually released dude <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 15377\n",
      "learning rate 0.000751609\n",
      "epoch 31\n",
      "batch 0\n",
      "training minibatch loss: 2.368826389312744\n",
      "  sample 1:\n",
      "    enc input           > new version of hard style wednesday <EOS>\n",
      "    dec input           > the is getting ticket raided literally pulling people off the train never seen somethin like this before <EOS>\n",
      "    dec train predicted > the people full full raided the pulling somethin out a train somethin like somethin like it and <EOS>\n",
      "dev minibatch loss: 9.181388854980469\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > sorry they ve got four point from ten match mate reckon stubbs should get the sack ya ? <EOS>\n",
      "    DEV dec input           > nobody talk stats like that here bro <EOS>\n",
      "    DEV dec train predicted > how really to to that is s <EOS>\n",
      "    DEV dec train infer > so many only team is really really important the only one <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15427\n",
      "learning rate 0.000750911\n",
      "epoch 31\n",
      "batch 50\n",
      "training minibatch loss: 2.3600728511810303\n",
      "  sample 1:\n",
      "    enc input           > can we talk about growing the game during the lockout ? <EOS>\n",
      "    dec input           > eh that s a separate conversation until it isn t in the meantime gim me some money <EOS>\n",
      "    dec train predicted > no i happens the full part of it s t to the meantime gim the a a <EOS>\n",
      "dev minibatch loss: 8.389918327331543\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i m in last place <EOS>\n",
      "    DEV dec input           > ouch what do your other wr look like ? <EOS>\n",
      "    DEV dec train predicted > i up are you favs friend are in <EOS> <EOS>\n",
      "    DEV dec train infer > i love this <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15477\n",
      "learning rate 0.000750215\n",
      "epoch 31\n",
      "batch 100\n",
      "training minibatch loss: 2.5515246391296387\n",
      "  sample 1:\n",
      "    enc input           > mr trump why is the medium obsessed with your foundation but ignores kerry s daughter s fraud ? <EOS>\n",
      "    dec input           > this is more than a campaign it is a movement sign up today amp we will win <EOS>\n",
      "    dec train predicted > a is a in a hofstra a s a great sign up and <EOS> we will vote <EOS>\n",
      "dev minibatch loss: 8.042645454406738\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > awesome job marc always a pleasure to have you riding <EOS>\n",
      "    DEV dec input           > thanks for a great class <EOS>\n",
      "    DEV dec train predicted > great for the great idea is\n",
      "    DEV dec train infer > great to sharing thank you for a great wall and sharing up <EOS> <EOS> <EOS>\n",
      "global_step: 15527\n",
      "learning rate 0.000749518\n",
      "epoch 31\n",
      "batch 150\n",
      "training minibatch loss: 2.378006935119629\n",
      "  sample 1:\n",
      "    enc input           > dont give any seed to kiefer <EOS>\n",
      "    dec input           > you can say it i don t mind <EOS>\n",
      "    dec train predicted > i say i i i can t say <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 7.577069282531738\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > inconsequential tweeter here <EOS>\n",
      "    DEV dec input           > not for the faint of heart <EOS>\n",
      "    DEV dec train predicted > you you the name be the what\n",
      "    DEV dec train infer > you re a name in tv ? <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15577\n",
      "learning rate 0.000748823\n",
      "epoch 31\n",
      "batch 200\n",
      "training minibatch loss: 2.378966808319092\n",
      "  sample 1:\n",
      "    enc input           > improved defense ? <EOS>\n",
      "    dec input           > why is this giant saint game such a defensive struggle ? <EOS>\n",
      "    dec train predicted > this is the the south game a a different struggle ? <EOS>\n",
      "dev minibatch loss: 9.240229606628418\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > that s the first question in the debate <EOS>\n",
      "    DEV dec input           > can t help but imagine what the world would be like without insane clown posse bless up <EOS>\n",
      "    DEV dec train predicted > this that actually this judge judge trump presidency wa actually paying that paying <EOS> <EOS> the this <EOS>\n",
      "    DEV dec train infer > the padre bird actually actually actually even better than l <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15627\n",
      "learning rate 0.000748128\n",
      "epoch 31\n",
      "batch 250\n",
      "training minibatch loss: 2.7115261554718018\n",
      "  sample 1:\n",
      "    enc input           > i wish i could <EOS>\n",
      "    dec input           > please explain this <EOS>\n",
      "    dec train predicted > ima wait my <EOS>\n",
      "dev minibatch loss: 8.144624710083008\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we do have very good experience in wordpress we have designed amp developed several wordpress site <EOS>\n",
      "    DEV dec input           > looking for serious web page developer in standby wordpress skill recommended price negotiable <EOS>\n",
      "    DEV dec train predicted > are for a <EOS> is <EOS> for <EOS> duo create in duo for to\n",
      "    DEV dec train infer > i need a website website for a wordpress developer to create a website <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15677\n",
      "learning rate 0.000747434\n",
      "epoch 31\n",
      "batch 300\n",
      "training minibatch loss: 2.7585654258728027\n",
      "  sample 1:\n",
      "    enc input           > thank you peyton <EOS>\n",
      "    dec input           > happy bday maya hope you have a good one tu tn <EOS>\n",
      "    dec train predicted > happy birthday to hope you have done good picture <EOS> tn <EOS>\n",
      "dev minibatch loss: 7.9457316398620605\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > but what if god made evolution ? <EOS>\n",
      "    DEV dec input           > god claim to be the creator evolution claim to come up from the slime origin of life <EOS>\n",
      "    DEV dec train predicted > the jesus the kill within l for <EOS> <EOS> kill by and the internet <EOS> <EOS> it <EOS>\n",
      "    DEV dec train infer > lmaooo man we have left <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15727\n",
      "learning rate 0.00074674\n",
      "epoch 31\n",
      "batch 350\n",
      "training minibatch loss: 2.6881375312805176\n",
      "  sample 1:\n",
      "    enc input           > cuz knocked beanie tf out <EOS>\n",
      "    dec input           > game stupid for putting the footage in his video <EOS>\n",
      "    dec train predicted > boy footage for the <EOS> footage on the art <EOS>\n",
      "dev minibatch loss: 9.287863731384277\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > it is my photo sweetheart <EOS>\n",
      "    DEV dec input           > that real profile pic <EOS>\n",
      "    DEV dec train predicted > my s s s is\n",
      "    DEV dec train infer > it s my bday <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15777\n",
      "learning rate 0.000746047\n",
      "epoch 31\n",
      "batch 400\n",
      "training minibatch loss: 2.4641411304473877\n",
      "  sample 1:\n",
      "    enc input           > maybe he should also talk that little server thing smashing device and destroying backup ? <EOS>\n",
      "    dec input           > one of my favorite thing about is s precision in calling out trump on his absurdity <EOS>\n",
      "    dec train predicted > great of the conversation conversation about the the precision on the out the on relationship absurdity <EOS>\n",
      "dev minibatch loss: 8.930709838867188\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > s ionnidis ioannidis sigh <EOS>\n",
      "    DEV dec input           > also read up on just about anything written by john ionnidis about bad study <EOS>\n",
      "    DEV dec train predicted > these how bold on bold bold bold and <EOS> the listen when the <EOS> listen\n",
      "    DEV dec train infer > when the retweets <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15827\n",
      "learning rate 0.000745355\n",
      "epoch 31\n",
      "batch 450\n",
      "training minibatch loss: 2.755112409591675\n",
      "  sample 1:\n",
      "    enc input           > awww thanks katie and you know it <EOS>\n",
      "    dec input           > happy birthday woody party hard this weekend <EOS>\n",
      "    dec train predicted > happy birthday woody man amazing this weekend day\n",
      "dev minibatch loss: 8.468160629272461\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > this is actually worse than i thought it would be <EOS>\n",
      "    DEV dec input           > they say you can tell who is winning just by watching with the sound off <EOS>\n",
      "    DEV dec train predicted > it re im re t you i you it yours me this u brook <EOS> <EOS>\n",
      "    DEV dec train infer > someone can t describe irrationally when newton into a brook fan wentz <EOS> <EOS> <EOS>\n",
      "global_step: 15873\n",
      "learning rate 0.000744718\n",
      "epoch 32\n",
      "batch 0\n",
      "training minibatch loss: 2.36430287361145\n",
      "  sample 1:\n",
      "    enc input           > what s the saying about republican falling in line ? <EOS>\n",
      "    dec input           > appreciate the congrats <EOS>\n",
      "    dec train predicted > appreciate everyone worst <EOS>\n",
      "dev minibatch loss: 8.515732765197754\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > are you going to ignore the tone and speed of that comment ? <EOS>\n",
      "    DEV dec input           > agree hillary need to wait for fact <EOS>\n",
      "    DEV dec train predicted > u it asks to flag for the who\n",
      "    DEV dec train infer > hi and it s a serious troop to the troop to the legal of the nba injury egg <EOS>\n",
      "global_step: 15923\n",
      "learning rate 0.000744027\n",
      "epoch 32\n",
      "batch 50\n",
      "training minibatch loss: 2.2507758140563965\n",
      "  sample 1:\n",
      "    enc input           > jesus christ i need someone like this in group of designer <EOS>\n",
      "    dec input           > spent my night hand sewing this jacket <EOS>\n",
      "    dec train predicted > spent a hand sewing sewing this jacket <EOS>\n",
      "dev minibatch loss: 8.204347610473633\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what the ever loving fuck <EOS>\n",
      "    DEV dec input           > mark halperin just said clinton never fought successfully in a presidential campaign to win <EOS>\n",
      "    DEV dec train predicted > i the really seen the the have the copy the struggle movie they end <EOS>\n",
      "    DEV dec train infer > what s the dnc s gay really interesting s action ? <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 15973\n",
      "learning rate 0.000743337\n",
      "epoch 32\n",
      "batch 100\n",
      "training minibatch loss: 2.379155158996582\n",
      "  sample 1:\n",
      "    enc input           > what wa that ? <EOS>\n",
      "    dec input           > verse do not teach hate towards jew <EOS>\n",
      "    dec train predicted > verse you called know non towards jew <EOS>\n",
      "dev minibatch loss: 8.600818634033203\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > deer whisperer very cool vid <EOS>\n",
      "    DEV dec input           > money s baby mama pull up game too strong <EOS>\n",
      "    DEV dec train predicted > yeah product leadership plus ha up <EOS> all far <EOS>\n",
      "    DEV dec train infer > what s the product industry manager we have to discus our product click <EOS> <EOS> <EOS>\n",
      "global_step: 16023\n",
      "learning rate 0.000742647\n",
      "epoch 32\n",
      "batch 150\n",
      "training minibatch loss: 2.5883901119232178\n",
      "  sample 1:\n",
      "    enc input           > i must ve missed this one but who is opening a new hotel you re too funny girl <EOS>\n",
      "    dec input           > oh look and some major free press for his new hotel lol please <EOS>\n",
      "    dec train predicted > oh happy on happy thursday black commentary and the supporter hotel and <EOS> <EOS>\n",
      "dev minibatch loss: 9.126815795898438\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > isnt it already set in sj though ? <EOS>\n",
      "    DEV dec input           > hayward por vida <EOS>\n",
      "    DEV dec train predicted > i prefer la <EOS>\n",
      "    DEV dec train infer > i really wan na try checking <EOS> ? ? <EOS> <EOS> <EOS> ? <EOS> <EOS>\n",
      "global_step: 16073\n",
      "learning rate 0.000741958\n",
      "epoch 32\n",
      "batch 200\n",
      "training minibatch loss: 2.373443365097046\n",
      "  sample 1:\n",
      "    enc input           > what do you like it best for ? <EOS>\n",
      "    dec input           > oh i m a total rookie compared to you <EOS>\n",
      "    dec train predicted > i i can not bella rookie compared to you <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 7.891044616699219\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > thank you god bless u din <EOS>\n",
      "    DEV dec input           > happy birthday ate nicey enjoy your day po and god bless <EOS>\n",
      "    DEV dec train predicted > finn meeting to <EOS> hope the day <EOS> <EOS> <EOS> <EOS> you\n",
      "    DEV dec train infer > following amp enjoy following <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 16123\n",
      "learning rate 0.000741269\n",
      "epoch 32\n",
      "batch 250\n",
      "training minibatch loss: 2.680934429168701\n",
      "  sample 1:\n",
      "    enc input           > ty in that case pls enjoy join <EOS>\n",
      "    dec input           > prez toxie say shave off your cable with <EOS>\n",
      "    dec train predicted > i shave say shave shave <EOS> cable in i\n",
      "dev minibatch loss: 8.336357116699219\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > see what you ve done here <EOS>\n",
      "    DEV dec input           > my mom s got ta giant scorpio tattoo apparently she s a libra now <EOS>\n",
      "    DEV dec train predicted > yea lemonade look wayy a stand lot butt and <EOS> re welcome smoke <EOS> <EOS>\n",
      "    DEV dec train infer > turning up with me to you <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 16173\n",
      "learning rate 0.000740581\n",
      "epoch 32\n",
      "batch 300\n",
      "training minibatch loss: 2.755934476852417\n",
      "  sample 1:\n",
      "    enc input           > yea donate trump need a new portrait of himself what an as <EOS>\n",
      "    dec input           > urgent today is a big day help u hit our million goal donate by pm <EOS>\n",
      "    dec train predicted > urgent thanks s the great meal help u donate the donate goal donate by pm <EOS>\n",
      "dev minibatch loss: 8.40735149383545\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > rip this hero dead because of some hash lab <EOS>\n",
      "    DEV dec input           > fdny chief dy in explosion at nyc home eyed a drug lab <EOS>\n",
      "    DEV dec train predicted > a a must of pi palace the please base balance base is cc\n",
      "    DEV dec train infer > someone need to take a simple meat for me <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 16223\n",
      "learning rate 0.000739894\n",
      "epoch 32\n",
      "batch 350\n",
      "training minibatch loss: 2.2960410118103027\n",
      "  sample 1:\n",
      "    enc input           > have you been toggling off the bluetooth radio or is it always on ? <EOS>\n",
      "    dec input           > my bose headphone are so constantly disconnecting randomly from my iphone but not any other device <EOS>\n",
      "    dec train predicted > my bose device are so constantly disconnecting randomly are the street and not <EOS> other device <EOS>\n",
      "dev minibatch loss: 8.645734786987305\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > well playing banjo doe build up some serious arm strength <EOS>\n",
      "    DEV dec input           > fyi boxing steve martin back in the ring after absence of a year <EOS>\n",
      "    DEV dec train predicted > i just just strawberry wa all same same hole it i <EOS> big <EOS>\n",
      "    DEV dec train infer > i wan na be thinking about this and i missed it <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 16273\n",
      "learning rate 0.000739207\n",
      "epoch 32\n",
      "batch 400\n",
      "training minibatch loss: 2.724623441696167\n",
      "  sample 1:\n",
      "    enc input           > second friend today that lost a grandparent <EOS>\n",
      "    dec input           > i had the strange bittersweet responsibility to give the sermon at my grandmother s funeral today <EOS>\n",
      "    dec train predicted > i kinda a funeral bittersweet responsibility to give it sermon <EOS> the funeral s funeral funeral <EOS>\n",
      "dev minibatch loss: 8.564026832580566\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > they ve been coming out with stuff nonstop <EOS>\n",
      "    DEV dec input           > the ultimate glow kit is fckn gorgeous i ll admit tbh colourpop is killing me rn <EOS>\n",
      "    DEV dec train predicted > oh as nail my <EOS> my since <EOS> could be it <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > oh nah i m just wearing a year <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 16323\n",
      "learning rate 0.000738521\n",
      "epoch 32\n",
      "batch 450\n",
      "training minibatch loss: 2.8813650608062744\n",
      "  sample 1:\n",
      "    enc input           > ok but i don t think i m arguing that but ok cool man later <EOS>\n",
      "    dec input           > so ? major achievement occurred because lot of white black people brought an end to the evil scourge <EOS>\n",
      "    dec train predicted > for you you becoming occurred who scourge of becoming scourge scourge are you scourge to me achievement scourge <EOS>\n",
      "dev minibatch loss: 8.38416862487793\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > glad you dig it i do too <EOS>\n",
      "    DEV dec input           > love the mint design your work ? ? ? <EOS>\n",
      "    DEV dec train predicted > thank this accuracy is is grandma <EOS> <EOS> love <EOS>\n",
      "    DEV dec train infer > thank you for visiting letter out to cbs <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 16369\n",
      "learning rate 0.000737891\n",
      "epoch 33\n",
      "batch 0\n",
      "training minibatch loss: 2.3624701499938965\n",
      "  sample 1:\n",
      "    enc input           > throwback to when edison started ge a a dragon slaying firm <EOS>\n",
      "    dec input           > i dont understand what this image ha to do with this article <EOS>\n",
      "    dec train predicted > i ve understand what you image ha to be up this article <EOS>\n",
      "dev minibatch loss: 8.724796295166016\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i m on the line <EOS>\n",
      "    DEV dec input           > idk but it s nifty <EOS>\n",
      "    DEV dec train predicted > i i i s a <EOS>\n",
      "    DEV dec train infer > don t be jealous <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 16419\n",
      "learning rate 0.000737206\n",
      "epoch 33\n",
      "batch 50\n",
      "training minibatch loss: 2.3471574783325195\n",
      "  sample 1:\n",
      "    enc input           > he s a terrorist <EOS>\n",
      "    dec input           > lmaoooo what is wrong with that guy <EOS>\n",
      "    dec train predicted > lmaoooo i s a <EOS> the <EOS> <EOS>\n",
      "dev minibatch loss: 8.454276084899902\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > me and together ? sure here is one from the hard rock in vega in the little room <EOS>\n",
      "    DEV dec input           > i meant a real pic of you and beetle <EOS>\n",
      "    DEV dec train predicted > just really not white thing and a in not not\n",
      "    DEV dec train infer > there s not done a few thing though lmao <EOS> <EOS> <EOS> <EOS> ? <EOS>\n",
      "global_step: 16469\n",
      "learning rate 0.000736522\n",
      "epoch 33\n",
      "batch 100\n",
      "training minibatch loss: 2.4831509590148926\n",
      "  sample 1:\n",
      "    enc input           > i stand corrected <EOS>\n",
      "    dec input           > correction only black people s gun he s fine with the kkk white domestic abuser white felon <EOS>\n",
      "    dec train predicted > trump between racism people is jam that s a with a kkk history domestic felon racism felon <EOS>\n",
      "dev minibatch loss: 8.103706359863281\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > but dude they even changed the lime one to green apple <EOS>\n",
      "    DEV dec input           > if i had a basket of skittle you can have it because i don t really like skittle <EOS>\n",
      "    DEV dec train predicted > i i have a lot of instruction i have win a people i have t understand understand that of\n",
      "    DEV dec train infer > yes classic up the best people that light are light on classic <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 16519\n",
      "learning rate 0.000735838\n",
      "epoch 33\n",
      "batch 150\n",
      "training minibatch loss: 2.306121587753296\n",
      "  sample 1:\n",
      "    enc input           > survival of the fittest that s life <EOS>\n",
      "    dec input           > they won t survive in the wild now though <EOS>\n",
      "    dec train predicted > i won t eat though the wild <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 8.915997505187988\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you too tay <EOS>\n",
      "    DEV dec input           > honestly such a great game lady <EOS>\n",
      "    DEV dec train predicted > i you a much day <EOS> <EOS>\n",
      "    DEV dec train infer > i love you enjoy the spirit <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 16569\n",
      "learning rate 0.000735155\n",
      "epoch 33\n",
      "batch 200\n",
      "training minibatch loss: 2.6662683486938477\n",
      "  sample 1:\n",
      "    enc input           > hey i is here if u ever need anything bb <EOS>\n",
      "    dec input           > when you just need a friend to be there for you <EOS>\n",
      "    dec train predicted > to you re still to plan ? come gon <EOS> you <EOS>\n",
      "dev minibatch loss: 7.642982482910156\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > right ? so so sweet <EOS>\n",
      "    DEV dec input           > that is so nice <EOS>\n",
      "    DEV dec train predicted > i is my so <EOS>\n",
      "    DEV dec train infer > i m so so much to hear my life and love to sleep <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 16619\n",
      "learning rate 0.000734473\n",
      "epoch 33\n",
      "batch 250\n",
      "training minibatch loss: 2.779636859893799\n",
      "  sample 1:\n",
      "    enc input           > i have playlist just like this it s great <EOS>\n",
      "    dec input           > it gave me mix and all of them are playlist i would make have made before <EOS>\n",
      "    dec train predicted > i s you i <EOS> i of you i a i can do i been <EOS> <EOS>\n",
      "dev minibatch loss: 8.483656883239746\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > thanks brother followed <EOS>\n",
      "    DEV dec input           > loved the creativity on this <EOS>\n",
      "    DEV dec train predicted > congratulation my review view have one\n",
      "    DEV dec train infer > happy birthday have a great way <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 16669\n",
      "learning rate 0.000733791\n",
      "epoch 33\n",
      "batch 300\n",
      "training minibatch loss: 2.2612557411193848\n",
      "  sample 1:\n",
      "    enc input           > chill out b i don t even have time for k <EOS>\n",
      "    dec input           > binge watch all of them yolo <EOS>\n",
      "    dec train predicted > binge yolo the of the yolo <EOS>\n",
      "dev minibatch loss: 8.055191040039062\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > only because he look so darn attractive on that print <EOS>\n",
      "    DEV dec input           > he seems to like the armrest <EOS>\n",
      "    DEV dec train predicted > donald can a be donald refugee president\n",
      "    DEV dec train infer > hillary clinton clinton doesn t be rigged hillary for hillary hillary clinton <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 16719\n",
      "learning rate 0.00073311\n",
      "epoch 33\n",
      "batch 350\n",
      "training minibatch loss: 2.546860694885254\n",
      "  sample 1:\n",
      "    enc input           > right ? the cat used to cuddle up with me now i m chopped liver <EOS>\n",
      "    dec input           > it s so not fair <EOS>\n",
      "    dec train predicted > what is so fair a <EOS>\n",
      "dev minibatch loss: 8.360733032226562\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the name say it all <EOS>\n",
      "    DEV dec input           > today youth tomorrow force for change call for action through <EOS>\n",
      "    DEV dec train predicted > i is blow hurt me a orange <EOS> the wasn the\n",
      "    DEV dec train infer > i think he wasn t stopped that is a good right now <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 16769\n",
      "learning rate 0.00073243\n",
      "epoch 33\n",
      "batch 400\n",
      "training minibatch loss: 2.370250940322876\n",
      "  sample 1:\n",
      "    enc input           > she look like my boy michael jackson <EOS>\n",
      "    dec input           > amal clooney seek justice for yazidi sex slave no matter the price <EOS>\n",
      "    dec train predicted > amal clooney seek his and yazidi gift slave go break his loop <EOS>\n",
      "dev minibatch loss: 8.979650497436523\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you spelled shitbag of humanity wrong <EOS>\n",
      "    DEV dec input           > lt oh look another ignorant racist let s have twitter follower block him <EOS>\n",
      "    DEV dec train predicted > doe cnn cnn honest word not comment u pay a tax <EOS> not claiming\n",
      "    DEV dec train infer > david video jr name ? <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 16819\n",
      "learning rate 0.00073175\n",
      "epoch 33\n",
      "batch 450\n",
      "training minibatch loss: 2.595306396484375\n",
      "  sample 1:\n",
      "    enc input           > love these video <EOS>\n",
      "    dec input           > video vol of the trump horror continued more reason must never be president <EOS>\n",
      "    dec train predicted > absolutely vol of the debate continued continued this insane will be be a <EOS>\n",
      "dev minibatch loss: 8.151126861572266\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > impossible to not love elizabeth warren <EOS>\n",
      "    DEV dec input           > sen warren slam well fargo ceo for pushing blame on lower level employee say it s gutless leadership <EOS>\n",
      "    DEV dec train predicted > great shelf fan portland for amp and a successful with the digital of work you <EOS> very heartbreak <EOS>\n",
      "    DEV dec train infer > great oil for twitter and digital digital news you all for inspiration <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 16865\n",
      "learning rate 0.000731125\n",
      "epoch 34\n",
      "batch 0\n",
      "training minibatch loss: 2.33854341506958\n",
      "  sample 1:\n",
      "    enc input           > propping bad story like and her pet <EOS>\n",
      "    dec input           > back in what capacity ? working ? picking up some mail ? which is it dena ? <EOS>\n",
      "    dec train predicted > working into picking capacity ? <EOS> ? picking some some mail hmu working ? you dena ? <EOS>\n",
      "dev minibatch loss: 8.718061447143555\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > straight up <EOS>\n",
      "    DEV dec input           > lord paulie just put the key in lol <EOS>\n",
      "    DEV dec train predicted > not you me me me a in television <EOS>\n",
      "    DEV dec train infer > yeah it s a greatest a champion ever <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 16915\n",
      "learning rate 0.000730447\n",
      "epoch 34\n",
      "batch 50\n",
      "training minibatch loss: 2.2376561164855957\n",
      "  sample 1:\n",
      "    enc input           > i rt thing people say all the time w out any particular comment about it <EOS>\n",
      "    dec input           > like the main thing the gator seem to be flipping out about is this tweet <EOS>\n",
      "    dec train predicted > like this real group to thing seem to say flipping to to speed <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 9.505147933959961\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > wait both of these should be not <EOS>\n",
      "    DEV dec input           > then the bridge is maybe <EOS>\n",
      "    DEV dec train predicted > learning i learning learning learning excited\n",
      "    DEV dec train infer > currently amp a lease is currently just currently in top and we have a learning in a road learning <EOS>\n",
      "global_step: 16965\n",
      "learning rate 0.000729769\n",
      "epoch 34\n",
      "batch 100\n",
      "training minibatch loss: 2.111858606338501\n",
      "  sample 1:\n",
      "    enc input           > i m assuming you mean since he s being sued for raping a year old <EOS>\n",
      "    dec input           > hill claim to be a champion child yet she laughed about defending a guilty child rapist <EOS>\n",
      "    dec train predicted > innocent laughed to losing innocent champion child on i laughed about a a rapist child rapist <EOS>\n",
      "dev minibatch loss: 9.193002700805664\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > at home wow <EOS>\n",
      "    DEV dec input           > my mom wa a teacher and she had one at home and i m still sane <EOS>\n",
      "    DEV dec train predicted > where blue look home robot near near look home home me <EOS> someday hang home dizzy <EOS>\n",
      "    DEV dec train infer > california blue coffee <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17015\n",
      "learning rate 0.000729092\n",
      "epoch 34\n",
      "batch 150\n",
      "training minibatch loss: 2.3937177658081055\n",
      "  sample 1:\n",
      "    enc input           > love the southern accent here <EOS>\n",
      "    dec input           > why did you reference willie horton when you were degrading gennifer flower <EOS>\n",
      "    dec train predicted > why did you support willie horton why you re degrading gennifer flower <EOS>\n",
      "dev minibatch loss: 8.603350639343262\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > but congrats on the book shoutout <EOS>\n",
      "    DEV dec input           > we had to buy it from amazon <EOS>\n",
      "    DEV dec train predicted > cool need a win this <EOS> the this\n",
      "    DEV dec train infer > great workout <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17065\n",
      "learning rate 0.000728415\n",
      "epoch 34\n",
      "batch 200\n",
      "training minibatch loss: 2.4668405055999756\n",
      "  sample 1:\n",
      "    enc input           > not a good night for u <EOS>\n",
      "    dec input           > tonight colon cespedes amp the take on the at pm in miami will be a sad night <EOS>\n",
      "    dec train predicted > now colon cespedes amp <EOS> amazing tonight the amazing the i miami i see good good day <EOS>\n",
      "dev minibatch loss: 8.483325958251953\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > you rap if so i am a producer <EOS>\n",
      "    DEV dec input           > i don t have no help where i m from i got ta put my self on <EOS>\n",
      "    DEV dec train predicted > i love t know a mom she i miss so her don to be a brother bitch <EOS>\n",
      "    DEV dec train infer > oh nah i m so proud of the best <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17115\n",
      "learning rate 0.000727739\n",
      "epoch 34\n",
      "batch 250\n",
      "training minibatch loss: 2.182021141052246\n",
      "  sample 1:\n",
      "    enc input           > whoop that work out to be a rather confusing typo thanks for pointing that out <EOS>\n",
      "    dec input           > when did they move fairfax from virginia to california ? <EOS>\n",
      "    dec train predicted > i they you get fairfax from virginia to virginia ? <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 9.336721420288086\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > they both did it <EOS>\n",
      "    DEV dec input           > nah he did it got demoted and no shout out in the president s speech <EOS>\n",
      "    DEV dec train predicted > i it really a out a <EOS> be it a of the debate <EOS> medium <EOS>\n",
      "    DEV dec train infer > i m playing in october during the whole team in october october yo october october yo burn the upgrade yo condom the\n",
      "global_step: 17165\n",
      "learning rate 0.000727064\n",
      "epoch 34\n",
      "batch 300\n",
      "training minibatch loss: 2.3705289363861084\n",
      "  sample 1:\n",
      "    enc input           > thanks nick what did you get out of the episode ? <EOS>\n",
      "    dec input           > some of my favorite podcasters have interviewed the fascinating <EOS>\n",
      "    dec train predicted > fascinating anime the favorite podcasters get interviewed your fascinating <EOS>\n",
      "dev minibatch loss: 8.509455680847168\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what did you do with the old one tho ? <EOS>\n",
      "    DEV dec input           > went from x xt xt in month this is what happens when you hang around <EOS>\n",
      "    DEV dec train predicted > sorry away my happy and <EOS> th <EOS> nice nice you to you re up <EOS>\n",
      "    DEV dec train infer > sorry my pain and begging to my hair <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17215\n",
      "learning rate 0.000726389\n",
      "epoch 34\n",
      "batch 350\n",
      "training minibatch loss: 2.380706787109375\n",
      "  sample 1:\n",
      "    enc input           > if you believe in unaccountable storm trooper he s your man <EOS>\n",
      "    dec input           > and the largest police union endorsed donald trump <EOS>\n",
      "    dec train predicted > and the largest family family endorsed <EOS> trump <EOS>\n",
      "dev minibatch loss: 8.751137733459473\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > who know on him he try hard every play <EOS>\n",
      "    DEV dec input           > how is buckner looking ? i haven t seen a lot of his snap and so am asking <EOS>\n",
      "    DEV dec train predicted > big can that of for <EOS> m t feel that boyfriend of mainstream holt with not excited <EOS> <EOS>\n",
      "    DEV dec train infer > nothing is booked in booked will be better <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17265\n",
      "learning rate 0.000725715\n",
      "epoch 34\n",
      "batch 400\n",
      "training minibatch loss: 2.601365327835083\n",
      "  sample 1:\n",
      "    enc input           > can t wait loved the first episode <EOS>\n",
      "    dec input           > not tonight but don t miss the premiere next sunday <EOS>\n",
      "    dec train predicted > fetch on will i t see you premiere premiere week <EOS>\n",
      "dev minibatch loss: 9.162751197814941\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > new revenue idea we re taking of every purchase so expect lot of ad for jet fighter <EOS>\n",
      "    DEV dec input           > so do you think i m in the market for a th generation military fighter jet ? <EOS>\n",
      "    DEV dec train predicted > a we you know for accept a a press with a simple <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "    DEV dec train infer > hole a hole on the hole hole out <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17315\n",
      "learning rate 0.000725042\n",
      "epoch 34\n",
      "batch 450\n",
      "training minibatch loss: 2.4754834175109863\n",
      "  sample 1:\n",
      "    enc input           > a week from tomorrow it s about to go down lol <EOS>\n",
      "    dec input           > you too big bro bday coming too right ? <EOS>\n",
      "    dec train predicted > i too good wa i to <EOS> <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 8.411294937133789\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > cindy so nice seeing you your new collection of medallion amp flower is absolutely gorgeous <EOS>\n",
      "    DEV dec input           > learn more about my <EOS>\n",
      "    DEV dec train predicted > i i i a own\n",
      "    DEV dec train infer > i can t wait to use running it s running with semester <EOS> <EOS> <EOS>\n",
      "global_step: 17361\n",
      "learning rate 0.000724423\n",
      "epoch 35\n",
      "batch 0\n",
      "training minibatch loss: 1.943448543548584\n",
      "  sample 1:\n",
      "    enc input           > and i guess quick is in net because tort remembers the quick that ended his stanley cup final <EOS>\n",
      "    dec input           > but we don t need good goal scorer like phil kessel on the team <EOS>\n",
      "    dec train predicted > i the need t need some thing scorer scorer phil kessel to the team <EOS>\n",
      "dev minibatch loss: 9.209242820739746\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > same here i ve been on edge all morning literally in the worst mood <EOS>\n",
      "    DEV dec input           > girl i don t even know what to do with myself <EOS>\n",
      "    DEV dec train predicted > y you don t rolled try if youre come anyway it <EOS>\n",
      "    DEV dec train infer > omg i don t got to go to ride before it doesnt get it then <EOS> <EOS>\n",
      "global_step: 17411\n",
      "learning rate 0.00072375\n",
      "epoch 35\n",
      "batch 50\n",
      "training minibatch loss: 2.3592684268951416\n",
      "  sample 1:\n",
      "    enc input           > i been here before that lol <EOS>\n",
      "    dec input           > they see the glow up <EOS>\n",
      "    dec train predicted > you just bro glow up <EOS>\n",
      "dev minibatch loss: 9.147838592529297\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i respect your opinion let hope whoever win america will be safe and prosperous for all <EOS>\n",
      "    DEV dec input           > i would prefer not to engage in this at least the man know about the greater good <EOS>\n",
      "    DEV dec train predicted > breaking say be be always be out a health building the social is how his social <EOS> <EOS>\n",
      "    DEV dec train infer > statement on the medium against trump cancel via <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17461\n",
      "learning rate 0.000723079\n",
      "epoch 35\n",
      "batch 100\n",
      "training minibatch loss: 2.292764186859131\n",
      "  sample 1:\n",
      "    enc input           > they are not going to the playoff this year if they play wildcard mets will win <EOS>\n",
      "    dec input           > i honestly thought the giant were going to win that one <EOS>\n",
      "    dec train predicted > i think think the fuck would not to win the <EOS> <EOS>\n",
      "dev minibatch loss: 8.781150817871094\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > calgary seems kind of nice <EOS>\n",
      "    DEV dec input           > they are thinking we are about to have much cheaper real estate value <EOS>\n",
      "    DEV dec train predicted > unsure had looking it had doing in block a a <EOS> tweet <EOS> <EOS>\n",
      "    DEV dec train infer > hahaha i had a mini thing <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17511\n",
      "learning rate 0.000722408\n",
      "epoch 35\n",
      "batch 150\n",
      "training minibatch loss: 2.4652156829833984\n",
      "  sample 1:\n",
      "    enc input           > aren t they <EOS>\n",
      "    dec input           > i m not ready to get up yet the bed at the are way too comfy <EOS>\n",
      "    dec train predicted > i love so so to be to about about life about my bitch so <EOS> bitch <EOS>\n",
      "dev minibatch loss: 9.318734169006348\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > super great thank you <EOS>\n",
      "    DEV dec input           > how wa the event yesterday ? <EOS>\n",
      "    DEV dec train predicted > thanks to you good for <EOS> <EOS>\n",
      "    DEV dec train infer > thanks for the thanks so proud of you <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17561\n",
      "learning rate 0.000721737\n",
      "epoch 35\n",
      "batch 200\n",
      "training minibatch loss: 2.2316017150878906\n",
      "  sample 1:\n",
      "    enc input           > roll tide baby all day <EOS>\n",
      "    dec input           > it s going to be interesting to see how the respond to this challenge <EOS>\n",
      "    dec train predicted > what s a to be here to see it s iphone to the <EOS> <EOS>\n",
      "dev minibatch loss: 9.078248977661133\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > awwwww it like finding out about easter bunny or cover mick s ear santa <EOS>\n",
      "    DEV dec input           > that poor kid wa so confused why i wa mean to mommy <EOS>\n",
      "    DEV dec train predicted > finally look look and a much <EOS> this miss amazing sitting the <EOS>\n",
      "    DEV dec train infer > looking forward to the best day and flying with cancer for all to see <EOS> <EOS>\n",
      "global_step: 17611\n",
      "learning rate 0.000721067\n",
      "epoch 35\n",
      "batch 250\n",
      "training minibatch loss: 2.555579900741577\n",
      "  sample 1:\n",
      "    enc input           > the league of the forked tongue <EOS>\n",
      "    dec input           > ted cruz announces support for donald trump orange weasel endorsed by slimy reptile for fuck s sake <EOS>\n",
      "    dec train predicted > ted cruz announces support to trump trump orange weasel endorsed and slimy reptile to logic s sake <EOS>\n",
      "dev minibatch loss: 9.144482612609863\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > omg piping hot fresh donut it s like a dream lol <EOS>\n",
      "    DEV dec input           > even captain holt approves ha your breakfast covered <EOS>\n",
      "    DEV dec train predicted > happy bro my <EOS> <EOS> you best <EOS> you\n",
      "    DEV dec train infer > happy birthday you <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 17661\n",
      "learning rate 0.000720398\n",
      "epoch 35\n",
      "batch 300\n",
      "training minibatch loss: 2.73651123046875\n",
      "  sample 1:\n",
      "    enc input           > i m gon na show you how to do the tornado spin <EOS>\n",
      "    dec input           > holy shit jamal smith <EOS>\n",
      "    dec train predicted > jamal shit jamal smith <EOS>\n",
      "dev minibatch loss: 8.502406120300293\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > keeping that face paint on <EOS>\n",
      "    DEV dec input           > cell phone fan what the fuck <EOS>\n",
      "    DEV dec train predicted > cesaro in in in were half were\n",
      "    DEV dec train infer > new quite the best world <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17711\n",
      "learning rate 0.00071973\n",
      "epoch 35\n",
      "batch 350\n",
      "training minibatch loss: 2.289759635925293\n",
      "  sample 1:\n",
      "    enc input           > doesn t break my heart long a they don t come here <EOS>\n",
      "    dec input           > woman being burnt alive amp killed by for refusing sexual <EOS>\n",
      "    dec train predicted > a in burnt in during master by for refusing trump <EOS>\n",
      "dev minibatch loss: 8.096391677856445\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i hate moore s swing <EOS>\n",
      "    DEV dec input           > because he s fucking shit <EOS>\n",
      "    DEV dec train predicted > found i s decent jealous phone\n",
      "    DEV dec train infer > tomato filter is a switch to <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17761\n",
      "learning rate 0.000719062\n",
      "epoch 35\n",
      "batch 400\n",
      "training minibatch loss: 2.6349287033081055\n",
      "  sample 1:\n",
      "    enc input           > no one wan na text u brandon <EOS>\n",
      "    dec input           > please dm me your number <EOS>\n",
      "    dec train predicted > thanks dm me you dm ?\n",
      "dev minibatch loss: 7.938358783721924\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what s new ? <EOS>\n",
      "    DEV dec input           > wont say if trump apologized to cruz for attack on the senator s wife and father <EOS>\n",
      "    DEV dec train predicted > i i the you seen from boy seat the forever the movie i <EOS> <EOS> ever sickening\n",
      "    DEV dec train infer > i m going to be satisfied of the new team in the license <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17811\n",
      "learning rate 0.000718394\n",
      "epoch 35\n",
      "batch 450\n",
      "training minibatch loss: 2.388726234436035\n",
      "  sample 1:\n",
      "    enc input           > wow teach me <EOS>\n",
      "    dec input           > to excited for fall makeup i had to post <EOS>\n",
      "    dec train predicted > just the of the and to could to be <EOS>\n",
      "dev minibatch loss: 8.177042961120605\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > yea that wa rough <EOS>\n",
      "    DEV dec input           > oh god help you i hated that movie <EOS>\n",
      "    DEV dec train predicted > it i i he know ve it kid like\n",
      "    DEV dec train infer > he s a creep checker about the bengal <EOS> i pretend <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17857\n",
      "learning rate 0.000717781\n",
      "epoch 36\n",
      "batch 0\n",
      "training minibatch loss: 2.146090269088745\n",
      "  sample 1:\n",
      "    enc input           > i would say that these are a wise man s reason for voting for hillary also <EOS>\n",
      "    dec input           > read the thread gt gt <EOS>\n",
      "    dec train predicted > i this debate <EOS> <EOS> <EOS>\n",
      "dev minibatch loss: 8.863260269165039\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > she look so demonic <EOS>\n",
      "    DEV dec input           > terra make me itch <EOS>\n",
      "    DEV dec train predicted > this this this said down\n",
      "    DEV dec train infer > this is a climate shoe <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17907\n",
      "learning rate 0.000717115\n",
      "epoch 36\n",
      "batch 50\n",
      "training minibatch loss: 2.2519705295562744\n",
      "  sample 1:\n",
      "    enc input           > can you imagine the waiting room ? <EOS>\n",
      "    dec input           > if islam is so tolerant why doesn t she go treat her parkinson in syria or iran ? <EOS>\n",
      "    dec train predicted > reporter i is tolerant tolerant there will t you get to her parkinson in syria <EOS> iran <EOS> <EOS>\n",
      "dev minibatch loss: 9.250502586364746\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > lawsuit brother <EOS>\n",
      "    DEV dec input           > damn near dude i fucking hate mustard <EOS>\n",
      "    DEV dec train predicted > the the ride me fight have the <EOS>\n",
      "    DEV dec train infer > the nd colored line of the planet <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 17957\n",
      "learning rate 0.000716449\n",
      "epoch 36\n",
      "batch 100\n",
      "training minibatch loss: 2.159148693084717\n",
      "  sample 1:\n",
      "    enc input           > he got me in my feel more than old drizzy song <EOS>\n",
      "    dec input           > yeah he just kept going i wa like close to tear <EOS>\n",
      "    dec train predicted > he it close flew going when m going him to tear <EOS>\n",
      "dev minibatch loss: 8.711726188659668\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > welcome to am island <EOS>\n",
      "    DEV dec input           > excellent want me to send some over ? are you in the u ? <EOS>\n",
      "    DEV dec train predicted > must a to to husband a young of young a looking college world person <EOS>\n",
      "    DEV dec train infer > spent this year <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 18007\n",
      "learning rate 0.000715784\n",
      "epoch 36\n",
      "batch 150\n",
      "training minibatch loss: 2.4547550678253174\n",
      "  sample 1:\n",
      "    enc input           > it feel like it ha a couple level <EOS>\n",
      "    dec input           > you got the joke right ? <EOS>\n",
      "    dec train predicted > i re the brook <EOS> ? <EOS>\n",
      "dev minibatch loss: 8.87496566772461\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > is a crooked hag who still think he is relevant to the people lmao <EOS>\n",
      "    DEV dec input           > harry reid claim a dem gun control measure would prevent ny style terrorist attack <EOS>\n",
      "    DEV dec train predicted > trump wonder said he husband feel t clinton he be her question he he pointing\n",
      "    DEV dec train infer > lester holt he denies calling a trump supporter that should be completely completely completely completely expose <EOS> <EOS>\n",
      "global_step: 18057\n",
      "learning rate 0.00071512\n",
      "epoch 36\n",
      "batch 200\n",
      "training minibatch loss: 2.35605788230896\n",
      "  sample 1:\n",
      "    enc input           > ill be ur number w a bullet <EOS>\n",
      "    dec input           > we re going down down in an earlier round sugar we re going down swinging <EOS>\n",
      "    dec train predicted > we re driving down swinging going local sugar at sugar i re going down swinging <EOS>\n",
      "dev minibatch loss: 9.022558212280273\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > another phone ? it s tough to build up shiny black rectangle when you re not apple <EOS>\n",
      "    DEV dec input           > google really is trying to build this up better be good <EOS>\n",
      "    DEV dec train predicted > dangerous of ha dying in die a in in <EOS> in in\n",
      "    DEV dec train infer > famous of deliberate sexy choice with screen and young so famous <EOS> <EOS> <EOS>\n",
      "global_step: 18107\n",
      "learning rate 0.000714457\n",
      "epoch 36\n",
      "batch 250\n",
      "training minibatch loss: 2.468916177749634\n",
      "  sample 1:\n",
      "    enc input           > morning beth <EOS>\n",
      "    dec input           > morning gz amp all <EOS>\n",
      "    dec train predicted > morning gz amp <EOS> <EOS>\n",
      "dev minibatch loss: 8.62203598022461\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > should have hit you up to go watch this game apology stef <EOS>\n",
      "    DEV dec input           > what a start to the game keep it up usf <EOS>\n",
      "    DEV dec train predicted > having do great now brown brown and thx in <EOS> <EOS>\n",
      "    DEV dec train infer > so dry day and my plan of the house alive and factory of my house thx <EOS>\n",
      "global_step: 18157\n",
      "learning rate 0.000713794\n",
      "epoch 36\n",
      "batch 300\n",
      "training minibatch loss: 2.428532123565674\n",
      "  sample 1:\n",
      "    enc input           > count how many time i say wait <EOS>\n",
      "    dec input           > you sound like you re having such a blast <EOS>\n",
      "    dec train predicted > the re like to re having a a blast <EOS>\n",
      "dev minibatch loss: 9.1359224319458\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > this make it clear that u shd subscribe to troma now <EOS>\n",
      "    DEV dec input           > joking aside if you have a b rated film ask how to properly submit it <EOS>\n",
      "    DEV dec train predicted > losing denies <EOS> she should not rape county column leave <EOS> rape ditch <EOS> to <EOS>\n",
      "    DEV dec train infer > identity not remind this year old man <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 18207\n",
      "learning rate 0.000713131\n",
      "epoch 36\n",
      "batch 350\n",
      "training minibatch loss: 2.2183711528778076\n",
      "  sample 1:\n",
      "    enc input           > this fuck the medium they re profiting from his game just the same <EOS>\n",
      "    dec input           > he didn t trick them into anything they re willingly doing this <EOS>\n",
      "    dec train predicted > nah mean t wan them into that ? re willingly nearby the ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev minibatch loss: 10.428548812866211\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > we re headed home now we were at dinner a few ave s over <EOS>\n",
      "    DEV dec input           > please stay home tonight omg <EOS>\n",
      "    DEV dec train predicted > mexico get in in <EOS> <EOS>\n",
      "    DEV dec train infer > sunday pm and sunday <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 18257\n",
      "learning rate 0.000712469\n",
      "epoch 36\n",
      "batch 400\n",
      "training minibatch loss: 2.449932813644409\n",
      "  sample 1:\n",
      "    enc input           > thanks for playing so glad you like it <EOS>\n",
      "    dec input           > dude i have played this too many time it so addictive ? <EOS>\n",
      "    dec train predicted > it how wa an with of it of <EOS> though addictive ? <EOS>\n",
      "dev minibatch loss: 8.179113388061523\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > no one ha original idea anymore i m trying to find my own creative spark <EOS>\n",
      "    DEV dec input           > why doe every photographer copy each other <EOS>\n",
      "    DEV dec train predicted > i do you hair wake bill fire <EOS>\n",
      "    DEV dec train infer > this is nothing so much <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 18307\n",
      "learning rate 0.000711808\n",
      "epoch 36\n",
      "batch 450\n",
      "training minibatch loss: 2.580280065536499\n",
      "  sample 1:\n",
      "    enc input           > actually they are very proven <EOS>\n",
      "    dec input           > gmo fear are absolutely unproven <EOS>\n",
      "    dec train predicted > gmo fear are absolutely unproven <EOS>\n",
      "dev minibatch loss: 8.417967796325684\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > wtfu dnc it s over <EOS>\n",
      "    DEV dec input           > when trump arrived colorado spring co <EOS>\n",
      "    DEV dec train predicted > ugh is got is is weather <EOS>\n",
      "    DEV dec train infer > and i m to pack up to dress <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "Saving session\n",
      "global_step: 18353\n",
      "learning rate 0.0007112\n",
      "epoch 37\n",
      "batch 0\n",
      "training minibatch loss: 2.23148250579834\n",
      "  sample 1:\n",
      "    enc input           > how cool is this <EOS>\n",
      "    dec input           > apple to create london home at battersea power station <EOS>\n",
      "    dec train predicted > bruh to create me pixel ignite battersea battersea pic <EOS>\n",
      "dev minibatch loss: 8.487201690673828\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > what a nice morning tweet <EOS>\n",
      "    DEV dec input           > i love outdated science video <EOS>\n",
      "    DEV dec train predicted > thanks hope this for disgusting climbing\n",
      "    DEV dec train infer > yep are the best one <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "global_step: 18403\n",
      "learning rate 0.00071054\n",
      "epoch 37\n",
      "batch 50\n",
      "training minibatch loss: 2.154642343521118\n",
      "  sample 1:\n",
      "    enc input           > how much a piece <EOS>\n",
      "    dec input           > guy i have four ticket to chance in toronto tmrw somebody pls buy them off of me <EOS>\n",
      "    dec train predicted > mets i m ta til to pound tmrw toronto tmrw toronto pls go them anywhere anywhere <EOS> <EOS>\n",
      "dev minibatch loss: 9.45351791381836\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i ve literally never watched anyone live on facebook nor do i care too <EOS>\n",
      "    DEV dec input           > i keep getting notification that people are facebook live <EOS>\n",
      "    DEV dec train predicted > i think a a from is want wrong like in\n",
      "    DEV dec train infer > oh that s funny what is a garbage comparison i m a garbage <EOS> <EOS>\n",
      "global_step: 18453\n",
      "learning rate 0.000709881\n",
      "epoch 37\n",
      "batch 100\n",
      "training minibatch loss: 2.3519530296325684\n",
      "  sample 1:\n",
      "    enc input           > uncle lou show this all of clinton s basket of deplorables trumpster oct th small donald go <EOS>\n",
      "    dec input           > did the debate change your mind on which candidate you re voting for in november ? <EOS>\n",
      "    dec train predicted > did you commission discus u family to the candidate you want voting for november police <EOS> <EOS>\n",
      "dev minibatch loss: 8.109835624694824\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > the only car that can be upgraded w autonomy are tesla amp anything compatible w <EOS>\n",
      "    DEV dec input           > this ignores the upgrade rate car will get upgraded before theyre replaced <EOS>\n",
      "    DEV dec train predicted > i is student student to being with a a to the college <EOS>\n",
      "    DEV dec train infer > i agree for the job and student i can spread it spread student we can not aid with <EOS>\n",
      "global_step: 18503\n",
      "learning rate 0.000709222\n",
      "epoch 37\n",
      "batch 150\n",
      "training minibatch loss: 2.4798336029052734\n",
      "  sample 1:\n",
      "    enc input           > i tried to tell noah wa a but nobody wanted to listen <EOS>\n",
      "    dec input           > this is the third shortest start of noah syndergaard s career <EOS>\n",
      "    dec train predicted > noah is that third shortest start of noah syndergaard s career <EOS>\n",
      "dev minibatch loss: 8.241036415100098\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > show me your not pound <EOS>\n",
      "    DEV dec input           > aint nothing wrong with being gay so embrace yourself <EOS>\n",
      "    DEV dec train predicted > too much to in i dress <EOS> much that i\n",
      "    DEV dec train infer > interesting read me in south car and just v car because lmao <EOS> <EOS> <EOS>\n",
      "global_step: 18553\n",
      "learning rate 0.000708564\n",
      "epoch 37\n",
      "batch 200\n",
      "training minibatch loss: 2.187566041946411\n",
      "  sample 1:\n",
      "    enc input           > i thought the profit recession wa ending <EOS>\n",
      "    dec input           > s amp p ex energy eps growth is falling not rising <EOS>\n",
      "    dec train predicted > s amp the court energy growth growth is rising at rising <EOS>\n",
      "dev minibatch loss: 8.974508285522461\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i dont think thats why if im being honest here <EOS>\n",
      "    DEV dec input           > because it here <EOS>\n",
      "    DEV dec train predicted > he he s can\n",
      "    DEV dec train infer > he can t play the business of his business <EOS> ? <EOS> <EOS> <EOS>\n",
      "global_step: 18603\n",
      "learning rate 0.000707906\n",
      "epoch 37\n",
      "batch 250\n",
      "training minibatch loss: 2.227922201156616\n",
      "  sample 1:\n",
      "    enc input           > tbh i wa just trying to do a monotone narrator voice to seem disinterested in it all haha <EOS>\n",
      "    dec input           > hey the narrative part totally remind me of porcupine tree voyage coincidence ? d <EOS>\n",
      "    dec train predicted > oh well narrative stage totally remind it of porcupine tree voyage coincidence <EOS> d <EOS>\n",
      "dev minibatch loss: 8.306958198547363\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > and get a standing ovation <EOS>\n",
      "    DEV dec input           > couldnt be more proud of my dad excited to see his remark during the final meeting <EOS>\n",
      "    DEV dec train predicted > best better a angel of the favorite handled to be the bed <EOS> the bed pit <EOS>\n",
      "    DEV dec train infer > like the gym is a bit at night on the log the bottle <EOS> <EOS> <EOS>\n",
      "global_step: 18653\n",
      "learning rate 0.000707249\n",
      "epoch 37\n",
      "batch 300\n",
      "training minibatch loss: 2.5199623107910156\n",
      "  sample 1:\n",
      "    enc input           > for real i be so mad listening to ppl argue bout it <EOS>\n",
      "    dec input           > whoever said there not is stupid <EOS>\n",
      "    dec train predicted > whoever said is a is a <EOS>\n",
      "dev minibatch loss: 7.676446914672852\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > wow johnson is considered a politician ? i ll vote for my yr old granddaughter <EOS>\n",
      "    DEV dec input           > for millennials trying to decide between amp i ask you to watch this amp then decide <EOS>\n",
      "    DEV dec train predicted > hillary the hillary to vote american <EOS> november vote president have president our country presidential <EOS> <EOS>\n",
      "    DEV dec train infer > watch the debate presidential debate ever <EOS> is here corrupt debate <EOS> <EOS> <EOS>\n",
      "global_step: 18703\n",
      "learning rate 0.000706593\n",
      "epoch 37\n",
      "batch 350\n",
      "training minibatch loss: 2.595003843307495\n",
      "  sample 1:\n",
      "    enc input           > we re you born yesterday ? when have black life ever mattered to the establishment ? <EOS>\n",
      "    dec input           > the buck stop with potus in every organization the responsibility lie with the leader <EOS>\n",
      "    dec train predicted > julia buck stop by potus by the organization the organization lie <EOS> the investigation <EOS>\n",
      "dev minibatch loss: 8.319317817687988\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > hillary is your candidate go follow oldman bush and support her already the globalists need your help <EOS>\n",
      "    DEV dec input           > not my candidate i ve been since the beginning the whole issue is just stupid it s over <EOS>\n",
      "    DEV dec train predicted > trump talking honor checker m been talking trump suggestion is reporter role for all a <EOS> s <EOS> <EOS>\n",
      "    DEV dec train infer > donald trump s brought to hrc s the scene about retweeting trump hrc with trump s given <EOS> <EOS>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 18753\n",
      "learning rate 0.000705937\n",
      "epoch 37\n",
      "batch 400\n",
      "training minibatch loss: 2.5187149047851562\n",
      "  sample 1:\n",
      "    enc input           > did someone put the decimal point in the wrong place ? <EOS>\n",
      "    dec input           > how is this possible ? good stuff <EOS>\n",
      "    dec train predicted > i much the possible ? <EOS> stuff <EOS>\n",
      "dev minibatch loss: 8.127521514892578\n",
      "  DEV sample 1:\n",
      "    DEV enc input           > i work day a week lil guy <EOS>\n",
      "    DEV dec input           > but it not you re it time to do something with your life <EOS>\n",
      "    DEV dec train predicted > so i s awesome sound a sound ? come a sweet you friend <EOS>\n",
      "    DEV dec train infer > now you re having good natural good a good shop sound like a grilled <EOS> <EOS> <EOS>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-c4d3ec96b9c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m                         decoder_targets_length: fd[3]}\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_track = []\n",
    "dev_loss_track = []\n",
    "\n",
    "all_weights = []\n",
    "dev_test_results = []\n",
    "metric_results = []\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    #saver.restore(session, \\\n",
    "    #    './seq2seq_CornelMovies_encode_100_decode_200_vocab_13679_embedding_200_seq_5_15_batch_64_layers_3_v5-210')\n",
    "    #saver.restore(session, \\\n",
    "    #    './seq2seq_CornelMovies_encode_100_decode_200_vocab_13679_embedding_200_seq_5_15_batch_64_layers_3_v5-100')\n",
    "    #saver.restore(session, \\\n",
    "    #    'd:\\coding\\seq2seq_CornelMovies_encode_200_decode_400_vocab_13679_embedding_200_seq_5_15_batch_128_layers_6_v6-990')\n",
    "    #saver.restore(session, \\\n",
    "    #    './seq2seq_CornelMovies_encode_500_decode_1000_vocab_13679_embedding_1024_seq_5_15_batch_32_layers_6_v6-30')\n",
    "    #saver.restore(session, \\\n",
    "    #'d:\\coding\\chkpt\\seq2seq_Cornell_encode_128_decode_256_vocab_13679_embedding_256_seq_5_15_batch_32_layers_3_enkeep_10_dekeep_10-203320')\n",
    "    #saver.restore(session, \\\n",
    "    #'d:\\coding\\seq2seq\\chkpt\\seq2seq_twitter_encode_128_decode_128_vocab_17182_embedding_1024_seq_3_19_batch_32_layers_1_enkeep_10_dekeep_10-16864')\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        df_all_train = df_all_train.sample(frac=1, random_state=tf.train.global_step(session, global_step))\n",
    "        #tf.train.global_step(session, global_step))\n",
    "       \n",
    "        encoded_text = df_all_train['alpha_Pair_0_encoding'].values\n",
    "        decoded_text = df_all_train['alpha_Pair_1_encoding'].values\n",
    "        text_index = df_all_train['Index'].values\n",
    "\n",
    "        input_batches = ([encoded_text[block_idx*n_batch_size:(block_idx+1)*n_batch_size], \n",
    "                 decoded_text[block_idx*n_batch_size:(block_idx+1)*n_batch_size], \n",
    "                         text_index[block_idx*n_batch_size:(block_idx+1)*n_batch_size]]\\\n",
    "                    for block_idx in range(len(encoded_text)))\n",
    "        \n",
    "        for batch in range(int(batches_in_epoch)):\n",
    "            mean_metric_train = []\n",
    "            mean_metric_dev = []\n",
    "\n",
    "            if copy_task == False:\n",
    "                \n",
    "                epoch_batches = next(input_batches)\n",
    "                \n",
    "                #input_batch_data = next(encoding_batches)\n",
    "                #target_batch_data = next(decoding_batches)\n",
    "                \n",
    "                input_batch_data = epoch_batches[0]\n",
    "                target_batch_data = epoch_batches[1]\n",
    "                batch_data_index = epoch_batches[2]\n",
    "\n",
    "            else:\n",
    "                ran_seq = generateRandomSeqBatchMajor(length_from=length_from, length_to=length_to,\n",
    "                                       vocab_lower=2, vocab_upper=vocab_upper,\n",
    "                                       batch_size=n_batch_size)\n",
    "                input_batch_data = ran_seq\n",
    "                target_batch_data = input_batch_data\n",
    "            \n",
    "            #fd = make_train_inputs(input_batch_data, target_batch_data)\n",
    "            fd = prepare_train_batch(input_batch_data, target_batch_data)\n",
    "            feed_dict = {encoder_inputs: fd[0],\n",
    "                        encoder_inputs_length: fd[1],\n",
    "                        decoder_targets: fd[2],\n",
    "                        decoder_targets_length: fd[3]}\n",
    "           \n",
    "            _, l = session.run([train_op, loss], feed_dict)\n",
    "            \n",
    "            if batch % 50 == 0: \n",
    "                \n",
    "                print ('global_step: %s' % tf.train.global_step(session, global_step))\n",
    "                print ('learning rate', session.run(optimizer._lr))\n",
    "                \n",
    "                print ('epoch', epoch)\n",
    "                print ('batch {}'.format(batch))\n",
    "                print ('training minibatch loss: {}'.format(l))\n",
    "                \n",
    "                train_loss_track.append([tf.train.global_step(session, global_step), l])\n",
    "\n",
    "                for i, (e_in, dt_targ, dt_pred) in enumerate(zip(feed_dict[encoder_inputs], \n",
    "                                                                 feed_dict[decoder_targets], \n",
    "                                                                 session.run(decoder_pred_train, feed_dict))):\n",
    "\n",
    "                    print('  sample {}:'.format(i + 1))\n",
    "                    #print('    Index', batch_data_index[i])\n",
    "                    #print('    enc input           > {}'.format(e_in))\n",
    "                    print('    enc input           > {}'.format(' '.join([inv_map[i] for i in e_in if i!=0])))\n",
    "\n",
    "                    #print('    dec input           > {}'.format(dt_targ))\n",
    "                    print('    dec input           > {}'.format(' '.join([inv_map[i] for i in dt_targ if i!=0])))\n",
    "\n",
    "                    #print('    dec train predicted > {}'.format(dt_pred))\n",
    "                    print('    dec train predicted > {}'.format(' '.join([inv_map[i] for i in dt_pred if i!=0])))\n",
    "                \n",
    "                    if i >= 0: break\n",
    "                        \n",
    "                #DEV CHECK\n",
    "                df_all_dev_check = df_all_dev.sample(n=32, random_state=tf.train.global_step(session, global_step))\n",
    "\n",
    "                dev_encoded_text = df_all_dev_check['alpha_Pair_0_encoding'].values\n",
    "                dev_decoded_text = df_all_dev_check['alpha_Pair_1_encoding'].values\n",
    "\n",
    "                fd_dev = prepare_train_batch([i for i in dev_encoded_text], [i for i in dev_decoded_text])\n",
    "\n",
    "                feed_dict_dev = {encoder_inputs: fd_dev[0],\n",
    "                                 encoder_inputs_length: fd_dev[1],\n",
    "                                 decoder_targets: fd_dev[2],\n",
    "                                 decoder_targets_length: fd_dev[3]}\n",
    "\n",
    "                #fd_inf = prepare_batch([i for i in dev_encoded_text])\n",
    "\n",
    "                feed_dict_inf = {encoder_inputs: fd_dev[0],\n",
    "                                 encoder_inputs_length: fd_dev[1]}\n",
    "\n",
    "                dev_inf_out = session.run([decoder_pred_decode, decoder_pred_decode_prob], feed_dict_inf) \n",
    "                dev_loss = session.run(loss, feed_dict_dev)\n",
    "                \n",
    "                dev_loss_track.append([tf.train.global_step(session, global_step), dev_loss])\n",
    "                print ('dev minibatch loss: {}'.format(dev_loss))\n",
    "\n",
    "                for i, (e_in, dt_targ, dt_pred, dt_inf, df_inf_out_prob) in enumerate(zip(feed_dict_dev[encoder_inputs], \n",
    "                                                                 feed_dict_dev[decoder_targets], \n",
    "                                                                 session.run(decoder_pred_train, feed_dict_dev),\n",
    "                                                                 dev_inf_out[0], dev_inf_out[1])):\n",
    "\n",
    "                    print('  DEV sample {}:'.format(i + 1))\n",
    "                    #print('    Index', batch_data_index[i])\n",
    "                    #print('    DEV enc input           > {}'.format(e_in))\n",
    "                    print('    DEV enc input           > {}'.format(' '.join([inv_map[i] for i in e_in if i!=0])))\n",
    "\n",
    "                   # print('    DEV dec input           > {}'.format(dt_targ))\n",
    "                    print('    DEV dec input           > {}'.format(' '.join([inv_map[i] for i in dt_targ if i!=0])))\n",
    "\n",
    "                    #print('    DEV dec train predicted > {}'.format(dt_pred))\n",
    "                    print('    DEV dec train predicted > {}'.format(' '.join([inv_map[i] for i in dt_pred if i!=0])))\n",
    "                    \n",
    "                    #print('    DEV dec train infer > {}'.format(dt_inf))\n",
    "                    print('    DEV dec train infer > {}'.format(' '.join([inv_map[i] for i in dt_inf if i!=0])))\n",
    "                \n",
    "                    if i >= 0: break\n",
    "\n",
    "             \n",
    "                 #   df_prediction_train = predictionCheck(mean_metric_train)\n",
    "                 #   print (df_prediction_train['meanCheckList'].describe()['mean'])\n",
    "\n",
    "                 #   df_prediction_dev = predictionCheck(mean_metric_dev)\n",
    "                #    print (df_prediction_dev['meanCheckList'].describe()['mean'])\n",
    "\n",
    "                 #   metric_results.append([df_prediction_train, df_prediction_dev])\n",
    "                \n",
    "        if epoch % 3 == 0: \n",
    "            print ('Saving session')\n",
    "            #eval_dev = devCheck(dev_encoded_text, dev_decoded_text, True)\n",
    "            \n",
    "            #dev_test_results.append(eval_dev)\n",
    "            \n",
    "            #pickle.dump(dev_test_results, open('d:\\coding\\chkpt\\dev_test_results_epoch_%d.pkl' % epoch, 'wb'))\n",
    "            \n",
    "            saver.save(session, \\\n",
    "'chkpt/seq2seq_%s_encode_%d_decode_%d_vocab_%d_embedding_%d_seq_%d_%d_batch_%d_layers_%d_enkeep_%d_dekeep_%d' % \\\n",
    "                (dataset, n_cells, n_cells, vocab_size, input_embedding_size, length_from, length_to, n_batch_size, num_layers,\n",
    "                int(encoder_output_keep*10), int(decoder_output_keep*10)), \\\n",
    "                       global_step = tf.train.global_step(session, global_step))\n",
    "            #saver.save(session, 'd:\\coding\\seq2seq\\chkpt\\copy_task', global_step = tf.train.global_step(session, global_step))\n",
    "       # variables_names =[v.name for v in tf.trainable_variables()]\n",
    "       # values = session.run(variables_names)\n",
    "       # all_weights.append([values[1], values[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(list(zip(*dev_loss_track))[0], list(zip(*dev_loss_track))[1], label='Dev')\n",
    "#plt.plot(list(zip(*loss_track))[0], list(zip(*loss_track))[1], label='Train')\n",
    "#plt.xlabel('Global Step')\n",
    "#plt.ylabel('Sequence Loss')\n",
    "#plt.legend(loc='best')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from chkpt/seq2seq_test-1501\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Assign requires shapes of both tensors to match. lhs shape= [211] rhs shape= [21]\n\t [[Node: save/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@decoder/output_projection/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](decoder/output_projection/bias, save/RestoreV2_7/_1)]]\n\nCaused by op 'save/Assign_7', defined at:\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-5f40fbb6d53f>\", line 8, in <module>\n    saver = tf.train.Saver()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1140, in __init__\n    self.build()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1172, in build\n    filename=self._filename)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 688, in build\n    restore_sequentially, reshape)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 419, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 274, in assign\n    validate_shape=validate_shape)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 43, in assign\n    use_locking=use_locking, name=name)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [211] rhs shape= [21]\n\t [[Node: save/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@decoder/output_projection/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](decoder/output_projection/bias, save/RestoreV2_7/_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [211] rhs shape= [21]\n\t [[Node: save/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@decoder/output_projection/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](decoder/output_projection/bias, save/RestoreV2_7/_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5f40fbb6d53f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m    \u001b[1;31m# saver.restore(session, \\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#'d:\\coding\\seq2seq\\chkpt\\seq2seq_enron_encode_24_decode_48_vocab_10_embedding_32_seq_3_10_batch_32_layers_1_enkeep_10_dekeep_10-9639')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'chkpt/seq2seq_test-1501'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_chunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1558\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1559\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1560\u001b[1;33m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1562\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [211] rhs shape= [21]\n\t [[Node: save/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@decoder/output_projection/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](decoder/output_projection/bias, save/RestoreV2_7/_1)]]\n\nCaused by op 'save/Assign_7', defined at:\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-5f40fbb6d53f>\", line 8, in <module>\n    saver = tf.train.Saver()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1140, in __init__\n    self.build()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1172, in build\n    filename=self._filename)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 688, in build\n    restore_sequentially, reshape)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 419, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 274, in assign\n    validate_shape=validate_shape)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 43, in assign\n    use_locking=use_locking, name=name)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [211] rhs shape= [21]\n\t [[Node: save/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@decoder/output_projection/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](decoder/output_projection/bias, save/RestoreV2_7/_1)]]\n"
     ]
    }
   ],
   "source": [
    "#Create inference\n",
    "mean_metric = []\n",
    "chunk_size = 500\n",
    "#n_chunks = int(df_all_train.shape[0]/chunk_size)\n",
    "n_chunks=10\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "   # saver.restore(session, \\\n",
    "#'d:\\coding\\seq2seq\\chkpt\\seq2seq_enron_encode_24_decode_48_vocab_10_embedding_32_seq_3_10_batch_32_layers_1_enkeep_10_dekeep_10-9639')\n",
    "    saver.restore(session, 'chkpt/seq2seq_test-1501')\n",
    "    \n",
    "    for chunk in range(n_chunks):\n",
    "        if chunk>0: break\n",
    "        ran_seq = generateRandomSeqBatchMajor(length_from=length_from, length_to=length_to,\n",
    "                       vocab_lower=2, vocab_upper=vocab_upper,\n",
    "                       batch_size=n_batch_size)\n",
    "        \n",
    "        #input_batch_data = ran_seq\n",
    "        input_batch_data = [[3,4,5,6,7,8, 9]]\n",
    "        #input_batch_data = df_all_dev['alpha_Pair_0_encoding'].values[:32]\n",
    "        \n",
    "        fd_inf = prepare_batch(input_batch_data)\n",
    "        feed_dict_inf = {encoder_inputs: fd_inf[0],\n",
    "                    encoder_inputs_length: fd_inf[1]}\n",
    "        inf_out = session.run([decoder_pred_decode, decoder_pred_decode_prob], feed_dict_inf)\n",
    "\n",
    "        #print (df_all_train.values[0][1], df_all_train.values[1][1])\n",
    "        #print (feed_dict_inf)\n",
    "        for i, (e_in, dt_inf) in enumerate(zip(feed_dict_inf[encoder_inputs], inf_out[0])):\n",
    "            #mean_metric.append([df_all_train.values[i][0], df_all_train.values[i][1], dt_inf])\n",
    "            print('    sample {}:'.format(i + 1))\n",
    "            print('    enc input                > {}'.format([inv_map[k] for k in e_in]))\n",
    "            print('    dec input                > {}'.format([inv_map[k] for k in df_all_dev['alpha_Pair_1_encoding'].values[i]]))\n",
    "            print('    dec train inference      > {}'.format([inv_map[k] for k in dt_inf]))\n",
    "            #print('    dec train inference prob > {}'.format([inf_out[1][j][i].max() for j in range((len(inf_out[1])))]))\n",
    "            \n",
    "            #if i>0: break\n",
    "        \n",
    "       # print ('Save Model')\n",
    "       # builder = tf.saved_model.builder.SavedModelBuilder('d:\\coding\\seq2seq\\model')\n",
    "       # builder.add_meta_graph_and_variables(session, ['serve'])\n",
    "\n",
    "        #builder.save()\n",
    "    #ops = session.graph.get_operations()\n",
    "\n",
    "    #feed_ops = [op for op in ops if op.type=='Placeholder']\n",
    "\n",
    "    #print(feed_ops)\n",
    "        #if n_chunks >0: \n",
    "         #   break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fd_inf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-280905bd70fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfd_inf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'fd_inf' is not defined"
     ]
    }
   ],
   "source": [
    "fd_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from chkpt/seq2seq_test-1501\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Assign requires shapes of both tensors to match. lhs shape= [211] rhs shape= [21]\n\t [[Node: save_5/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@decoder/output_projection/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](decoder/output_projection/bias, save_5/RestoreV2_7/_1)]]\n\nCaused by op 'save_5/Assign_7', defined at:\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-23-7df2da4875fb>\", line 3, in <module>\n    saver = tf.train.Saver()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1140, in __init__\n    self.build()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1172, in build\n    filename=self._filename)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 688, in build\n    restore_sequentially, reshape)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 419, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 274, in assign\n    validate_shape=validate_shape)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 43, in assign\n    use_locking=use_locking, name=name)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [211] rhs shape= [21]\n\t [[Node: save_5/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@decoder/output_projection/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](decoder/output_projection/bias, save_5/RestoreV2_7/_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [211] rhs shape= [21]\n\t [[Node: save_5/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@decoder/output_projection/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](decoder/output_projection/bias, save_5/RestoreV2_7/_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-7df2da4875fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'chkpt/seq2seq_test-1501'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m#ran_seq = generateRandomSeqBatchMajor(length_from=length_from, length_to=length_to,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#               vocab_lower=2, vocab_upper=vocab_upper,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1558\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1559\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1560\u001b[1;33m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1562\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [211] rhs shape= [21]\n\t [[Node: save_5/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@decoder/output_projection/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](decoder/output_projection/bias, save_5/RestoreV2_7/_1)]]\n\nCaused by op 'save_5/Assign_7', defined at:\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-23-7df2da4875fb>\", line 3, in <module>\n    saver = tf.train.Saver()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1140, in __init__\n    self.build()\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1172, in build\n    filename=self._filename)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 688, in build\n    restore_sequentially, reshape)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 419, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 274, in assign\n    validate_shape=validate_shape)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 43, in assign\n    use_locking=use_locking, name=name)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"c:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [211] rhs shape= [21]\n\t [[Node: save_5/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@decoder/output_projection/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](decoder/output_projection/bias, save_5/RestoreV2_7/_1)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "  \n",
    "    saver.restore(session, 'chkpt/seq2seq_test-1501')\n",
    "    #ran_seq = generateRandomSeqBatchMajor(length_from=length_from, length_to=length_to,\n",
    "    #               vocab_lower=2, vocab_upper=vocab_upper,\n",
    "   #                batch_size=n_batch_size)\n",
    "\n",
    "    #input_batch_data = ran_seq\n",
    "    #input_batch_data = [[3, 4, 5, 6, 7, 8,9]]\n",
    "\n",
    "    #fd_inf = prepare_batch(input_batch_data)\n",
    "    #feed_dict_inf = {encoder_inputs: fd_inf[0],\n",
    "    #            encoder_inputs_length: fd_inf[1]}\n",
    "\n",
    "    #inf_out = session.run([encoder_inputs_embedded], feed_dict_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m(1340)\u001b[0;36m_do_call\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m   1338 \u001b[1;33m        \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1339 \u001b[1;33m          \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m-> 1340 \u001b[1;33m      \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1341 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1342 \u001b[1;33m  \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> up\n",
      "> \u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m(1321)\u001b[0;36m_do_run\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m   1319 \u001b[1;33m    \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1320 \u001b[1;33m      return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n",
      "\u001b[0m\u001b[1;32m-> 1321 \u001b[1;33m                           options, run_metadata)\n",
      "\u001b[0m\u001b[1;32m   1322 \u001b[1;33m    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1323 \u001b[1;33m      \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> fetches\n",
      "[]\n",
      "ipdb> targets\n",
      "[b'save_2/restore_all']\n",
      "ipdb> feeds\n",
      "{b'save_2/Const:0': array('chkpt/seq2seq_test-1501', dtype=object)}\n",
      "ipdb> up\n",
      "> \u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m(1124)\u001b[0;36m_run\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m   1122 \u001b[1;33m    \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1123 \u001b[1;33m      results = self._do_run(handle, final_targets, final_fetches,\n",
      "\u001b[0m\u001b[1;32m-> 1124 \u001b[1;33m                             feed_dict_tensor, options, run_metadata)\n",
      "\u001b[0m\u001b[1;32m   1125 \u001b[1;33m    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1126 \u001b[1;33m      \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> up\n",
      "> \u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m(903)\u001b[0;36mrun\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m    901 \u001b[1;33m        \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    902 \u001b[1;33m      \u001b[1;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m--> 903 \u001b[1;33m        \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    904 \u001b[1;33m    \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    905 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> up\n",
      "> \u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m(1560)\u001b[0;36mrestore\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m   1558 \u001b[1;33m    \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1559 \u001b[1;33m    sess.run(self.saver_def.restore_op_name,\n",
      "\u001b[0m\u001b[1;32m-> 1560 \u001b[1;33m             {self.saver_def.filename_tensor_name: save_path})\n",
      "\u001b[0m\u001b[1;32m   1561 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1562 \u001b[1;33m  \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> up\n",
      "> \u001b[1;32m<ipython-input-17-fa2c85879735>\u001b[0m(13)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m      9 \u001b[1;33m    \u001b[0mfd_inf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_batch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     10 \u001b[1;33m    feed_dict_inf = {encoder_inputs: fd_inf[0],\n",
      "\u001b[0m\u001b[1;32m     11 \u001b[1;33m                encoder_inputs_length: fd_inf[1]}\n",
      "\u001b[0m\u001b[1;32m     12 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 13 \u001b[1;33m    \u001b[0minf_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoder_inputs_embedded\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_inf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
